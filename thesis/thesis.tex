%%% The main file. It contains definitions of basic parameters and includes all other parts.

%% Settings for single-side (simplex) printing
% Margins: left 40mm, right 25mm, top and bottom 25mm
% (but beware, LaTeX adds 1in implicitly)
\documentclass[12pt,a4paper]{report}
\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}
% \openright makes the following text appear on a right-hand page
\let\openright=\clearpage

%% Settings for two-sided (duplex) printing
% \documentclass[12pt,a4paper,twoside,openright]{report}
% \setlength\textwidth{145mm}
% \setlength\textheight{247mm}
% \setlength\oddsidemargin{14.2mm}
% \setlength\evensidemargin{0mm}
% \setlength\topmargin{0mm}
% \setlength\headsep{0mm}
% \setlength\headheight{0mm}
% \let\openright=\cleardoublepage

%% Character encoding: usually latin2, cp1250 or utf8:
\usepackage[utf8]{inputenc}

%% Further useful packages (included in most LaTeX distributions)
\usepackage{amsmath}        % extensions for typesetting of math
\usepackage{amsfonts}       % math fonts
\usepackage{amsthm}         % theorems, definitions, etc.
\usepackage{bbding}         % various symbols (squares, asterisks, scissors, ...)
\usepackage{bm}             % boldface symbols (\bm)
\usepackage{graphicx}       % embedding of pictures
\usepackage{fancyvrb}       % improved verbatim environment
\usepackage{natbib}         % citation style AUTHOR (YEAR), or AUTHOR [NUMBER]
\usepackage[nottoc]{tocbibind} % makes sure that bibliography and the lists
			    % of figures/tables are included in the table
			    % of contents
\usepackage{dcolumn}        % improved alignment of table columns
\usepackage{booktabs}       % improved horizontal lines in tables
\usepackage{paralist}       % improved enumerate and itemize
\usepackage[usenames]{xcolor}  % typesetting in color

%%% User-defined packages
\usepackage{amssymb}
\usepackage[inline]{enumitem}
\DeclareMathOperator{\tr}{tr}
\usepackage{adjustbox}
\usepackage{dcolumn}

%%% Basic information on the thesis

% Thesis title in English (exactly as in the formal assignment)
\def\ThesisTitle{Evolutionary Algorithms for Data Transformation}

% Author of the thesis
\def\ThesisAuthor{Ondřej Švec}

% Year when the thesis is submitted
\def\YearSubmitted{2017}

% Name of the department or institute, where the work was officially assigned
% (according to the Organizational Structure of MFF UK in English,
% or a full name of a department outside MFF)
\def\Department{Department of Theoretical Computer Science and Mathematical Logic}

% Is it a department (katedra), or an institute (ústav)?
\def\DeptType{Department}

% Thesis supervisor: name, surname and titles
\def\Supervisor{Mgr. Martin Pilát, Ph.D.}

% Supervisor's department (again according to Organizational structure of MFF)
\def\SupervisorsDepartment{Department of Theoretical Computer Science and Mathematical Logic}

% Study programme and specialization
\def\StudyProgramme{Computer Science}
\def\StudyBranch{Artificial Intelligence}

% An optional dedication: you can thank whomever you wish (your supervisor,
% consultant, a person who lent the software, etc.)
\def\Dedication{%
Dedication. %TODO
}

% Abstract (recommended length around 80-200 words; this is not a copy of your thesis assignment!)
\def\Abstract{%
Abstract. %TODO
}

% 3 to 5 keywords (recommended), each enclosed in curly braces
\def\Keywords{%
{key} {words} %TODO
}

%% The hyperref package for clickable links in PDF and also for storing
%% metadata to PDF (including the table of contents).
\usepackage[pdftex,unicode]{hyperref}   % Must follow all other packages
\hypersetup{breaklinks=true}
\hypersetup{pdftitle={\ThesisTitle}}
\hypersetup{pdfauthor={\ThesisAuthor}}
\hypersetup{pdfkeywords=\Keywords}
\hypersetup{urlcolor=blue}

% Definitions of macros (see description inside)
\include{macros}

% Title page and various mandatory informational pages
\begin{document}
\include{title}

%%% A page with automatically generated table of contents of the master thesis

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Guidelines

% Transformace dat jsou důležitou součástí strojového učení, která výrazně ovlivňuje kvalitu vytvořených modelů. Transformace dat se často používají i pro jejich zobrazení do prostoru s menší dimenzí, kde se dají snáze vizualizovat. Většina metod ale funguje bez učitele a tedy není schopna najít taková zobrazení dat, která by brala v úvahu vlastnosti metod strojového učení, které následují po ní, případně se snažila vizualizaci upravit tak, aby data ze stejné třídy byla blízko u sebe. Některé metody určené přímo pro vizualizaci (jako např. t-SNE) navíc ani neposkytují transformační funkci a nejsou tedy schopny zobrazit nová data bez přepočítání celého zobrazení. Cílem práce je tedy pomocí kombinace evolučních algoritmů a dalších přístupů (např. neuronových sítí) navrhnout metody pro vytvoření transformační funkce, která bude brát v úvahu i označkování dat. 

% Student se seznámí s postupy pro automatické transformace dat. Na základě zjištěných informací implementuje vlastní metody a porovná je s existujícími přístupy. Součástí srovnání bude i vhodnost použité metody pro zobrazení dat do prostoru s malou dimenzí vhodnou pro vizualizaci.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{preface}
%\include{chap01_competition}
%\include{chap01}
%\include{chap02}
%\include{chap03_mahalanobis}
%\include{epilog}

\chapter{Introduction} \label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

The need for measuring distance or similarity between data is ubiquitous in machine learning and creating custom metrics by hand is very difficult process. This has led to an introduction of distance metric learning where we learn distance metrics or similarities from data themselves.

One of the popular metrics is Mahalanobis distance, which generalizes Euclidean distance with a parameter matrix. We examine methods for learning this Mahalanobis distance and compare their performance on common datasets.

% Úvod do problematiky
% - Co už se udělalo
% - Co je známo
% - Co není známo
% - Co a pro Co a proč chceme naší prací objasnit naší prací objasnit
% - Jasná definice cíle práce
% - Struktura práce

% Related works
% - Co už udělali jiní
% - Co nového hodlá udělat autor

% v úvodu
% samostatná kapitola za úvodem samostatná kapitola za úvodem
% v závěru článku

\section{Motivation}

Curse of dimensionality, bad euclid distance

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Goals}

\subsection{Foundation stone}

Many researches devoted to learn a Mahalanobis distance metric, which has mostly been used to improve the performance of kNN classification. Most of the research has been focused on learning this metric from labeled training instances.

In this thesis, we review existing state-of-the-art methods for learning global Mahalanobis distance metric (Xing, LMNN, NCA, ...) and multiple local Mahalanobis distance metrics (PLML, ...) and compare them on several popular datasets. Learning the full Mahalanobis metric means learning $d^2$ parameters and so we also try to restrict the existing methods to learn only a diagonal of the Mahalanobis distance metric with $d$ parameters.

There was no unified library containing the algorithms for learning the distance metrics and so we created a library with unified interface according to the current standards.

\subsection{Nice to have}

In the next part of our work we apply the known methods from supervised setting to semi-supervised where we have both labeled and unlabeled training instances. The idea is to use the Mahalanobis metric distance learnt from labeled pairs for clustering the unlabeled instances. It is possible (and very likely) that the unlabeled instances contain more classes.

We examine this for methods using global metrics and also for methods using multiple local distance metrics where we needed to adapt existing clustering algorithms for our multi-metric setting.

\subsection{Moonshot}

In the last section of the thesis we examined an iterative distance metric learning process consisting of these steps:
\begin{enumerate*}
\item{learning a distance metric from the labeled data}
\item{applying a non-supervised clustering method to obtain new labels (and possibldifferent number of classes)}
\item{repeat}
\end{enumerate*}
.

\subsection{Not included}

online metrics? - usually global

\section{Methodology}
\section{Thesis structure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Notation} \label{notation}

% More inspiration:
% http://www.deeplearningbook.org/contents/notation.html

\begin{tabbing}
\hspace{200pt}\=\kill
$\mathbb{R}$ \> Set of real numbers \\
$\mathbb{R}^d$ \> Set of d-dimensional real-valued vectors \\
$\mathbb{R}^{c \times d}$ \> Set of c$\times$d real-valued matrices \\
$\mathbb{S}^{d \times d}_+$ \> Cone of symmetric PSD d$\times$d real-valued matrices \\
$\textbf{x}$ \> An arbitrary vector \\
$\bm{M}$ \> An arbitrary matrix \\
$\textbf{\textit{I}}$ \> Identity matrix \\
$\bm{M} \succeq 0$ \> PSD matrix M \\
$\parallel \cdot \parallel_p$ \> p-norm \\
$\parallel \cdot \parallel_\mathcal{F}$ \> Frobenius norm \\
$\parallel \cdot \parallel_*$ \> Nuclear norm \\
$\tr(\bm{M})$ \> Trace of matrix M \\
$[t]_+ = max(0, 1-t)$ \> Hinge loss function \\
$\xi$ \> Slack variable \\
$\Sigma$ \> Finite alphabet \\

$\mathcal{X} \subseteq \mathbb{R}^d$ \> Input (instance) space \\
$\mathcal{Y} = \{ 1, \ldots ,c \}$ \> Set of $c$ output labels (ground truth) \\
$\widehat{\mathcal{Y}} = \{ 1, \ldots ,\widehat{c} \} \subseteq \mathcal{Y}$ \> Set of $\widehat{c}$ known output labels \\
% $x$ \> String of finite size
$z=(x,y) \in \mathcal{X} \times \mathcal{Y}$ \> An arbitrary labeled instance \\
$\mathcal{L}=\{z_i=(x_i, y_i)\}^n_{i=1}$ \> Set of $n$ labeled training instances \\
$\mathcal{U} = \{x_i \}^m_{i=1}$ \> Set of $m$ unlabeled training instances \\
$\overline{z} = (x, \overline{y}) \in \mathcal{X} \times \widehat{\mathcal{Y}} $ \> An arbitrary instance with predicted label \\
$\overline{\mathcal{Z}} = \{\overline{z_i} = (x_i, \overline{y_i}) \}^m_{i=1}$ \> Set of $m$ training instances with predicted labels \\
$\mathcal{S} = \{ (x_i, x_j) \mid y_i = y_j \}$ \> Set of must-link constraints \\
$\mathcal{D} = \{ (x_i, x_j) \mid y_i \neq y_j \}$ \> Set of cannot-link constraints \\
$\mathcal{R} = \{ (x_i, x_j, x_k) \mid y_i = y_j \wedge y_i \neq y_k \}$ \> Set of relative constraints \\
\end{tabbing} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Metric learning}

Introduction

\section{Applications}

Looking at metric learning as a "data transformation/preprocessing".

Computer vision
Information retrieval
Bioinformatics

\section{Related Topics}

Kernel learning
Multiple kernel learning
Dimensionality reduction

\section{Key Properties of Metric Learning Algorithms}

Learning paradigms: supervised, semi-supervised, unsupervised

Form of metrics: global (linear), non-linear, local

Scalability

Optimality of a solution (global optimum, local optimum)

Dimensionality reduction

\section{Supervised Mahalanobis Metric Learning}

Definition of a metric

% https://numerics.mathdotnet.com/distance.html
\begin{equation} 
d_{\mathbf{2}} : (x, y) \mapsto \|x-y\|_2 = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2}
\end{equation}

Euclidean metric

Minkowski metric
\begin{equation}
d_{\mathbf{p}} : (x, y) \mapsto \|x-y\|_p = \bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p}
\end{equation}

Chebyshev Distance: p goes to infinity
\begin{equation}
d_{\mathbf{\infty}} : (x, y) \mapsto \|x-y\|_\infty = \lim_{p \rightarrow \infty}\bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p} = \max_{i} |x_i-y_i|
\end{equation}

Mahalanobis metric

\begin{equation}
d_{Mahalanobis}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^T\bm{\Omega}^{-1}(\textbf{x}-\textbf{y})} 
\end{equation}

\cite{mahalanobis1936generalized}

\begin{align*}
  d_{\bm{M}}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})} \\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{T}\bm{L}(\textbf{x}-\textbf{y})} \\
         &= \sqrt{(\bm{L}\textbf{x}-\bm{L}\textbf{y})^{T}(\bm{L}\textbf{x}-\bm{L}\textbf{y})} \\
         &= d(\bm{L}\textbf{x}, \bm{L}\textbf{y})
\end{align*}

\subsection{SVD decomposition}

proof that $L^TL$ is PSD matrix

% http://www.deeplearningbook.org/contents/linear_algebra.html

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related work} \label{chap:related-work}

In this work we chose to examine the following methods:

\begin{enumerate}
\item Simple Covariance Metric
\item Xing \cite{xing2002distance}
\item Large Margin Nearest Neighbor (LMNN) \cite{weinberger2009distance}
\item Information Theoretic Metric Learning (ITML) \cite{davis2007information}
\item Sparse Determinant Metric Learning (SDML) \cite{qi2009efficient}
\item Least Squares Metric Learning (LSML) \cite{liu2012metric}
\item Neighborhood Components Analysis (NCA) \\ \cite{jacobgoldberger2004neighbourhood}
\item Local Fisher Discriminant Analysis (LFDA) \cite{sugiyama2007dimensionality}
\item Relative Components Analysis (RCA) \cite{shental2002adjustment}
\item \cite{fukui2013evolutionary}
\end{enumerate}

- Teoretická analýza

Zmínit lokální metriky, online, ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Our new method} \label{chap:our-method}

CMAES + kNN

full matrix, diagonal matrix, neural network shape

k-Means for dimensionality reduction

\section{Evolutionary algorithms}
\section{Evolutionary strategy (ES)}

simple evolution, jDE, CMAES, ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments and Examples}
We performed three different experiments to assess the performance of metric learning algorithms. In section \ref{chap:trivial-dataset} we showcase, where the metric learning algorithms perform very well on a trivial dataset. In section \ref{chap:experiment-knn} we use learnt metric in k-Nearest~Neighbour classificator and compare classification errors on various datasets, which are described in \ref{chap:datasets}. In the next experiment in section \ref{chap:experiment-fitness} we compare different evolutionary algorithms together with various fitnesses and show how well they generalize. As the last experiment in section \ref{chap:experiment-dim-reduction} we use metric learning algorithms for dimensionality reduction and compare the resulting embeddings.

\section{Experimental settings} 
In our experiments we compare metric learning algorithms listed in section \ref{chap:related-work}. Most of these algorithms are implemented in an open-sourced Python \textit{metric-learn} library \footnote{\url{https://github.com/all-umass/metric-learn}}. Only our evolutionary method described in section \ref{chap:our-method} and \cite{fukui2013evolutionary} are missing in this library. Therefore we designed a modular interface and implemented these two methods as a part of the \textit{metric-learn} library. The~implementation is described in section \ref{chap:implementation}.

\subsection{Datasets} \label{chap:datasets}
We chose to experiment on classification datasets that were most common among all related works, particularily \cite{xing2002distance}, \cite{weinberger2009distance}, \cite{jacobgoldberger2004neighbourhood} and \cite{fukui2013evolutionary}. We found that most common among all these papers was balance-scale, breast-cancer-wisconsin, ionosphere, iris, mice-protein, pima-indians-diabetes, sonar, soybean-large and wine dataset. All these datasets were obtained from the well-known UCI Machine Learning Repository \footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Even among these datasets we have ionosphere, mice-protein, sonar and soybean-large that all have high dimensionality, however they are all small in terms of number of samples and so we decided to keep them among the small datasets.

\begin{table}[ht] \centering
\begin{tabular}{lrrr}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Datasets & \#~samples & \#~dimensions & \#~classes \\
\hline
balance-scale           & 625   & 4    & 3  \\
breast-cancer-wisconsin & 699   & 9    & 2  \\
ionosphere              & 351   & 34   & 2  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & 77   & 8  \\
pima-indians-diabetes   & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
soybean-large           & 307   & 35   & 19 \\
wine                    & 178   & 13   & 3  \\
\hline
\end{tabular}
\caption{Small UCI datasets summary} \label{tab:datasets}
\end{table}

We also wanted to test whether the algorithms scale well on datasets that are larger  in terms of dimensionality and number of samples. For these experiments we picked MNIST, digits6, digits10 and ofaces datasets. These datasets were obtained using Python library Scikit-Learn.org \footnote{\url{http://scikit-learn.org/stable/datasets/}}.

Table \ref{tab:datasets} summarizes small datasets and table \ref{tab:datasets-big} summarizes large datasets that we experimented on. Both table show number of samples, dimensionality and number of distinct classes for each dataset.

[Short info about all datasets we used? Multivariate, Real / Integer / Categorical, Missing data? %TODO ]

\begin{table}[ht] \centering
\begin{tabular}{lrrr}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Datasets & \#~instances & \#~dimensions & \#~classes \\
\hline
digits6                 & 1083  & 64   & 6  \\
digits10                & 1797  & 64   & 10 \\
letters                 & 20000 & 16   & 26 \\
mnist                   & 70000 & 784  & 10 \\
ofaces                  & 400   & 4096 & 40 \\
\hline
\end{tabular}
\caption{Larger datasets summary} \label{tab:datasets-big} 
\end{table}

\subsection{Preprocessing data} \label{chap:preprocessing}
Some of the algorithms have missing attributes. There are several strategies for dealing with missing attributes, such as removing affected samples, filling missing values with zeros, or with a random value, mean, median or the most frequent value. We chose to fill missing values using the mean for any given attribute as we found it behaves well with all chosen datasets.

Another difficulty with the datasets is that many of them do not have normalized attributes and their attribute values are unbounded and highly variable. We were curious if the algorithms are able to deal with this problem alone and so we chose to benchmark the original unnormalized version of the datasets as well as  normalizing them. The datasets were normalized using standardization defined in equation \ref{eq:stand}, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations.

\begin{equation} \label{eq:stand}
\hat{X} = \frac{X-\mu}{\sigma}
\end{equation}

\section{Trivial toy dataset} \label{chap:trivial-dataset}

[show]

	*** **** *** *** *** * ** **** * * **** ***
	-  -   -    -    -   -   -    -    -     -


	********
	********

	--------
	--------

\section{Experiment: Classification} \label{chap:experiment-knn}

In this experiment we wanted to see how much does learnt metric help in classification task compared to standard Euclidean distance. We chose k-Nearest~Neighbour (kNN) classifier, which is one of the simplest and yet powerful classifiers and it is very easy to modify to use a custom metric.

To assess performance of the metric learning algorithms we used datasets listed in section \ref{chap:datasets} and used 10-fold cross validation for each algorithm. For each fold data were first preprocessed and standardized as described in section \ref{chap:preprocessing}, then the metric was trained using one of the methods and finally the metric was evaluated using kNN algorithm.

We also wanted to test, how does standardization influence the learnt metrics and whether the metric learning algorithms can handle unnormalized data and so we also tried all these experiments again, but without standardizing the data.

Every metric learning algorithm has a set of hyperparameters which highly influence the learnt metric. For each hyperparameter we selected sensible values in the range that would make most sense. All hyperparameters for each algorithm are described in table \ref{tab:hyperparams}. The performance of kNN classifier hugely depends on its number of neighbours hyperparameter. We chose to try values 1, 2, 4, 8, 16, 32, 64, 128 for this parameter.

\input{results/hyperparameters}

Even for these sensibly chosen hyperparameter values there are thousands of possible combinations for every single algorithm. Let’s consider LMNN algorithm. There are 10 folds, 8 possible values for kNN hyperparameter, LMNN itself has 4 hyperparameters with 6, 3, 4 and 3 values, which means there would be $10*8*(6*3*4*3)=17280$ different models to train if we used a simple grid search. This is a problem because some of the algorithms take a long time to train even on small datasets. 

We can notice, however, that we do not need to retrain our metric for every hyperperameter of the final kNN classifier. Therefore, we designed our version of grid search that is hierarchical in a sense that it will train a metric with all combinations of hyperparameters first and only then it will evaluate these metrics using kNN with its own hyperparameters. There will still be the same total number of models, however there will be 8 times less evaluations of every metric learning algorithm. This can be looked at as a simple space-time tradeoff.
%TODO add a sketch of hierarchical grid search


\subsection{Results}

How hyperparameters influence the successrate. [graphs]

%TODO add results for larger datasets
\input{results/errors-small-dataset}


\section{Experiment: Generalization of EAs} \label{chap:experiment-fitness}


\section{Experiment: Dimensionality reduction} \label{chap:experiment-dim-reduction}

\subsection{Direct comparison with other methods}

\subsection{Preprocessing for t-SNE}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation} \label{chap:implementation}

\section{Hierarchical Grid search}

how it works - scheme

how cross-validation is in memory

\section{Metric learning in metric\_learn package}

Description of modules, fitnesses, transformers, ...

\begin{figure}[h!] \label{fig:implementation-modules}
	\centering
    \includegraphics[width=0.2\textwidth]{img/notfound}
    \caption{Interaction between modules for Metric evolution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}

Graphs, performace, caveats, what worked, what didnt work...

LMNN, ITML, SDML take too long to calculate

LMNN and LFDA perform very well

LMNN has too many hyper parameters

Simple Covariance metric was impossible to calculate on certain datasets

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

This worked, this didn't work. What we did.

\chapter*{Recommendations for future work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Test} \label{subsection}


\noindent Numbered equation:
\begin{equation*}\label{my favorite equation}
  e^{i\pi}=-1
\end{equation*}

\begin{equation} \label{eq:1}
\sum_{i=0}^{\infty} a_i x^i
\end{equation} 
The equation \ref{eq:1} is a typical power series.


\section{Math references} \label{mathrefs}
As mentioned in section \autoref{introduction}, different elements can 
be referenced within a document

 
In the subsection \ref{subsection} at the page \pageref{eq:1} an 
example of a power series was presented.


hello world \\ hello world2
$x=5$
$\frac{5}{6}$
$\dfrac{5}{6}$
\[111\]
$$121$$


\cite{bellet2013survey}
% \cite{baez/article}
% \cite{baez/online}
% \cite{wassenberg}
% An~example citation: \cite{Andel07}

\section{Title of the first subchapter of the first chapter}


\begin{figure}[ht]
    \centering
    \includegraphics[width=30mm]{img/logo-en}
    \caption{Logo of MFF UK}
    \label{fig:mff}
\end{figure}


 \begin{figure}[ht] \label{fig:foo}
 	\centering
 	% \hspace*{-10mm}
     %\fbox{
     \includegraphics[width=1.0\textwidth]{img/balance-scale}
     %}
     % trim={<left> <lower> <right> <upper>}
     \caption{Logo of MFF UK}
 \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1.0\textwidth]{img/dim-reduction/plotly}
%     \caption{Logo of MFF UK}
%     \label{fig:xxx}
% \end{figure}


% Example 1
\ldots when Einstein introduced his formula
\begin{equation}
e = m \cdot c^2 \; ,
\end{equation}
which is at the same time the most widely known
and the least well understood physical formula.
% Example 2
\ldots from which follows Kirchhoff’s current law:
\begin{equation}
\sum_{k=1}^{n} I_k = 0 \; .
\end{equation}
Kirchhoff’s voltage law can be derived \ldots
% Example 3
\ldots which has several advantages.
\begin{equation}
I_D = I_F - I_R
\end{equation}
is the core of a very different transistor model. \ldots

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Bibliography
\include{bibliography}

%%% Figures used in the thesis (consider if this is needed)
\listoffigures

%%% Tables used in the thesis (consider if this is needed)
%%% In mathematical theses, it could be better to move the list of tables to the beginning of the thesis.
\listoftables

%%% Abbreviations used in the thesis, if any, including their explanation
%%% In mathematical theses, it could be better to move the list of abbreviations to the beginning of the thesis.
\chapwithtoc{List of Abbreviations}

%%% Attachments to the master thesis, if any. Each attachment must be
%%% referred to at least once from the text of the thesis. Attachments
%%% are numbered.
%%%
%%% The printed version should preferably contain attachments, which can be
%%% read (additional tables and charts, supplementary text, examples of
%%% program output, etc.). The electronic version is more suited for attachments
%%% which will likely be used in an electronic form rather than read (program
%%% source code, data files, interactive charts, etc.). Electronic attachments
%%% should be uploaded to SIS and optionally also included in the thesis on a~CD/DVD.
\chapwithtoc{Attachments}

\openright
\end{document}
