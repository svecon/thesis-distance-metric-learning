%%% The main file. It contains definitions of basic parameters and includes all other parts.

%% Settings for single-side (simplex) printing
% Margins: left 40mm, right 25mm, top and bottom 25mm
% (but beware, LaTeX adds 1in implicitly)
\documentclass[12pt,a4paper]{report}
\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}
% \openright makes the following text appear on a right-hand page
\let\openright=\clearpage
\renewcommand{\baselinestretch}{1.2}

%% Settings for two-sided (duplex) printing
% \documentclass[12pt,a4paper,twoside,openright]{report}
% \setlength\textwidth{145mm}
% \setlength\textheight{247mm}
% \setlength\oddsidemargin{14.2mm}
% \setlength\evensidemargin{0mm}
% \setlength\topmargin{0mm}
% \setlength\headsep{0mm}
% \setlength\headheight{0mm}
% \let\openright=\cleardoublepage

%% Character encoding: usually latin2, cp1250 or utf8:
\usepackage[utf8]{inputenc}

%% Further useful packages (included in most LaTeX distributions)
\usepackage{amsmath}        % extensions for typesetting of math
\usepackage{amsfonts}       % math fonts
\usepackage{amsthm}         % theorems, definitions, etc.
\usepackage{bbding}         % various symbols (squares, asterisks, scissors, ...)
\usepackage{bm}             % boldface symbols (\bm)
\usepackage{graphicx}       % embedding of pictures
\usepackage{fancyvrb}       % improved verbatim environment
\usepackage{natbib}         % citation style AUTHOR (YEAR), or AUTHOR [NUMBER]
\usepackage[nottoc]{tocbibind} % makes sure that bibliography and the lists
			    % of figures/tables are included in the table
			    % of contents
\usepackage{dcolumn}        % improved alignment of table columns
\usepackage{booktabs}       % improved horizontal lines in tables
\usepackage{paralist}       % improved enumerate and itemize
\usepackage[usenames]{xcolor}  % typesetting in color

%%% User-defined packages
\usepackage{amssymb}
\usepackage[inline]{enumitem}
\DeclareMathOperator{\tr}{tr}
\usepackage{adjustbox}
\usepackage{dcolumn}
\usepackage{float}
\usepackage[section]{placeins}
% \usepackage{capt-of}

\usepackage{acro}
\include{acronyms}
% http://ftp.cvut.cz/tex-archive/macros/latex/contrib/acro/acro_en.pdf
% \acsetup{first-style=short}


% define "struts", as suggested by Claudio Beccari in
%    a piece in TeX and TUG News, Vol. 2, 1993.
\newcommand\Tstrut{\rule{0pt}{2.6ex}}         % = `top' strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}}   % = `bottom' strut


%%% Basic information on the thesis
% Thesis title in English (exactly as in the formal assignment)
\def\ThesisTitle{Evolutionary Algorithms for Data Transformation}

% Author of the thesis
\def\ThesisAuthor{Ondřej Švec}

% Year when the thesis is submitted
\def\YearSubmitted{2017}

% Name of the department or institute, where the work was officially assigned
% (according to the Organizational Structure of MFF UK in English,
% or a full name of a department outside MFF)
\def\Department{Department of Theoretical Computer Science and Mathematical Logic}

% Is it a department (katedra), or an institute (ústav)?
\def\DeptType{Department}

% Thesis supervisor: name, surname and titles
\def\Supervisor{Mgr. Martin Pilát, Ph.D.}

% Supervisor's department (again according to Organizational structure of MFF)
\def\SupervisorsDepartment{Department of Theoretical Computer Science and Mathematical Logic}

% Study programme and specialization
\def\StudyProgramme{Computer Science}
\def\StudyBranch{Artificial Intelligence}

% An optional dedication: you can thank whomever you wish (your supervisor,
% consultant, a person who lent the software, etc.)
\def\Dedication{%
Dedication. %TODO
}

% Abstract (recommended length around 80-200 words; this is not a copy of your thesis assignment!)
\def\Abstract{%
Abstract. %TODO
}

% 3 to 5 keywords (recommended), each enclosed in curly braces
\def\Keywords{%
{key} {words} %TODO
}

%% The hyperref package for clickable links in PDF and also for storing
%% metadata to PDF (including the table of contents).
\usepackage[pdftex,unicode]{hyperref}   % Must follow all other packages
\hypersetup{breaklinks=true}
\hypersetup{pdftitle={\ThesisTitle}}
\hypersetup{pdfauthor={\ThesisAuthor}}
\hypersetup{pdfkeywords=\Keywords}
\hypersetup{urlcolor=blue}

% Definitions of macros (see description inside)
\include{macros}

% Title page and various mandatory informational pages
\begin{document}
\include{title}

%%% A page with automatically generated table of contents of the master thesis

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Guidelines

% Transformace dat jsou důležitou součástí strojového učení, která výrazně ovlivňuje kvalitu vytvořených modelů. Transformace dat se často používají i pro jejich zobrazení do prostoru s menší dimenzí, kde se dají snáze vizualizovat. Většina metod ale funguje bez učitele a tedy není schopna najít taková zobrazení dat, která by brala v úvahu vlastnosti metod strojového učení, které následují po ní, případně se snažila vizualizaci upravit tak, aby data ze stejné třídy byla blízko u sebe. Některé metody určené přímo pro vizualizaci (jako např. t-SNE) navíc ani neposkytují transformační funkci a nejsou tedy schopny zobrazit nová data bez přepočítání celého zobrazení. Cílem práce je tedy pomocí kombinace evolučních algoritmů a dalších přístupů (např. neuronových sítí) navrhnout metody pro vytvoření transformační funkce, která bude brát v úvahu i označkování dat. 

% Student se seznámí s postupy pro automatické transformace dat. Na základě zjištěných informací implementuje vlastní metody a porovná je s existujícími přístupy. Součástí srovnání bude i vhodnost použité metody pro zobrazení dat do prostoru s malou dimenzí vhodnou pro vizualizaci.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{preface}
%\include{chap01_competition}
%\include{chap01}
%\include{chap02}
%\include{chap03_mahalanobis}
%\include{epilog}

\chapter{Introduction} \label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

A metric is ubiquitous in machine learning. For instance, in classification, \ac{knn} classifier (@REF) uses a metric to find the nearest neighbours. In clustering, many clustering algorithms, including wildly used k-Means (@REF), calculate distance between data points and groups similar instances together. In information retrieval, when searching for the most relevant document, the documents themselves are ranked according to their similarity scores. Also security systems use a metric for face verification. 

It is obvious that performance of all these algorithms depends on the quality of the metric defined on the input space. The better the metric judges how close or how similar any two instances in the space are, the more the final machine algorithm will behave.

Several general purpose metrics exist, such as Euclidean distance, defined in equation \ref{eq:euclid}, or its generalized Minkowski distance, defined in equation \ref{eq:minkowski}, and cosine similarity, defined in equation \ref{eq:cosine} used for vectors of real numbers, or Levenshtein distance (also called edit distance) for measuring distances between strings.

% https://numerics.mathdotnet.com/distance.html
\begin{align}
d_{\mathbf{2}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_2 = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2} \label{eq:euclid} \\
d_{\mathbf{p}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_p = \bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p} \label{eq:minkowski} \\
sim(\textbf{x}, \textbf{y}) &= \cos{\phi} = \frac{\textbf{x}\textbf{y}}{\|\textbf{x}\|\|\textbf{y}\|} \label{eq:cosine}
\end{align}

These metric, however, do not take any advantage of a prior knowledge of the task being solved or the structure of the data. Improved results are expected when the metric is designed for a particular task. Nevertheless, manually hand-crafting a metric is a strenuous and tedious process, which requires an expert knowledge of the task and the data. This has led to an introduction of distance metric learning where we learn distance metrics or similarities from data themselves.

Therefore the goal of metric learning is to adapt some pairwise metric function to the task at hand by using the training examples. This process requires almost no prior knowledge about the data is very fast compared to hand-crafting a metric function, which may take months of experimenting. That is why metric learning algorithms that learn a distance metric or similarity directly from the data have been getting more and more traction in the recent years.

The origins of metric learning can be traced back to \cite{short1981optimal}, however the first paper that received a lot of attention was \cite{xing2002distance} where the metric learning is formulated as a convex optimization problem. Since then metric learning has been getting increasing amount of attention and appeared at conferences ICML 2010\footnote{\url{http://www.icml2010.org/program.html}}, ECCV 2010\footnote{\url{http://www.ics.forth.gr/eccv2010/tutorials.php}} and workshops at ICCV 2011\footnote{\url{http://www.iccv2011.org/authors/workshops}}, NIPS 2011\footnote{\url{https://nips.cc/Conferences/2011/Schedule}} and ICML 2013 \footnote{\url{http://icml.cc/2013/?page_id=41}}.

Many researches tackle metric learning by adapting a Mahalanobis distance metric first described in \cite{mahalanobis1936generalized} in \ref{eq:maha:orig}, where $\bm{\Sigma}$ is a covariance matrix of the data defined in equation \ref{eq:cov}. However this name is now overloaded and Mahalanobis distance now stands for any metric parametrized by a matrix $\bm{M}$ as defined in equation \ref{eq:maha}.

\begin{align} \label{eq:maha:orig}
d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^T\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})}  \\
\bm{\Sigma} &= \mathrm {E} \left[\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)^{\rm {T}}\right] \label{eq:cov}
\end{align}

\begin{equation} \label{eq:maha}
d_{Mahalanobis}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^T\bm{M}(\textbf{x}-\textbf{y})} 
\end{equation}

\section{Mahalanobis metric}
% http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html

The problem with Euclidean distance is that it does not account for variances in different directions and correlations between pairs of dimensions in the data. For normally distributed data, the distance from the mean can be specified by computing the z-score, defined as $z = \frac{x-\mu}{\sigma}$, where $\mu$ is the population mean and $\sigma$ is the population standard deviation. The z-score of $x$ can be looked at as a dimensionless quantity and can be interpreted as the number of standard deviations of $x$ away from the mean.

The figure \ref{fig:corrdata} demonstrates this problem in two dimensions: the figure visualises a dataset with both dimensions highly correlated and with the first dimension highly variable. In the plot the coloured ellipses show the probability density of the Gaussian mixture model. The probability density is high for ellipses near the origin and low for ellipses further away. In the figure there is $[0,0]$ (origin) and two other red points: $[0,1]$ and $[2,0]$. In Euclidean distance the point $[0,1]$ is twice closer to the origin compared to the point $[2,0]$. However, because the first axis has larger variance, it is clear that seeing point $[2,0]$ is much more probable in our dataset than it is seeing point $[0,1]$ because the point $[2,0]$ lies on the edge of the first ellipsis, meanwhile point $[0,1]$ lies beyond the edge of the second ellipsis. Using the terminology of the z-score, the point $[0,1]$ is more standard deviations away from the origin than the point $[2,0]$.

\cenfig{graphs/correlated_data}{Correlated data centered around origin}{fig:corrdata}

% http://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html
The data can be uncorrelated using the covariance matrix $\bm{\Sigma}$, which is a \ac{psd} matrix and therefore the Cholesky decomposition $\bm{\Sigma}=\bm{L}\bm{L}^T$ can be used to obtain a lower triangular matrix $\bm{L}$. The inverse of this triangular matrix $\bm{L}$ can be used to remove the correlation from the data defined by $\bm{\Sigma}$. The figure \ref{fig:uncorrdata} demonstrates the effect of multiplying the original data by $\bm{L}^{-1}$. In this figure it is now clear that the point at original coordinates $[2,0]$ is much closer to the origin in Euclidean distance than the the point with original coordinates $[0,1]$.

\cenfig{graphs/uncorrelated_data}{Uncorrelated data using Cholesky decomposition of covariance matrix}{fig:uncorrdata}

This problem of correlation between dimensions does not arise in the Mahalanobis distance \eqref{eq:maha:orig} because it decorrelates the data intrinsically before calculating the distance itself. If the Cholesky decomposition is applied directly inside the equation \ref{eq:maha:orig}, we get that the Mahalanobis distance of the original data is equal to the Euclidean distance of the decorrelated data as shown in \ref{eq:mahalanobis:decorrelation}.

\begin{align}
  d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}(\bm{L}\bm{L}^{T})^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{-T}\bm{L}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})^{T}(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}^{-1}\textbf{x}, \bm{L}^{-1}\textbf{y}) \label{eq:mahalanobis:decorrelation}
\end{align}

Therefore from the \ref{eq:mahalanobis:decorrelation} it is possible to see the Mahalanobis distance has the following properties:
\begin{enumerate}
\item it accounts for the variances in each direction
\item it accounts for the covariance between variables
\item it reduces to the Euclidean distance for uncorrelated data with unit variance
\end{enumerate}

Now consider the generalized form of the Mahalanobis distance defined in equation \ref{eq:maha}. For what matrices $\bm{M}$ does the equation remain a distance metric? From a mathematical perspective, a metric is a function $d(x,y)$ that defines a distance, similarity or dissimilarity between two instances from some input space: $d:\mathcal{X} \times \mathcal{X} \mapsto [0,\inf)$. For a function to be a metric it has to follow 4 conditions: nonnegativity, identity of indiscernibles, symmetry and triangle inequality as defined in equations \ref{eq:metricdef-1} --- \ref{eq:metricdef-4}.

% https://en.wikipedia.org/wiki/Metric_(mathematics)
\begin{align}
d(x,y) &\geq 0 & nonnegativity \label{eq:metricdef-1} \\
d(x,y) &= 0 \iff x=y & identity \ of \ indiscernibles \label{eq:metricdef-2} \\
d(x,y) &= d(y,x) & symmetry \label{eq:metricdef-3} \\
d(x,z) &\leq d_M(x,y) + d_M(y,z) & triangle \ inequality \label{eq:metricdef-4}
\end{align} 

In order for the Mahalanobis metric from equation \eqref{eq:maha} to be a metric, the matrix $\bf{M}$ that parametrizes the distance needs to pass the metric conditions \ref{eq:metricdef-1} --- \ref{eq:metricdef-4}. It turns out that a strictly \ac{pd} matrices $\bm{M} \succ 0$ do pass the conditions and therefore the Mahalanobis distance with a \ac{pd} matrix is a distance metric.

Sometimes, however, it is not easy to guarantee the positive definiteness of a matrix and we have to settle with a \ac{psd} matrix $\bf{M} \succeq 0$. For \ac{psd} matrices the identity of indiscernibles condition \ref{eq:metricdef-2} is not met and therefore the Mahalanobis distance with a \ac{psd} matrix is not a metric. For this reason the condition is usually relaxed into a condition of identity \ref{eq:pseudodef-2}. Together with the rest of the conditions, this forms a new set of conditions \ref{eq:pseudodef-1} ---\ref{eq:pseudodef-4}, which together defines a pseudo-metric. Therefore the Mahalanobis distance with a \ac{psd} matrix is a pseudo-metric.

%TODO fix corrent formatting for letters
\begin{align}
d(x,y) &\geq 0 & nonnegativity \label{eq:pseudodef-1} \\
d(x,x) &= 0 & identity \label{eq:pseudodef-2} \\
d(x,y) &= d(y,x) & symmetry \label{eq:pseudodef-3} \\
d(x,z) &\leq d_M(x,y) + d_M(y,z) & triangle \ inequality \label{eq:pseudodef-4}
\end{align} 

For the generalized Mahalanobis distance the same trick from the \ref{eq:mahalanobis:decorrelation} can be used for any general matrix $\bm{M}$ as can be seen in \ref{eq:mahalanobis:transform}. Cholesky decomposition is defined for any \ac{psd} matrix: $\bm{M}=\bm{L}\bm{L}^T$. Thus every Mahalanobis distance with a \ac{psd} matrix can be instead replaced by some linear transformation using a matrix $\bm{L}$ and then simply calculating the Euclidean distance.

\begin{align}
  d_{\bm{M}}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{T}\bm{L}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}\textbf{x}-\bm{L}\textbf{y})^{T}(\bm{L}\textbf{x}-\bm{L}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}\textbf{x}, \bm{L}\textbf{y}) \label{eq:mahalanobis:transform}
\end{align}

%TODO WHY DO WE NEED OTHER METRIC LEARNING ALGORITHMS? (THINK OF AN EXAMPLE)

%TODO use this 
% https://en.wikipedia.org/wiki/Mahalanobis_distance

% https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/PR_Mahal/M_metric.htm

%TODO FIX ALL THIS
% Úvod do problematiky
% - Co už se udělalo
% - Co je známo
% - Co není známo
% - Co a pro Co a proč chceme naší prací objasnit naší prací objasnit MOTIVATION
% - Jasná definice cíle práce GOALS
% - Struktura práce

% Related works
% - Co už udělali jiní
% - Co nového hodlá udělat autor

%TODO HAVENT USED THIS YET
% curse of dimensionality
% bad euclid distance

% In this thesis, we review existing state-of-the-art methods for learning global Mahalanobis distance metric (Xing, LMNN, NCA, ...) and compare them on several popular datasets. Learning the full Mahalanobis metric means learning $d^2$ parameters and so we also try to restrict the existing methods to learn only a diagonal of the Mahalanobis distance metric with $d$ parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methodology}

\section{Applications}

Looking at metric learning as a "data transformation/preprocessing".

Computer vision
Information retrieval
Bioinformatics

\section{Related Topics}

Kernel learning
Multiple kernel learning
Dimensionality reduction

\section{Key Properties of Metric Learning Algorithms}

Learning paradigms: supervised, semi-supervised, unsupervised

Form of metrics: global (linear), non-linear, local

Scalability

Optimality of a solution (global optimum, local optimum)

Dimensionality reduction

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evolution strategies} \label{chap:ea}
Quick intro

Describe EAs, ... crossover, mutation, selection (+schema)

\subsection{CMA-ES} \label{chap:ea:cmaes}
move to new chapter
\subsection{jDE} \label{chap:ea:jde}
move to related work


\section{Thesis structure}
%TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related work} \label{chap:rw}

[mention Bellet + make an outline + mention all possible metrics (online metrics)]

Metric learning has mostly been used to improve the performance of k-Nearest~Neighbour classification. Most of the research has been focused on learning this metric from labelled training instances, however researchers also focus on weakly-supervised tasks, where the labels of the instances are unknown, but the information about the data comes in the form of similar and dissimilar pairs:

\begin{align}
\mathcal{S} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be similar} \rbrace \\
\mathcal{D} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be dissimilar} \rbrace
\end{align}

Metric learning algorithm then tries to exploit these constraints to find the best parameters of a metric that best agrees with these constraints.

[Scalability in N and D] %TODO

In this work we chose to examine the following methods:

\begin{table}[ht] \centering
\caption{Summary of related metric learning methods} \label{tab:rw:summary}
\begin{tabular}{llllll}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Name & Year & Supervision & Optimum & Regularizer & Notes \\
\hline
MMC & 2002 & Weak & Global & None & — \\
LMNN & 2005 & Full & Global & None & For k-NN \\
NCA & 2004 & Full & Local & None & For k-NN \\
LFDA & 2007 & Full & Global & None & — \\
% RCA & 2003 & Weak & Global & None & — \\
% ITML & 2007 & Weak & Global & LogDet & Online version \\
% SDML & 2009 & Weak & Global & LogDet+L1 & $n \ll d$ \\
jDE.wFme & 2013 & Full & Local & None & Evolution \\
\hline
\end{tabular}
\end{table}

\cite{mahalanobis1936generalized}

\section{MMC Xing} \label{chap:rw:xing}
\cite{xing2002distance}

\section{Large Margin Nearest Neighbor (LMNN)} \label{chap:rw:lmnn}
\cite{weinberger2009distance}

% \section{Information Theoretic Metric Learning (ITML)} \label{chap:rw:itml}
% \cite{davis2007information}

% \section{Sparse Determinant Metric Learning (SDML)} \label{chap:rw:sdml}
% \cite{qi2009efficient}

% \section{Least Squares Metric Learning (LSML)} \label{chap:rw:lsml}
% \cite{liu2012metric}

% \section{Relative Components Analysis (RCA)} \label{chap:rw:rca}
% \cite{shental2002adjustment}

\section{Neighborhood Components Analysis (NCA)} \label{chap:rw:nca}
\cite{jacobgoldberger2004neighbourhood}

\section{Local Fisher Discriminant Analysis (LFDA)} \label{chap:rw:lfda}
\cite{sugiyama2007dimensionality}

\section{Evolutionary distance metric learning} \label{chap:rw:fukui}
\cite{fukui2013evolutionary}


- Teoretická analýza

Zmínit lokální metriky, online, ...

PCA is connected to Mahalanobis!!
% http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
% http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance/62147#62147
->
formally explain that evolving 2*D matrix corresponds to some eigen value decomposition???

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving metric evolution} \label{chap:our-method}

main difference: we evolve L (no adjustments needed!)
proof that $L^TL$ is PSD matrix  (using SVD)
% http://www.deeplearningbook.org/contents/linear_algebra.html

CMAES + kNN

full matrix, diagonal matrix, neural network shape

k-Means for dimensionality reduction

\section{CMAES strategy}

simple evolution, jDE, CMAES, ...

\section{k-Nearest~Neighbour fitness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Dimensionality Reduction} \label{chap:dim:reduction}

\section{Linear}

\section{Neural network transformation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments and Examples}
We performed four different experiments to assess the performance of metric learning algorithms. In section \ref{chap:exp:classification} we use learnt metric in k-Nearest~Neighbour classificator and compare classification errors on various datasets, which are described in \ref{chap:exp:datasets}. In the next experiment in section \ref{chap:exp:fitness} we compare different evolutionary algorithms together with various fitnesses and show how well they generalize. In section \ref{chap:exp:runtimes} we measure the training times of the algorithms and in the last experiment in section \ref{chap:exp:dimred} we use metric learning algorithms for dimensionality reduction and we compare the resulting embeddings.

\section{Experimental settings} 
In our experiments we compare metric learning algorithms listed in section \ref{chap:rw}. Most of these algorithms are implemented in an open-sourced Python \textit{metric-learn} library \footnote{\url{https://github.com/all-umass/metric-learn}}. Only our evolutionary method described in section \ref{chap:our-method} and \cite{fukui2013evolutionary} are missing in this library. Therefore we designed a modular interface and implemented these two methods as a part of the \textit{metric-learn} library. The~implementation is described in section \ref{chap:impl}.

\subsection{Datasets} \label{chap:exp:datasets}
We chose to experiment on classification datasets that were most common among all related works, particularily \cite{xing2002distance}, \cite{weinberger2009distance}, \cite{jacobgoldberger2004neighbourhood} and \cite{fukui2013evolutionary}. We found that most common among all these papers was balance-scale, breast-cancer, iris, mice-protein, pima-indians, sonar and wine dataset. All these datasets were obtained from the well-known UCI Machine Learning Repository \footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Even among these datasets mice-protein and sonar  have high dimensionality, however they are small in terms of number of samples.

We also added digits6 and digits10 datasets, which are relatively larger datasets containing $8\times 8$ pixel images of digits with 6 and 10 classes respectively. These datasets were obtained using Python library Scikit-Learn.org \footnote{\url{http://scikit-learn.org/stable/datasets/}}.

In order to test the metric learning algorithms on a highly variable data, we created an artificial dataset of four multinomial Gaussians. Each Gaussian has a different center, defined as a matrix in equation \eqref{eq:gauss:means} where each row corresponds to one center. All Gaussians were generated sharing one covariance matrix, defined in equation \eqref{eq:gauss:cov}. Dimensions are uncorrelated, however the first dimension has an enormous variability $10^8$.

\begin{equation} \label{eq:gauss:means}
means = \begin{pmatrix}
10 & 0 & 0 & 2 \\
0 & 10 & 0 & -2 \\
0 & 0 & 10 & -2 \\
0 & 0 & -10 & 2 \\
\end{pmatrix}
\end{equation}
\begin{equation} \label{eq:gauss:cov}
covariance = \begin{pmatrix}
10^8 & 0 & 0 & 0 \\
0 & 100 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}

Table \ref{tab:datasets} summarizes datasets that we experimented on. The table shows number of samples, dimensionality and number of distinct classes for every dataset. The largest dataset in terms of number of samples and number of classes is digits10 dataset with 1797 samples. However the largest dataset in terms of number of dimensions is mice-protein dataset.

\begin{table}[ht] \centering
\begin{tabular}{lrrr}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Dataset & \#~samples & \#~dimensions & \#~classes \\
\hline
balance-scale           & 625   & 4    & 3  \\
breast-cancer           & 699   & 9    & 2  \\
digits6                 & 1083  & 64   & 6  \\
digits10                & \textbf{1797}  & 64  & \textbf{10} \\
gaussians               & 400   & 5   & 4  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & \textbf{77}   & 8  \\
pima-indians            & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
wine                    & 178   & 13   & 3  \\
%soybean-large           & 307   & 35   & \textbf{19} \\
%ionosphere              & 351   & 34   & 2  \\
%letters                 & 20000 & 16   & 26 \\
%mnist                   & 70000 & 784  & 10 \\
%ofaces                  & 400   & 4096 & 40 \\
\hline
\end{tabular}
\caption{Datasets overall summary} \label{tab:datasets}
\end{table}

[Short info about all datasets we used? Multivariate, Real / Integer / Categorical, Missing data? %TODO

\subsection{Preprocessing data} \label{chap:exp:preprocessing}
Some of the datasets have missing attributes. There are several strategies for dealing with missing attributes, such as removing affected samples, filling missing values with a random value or zeros, their mean, median or the most frequent value. We chose to fill missing values using the mean for any given attribute as we found it behaves well with all chosen datasets.

\begin{table}[ht] \centering
\begin{tabular}{lrrrrr}
\hline
Dataset & minimum & maximum & mean & std. deviation \\
\hline
balance-scale           & 1.00  & 5.00    & 3.00  & 1.41 \\
breast-cancer           & 1.00  & 10.00   & 3.13  & 2.88 \\
digits6                 & 0.00  & 16.00   & 4.87  & 6.04 \\
digits10                & 0.00  & 16.00   & 4.88  & 6.02 \\
gaussians               & \textbf{-33870.07} & \textbf{31033.73} & -7.65 & \textbf{5285.16} \\
iris                    & 0.10  & 7.90    & 3.46  & 1.97 \\
mice-protein            & -0.06 & 8.48    & 0.68  & 0.79 \\
pima-indians            & 0.00  & 846.00  & 44.99 & 58.37 \\
sonar                   & 0.00  & 1.00    & 0.28  & 0.28 \\
wine                    & 0.13  & 1680.00 & \textbf{69.13} & 215.75 \\
\hline
\end{tabular}
\caption{Datasets samples statistics} \label{tab:datasets-samples}
\end{table}

Another difficulty with the datasets is that many of them do not have normalized attributes and their attribute values are unbounded and highly variable. From table \ref{tab:datasets-samples} it is clear that the most variable dataset is our artificial gaussians dataset, however pima-indians and wine datasets are also highly variable with a standard deviation of $58.37$ and $215.75$ respectively. The~datasets were normalized using standardization defined in equation \ref{eq:stand}, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations.

\begin{equation} \label{eq:stand}
\hat{X} = \frac{X-\mu}{\sigma}
\end{equation}

\section{Experiment: Classification} \label{chap:exp:classification}

In this experiment we wanted to see how much does learnt metric help in classification task compared to standard Euclidean distance. We chose \ac{knn} classifier, which is one of the simplest and yet powerful classifiers and it is very easy to modify to use a custom metric.

[list of methods that we used in the experiment] %TODO

To assess performance of the metric learning algorithms we used datasets listed in section \ref{chap:exp:datasets} and used 10-fold cross validation for each algorithm. For each fold data were first preprocessed and standardized as described in section \ref{chap:exp:preprocessing}, then the metric was trained using one of the methods and finally the metric was evaluated using kNN algorithm.

We also wanted to test, how does standardization influence the learnt metrics and whether the metric learning algorithms can handle unnormalized data and so we also tried all these experiments again, but without standardizing the data.

\subsection{Hyperperameter search} \label{chap:exp:hypsearch}

Every metric learning algorithm has a set of hyperparameters which highly influences the learnt metric. For each hyperparameter we selected sensible values from a reasonable range. All hyperparameters for each algorithm are described in table \ref{tab:hyperparams}. The performance of kNN classifier hugely depends on its number of neighbours hyperparameter. We chose to try values 1, 2, 4, 8, 16, 32, 64, 128 for this parameter.

\input{results/hyperparameters}

Even for these sensibly chosen hyperparameter values there are thousands of possible combinations for every single algorithm and thus we have to fit thousands of different models. Let’s consider LMNN algorithm: there are 10 folds, 8 possible values for the final kNN classifier hyperparameter, LMNN itself has 4 hyperparameters with 6, 3, 4 and 3 values, which means there would be $10*8*(6*3*4*3)=17280$ different models to train just for LMNN algorithm if a simple grid search was used. This is a major problem because some of the algorithms take a long time to train even on small datasets.

We can notice, however, that we do not need to retrain our metric for every hyperperameter of the final kNN classifier. Instead we can train a metric model first and then train and test kNN classifier with all hyperparameter options. Therefore, we designed our version of grid search that is hierarchical in a sense that it will train a metric with all combinations of hyperparameters first and only then it will evaluate these metrics using kNN with its own hyperparameters. There will still be the same total number of models, however there will be 8 times less evaluations of every metric learning algorithm, which roughly translates to 8 times less computation time as training kNN classifier is negligible compared to a metric model. This can be looked at as a simple space-time trade-off.

%TODO add a sketch of hierarchical grid search

\subsection{Results}

How hyperparameters influence the successrate. [graphs] %TODO

Results discussion %TODO

\input{results/successrates}

[Comparison to unstandardized data]
can metric learning replace standardization? %TODO
comparison discussion... %TODO
[table of error rates] %TODO
side by side successrate graphs %TODO

\cenfig{graphs/classification/sr.pdf}{Boxplots of successrates from 10-fold crossvalidation}{fig:cl:sr}

\cenfig{graphs/classification/sr_knn}{Successrates of individual algorithms when fixing `k` in the final kNN classifier}{fig:cl:kpar}

\cenfig{graphs/classification/sr_hyp}{Successrates of individual algorithms when fixing some of their hyperparameters}{fig:cl:hyp}

\section{Experiment: Generalization of EAs} \label{chap:exp:fitness}

To test out how well do solutions from evolution algorithms evolve and generalize we performed another series of tests. Using CMAES and jDE evolution strategies described in sections \ref{chap:ea:cmaes} and \ref{chap:ea:jde}, combined with k-Nearest~Neighbour (kNN) and weighted~F-measure (wFme) fitnesses described in sections @WHERE and \ref{chap:rw:fukui} respectively. Both strategies with both fitnesses were tested, giving us 4 different models: CMAES.kNN, CMAES.wFme, jDE.kNN and jDE.wFme where the first part until the dot describes evolution strategy used in the algorithm and the part after the dot describes its fitness function.

All methods have some hyperparameters and in order to avoid the time consuming hyperparameter search, the hyperparameters for the models in this experiment were picked according to results from the previous experiment. Only one set of hyperparameters was picked for all datasets. The number of nearest neighbours in the kNN classifier used in the fitness function was chosen to be 8 and the number of nearest neighbours in the final kNN classifier was set to 4. All methods were evolved for a full thousand generations. Early stopping does not make sense in this experiment because the learnt metric is evaluated after every single generation, compared to the classification experiment where more generations could overfit the testing error and therefore the produced metric would not generalize well.

[full vs diagonal!?]
[some datasets too large]

Datasets described in section \ref{chap:exp:datasets} were used as well for this experiment. Data were preprocessed and standardized the same way as previous experiment, both methods are described in section \ref{chap:exp:preprocessing}. Data were split into training and testing sets using a 67:33 ratio in a stratified fashion, keeping the sizes of classes balanced between the two splits.

To gauge the performance of the fitness function we logged the fitness values of all individuals in every generation and then calculated median, minimum, maximum, 25th and 75th percentiles in each generation.

To see how well the individuals generalize, individuals with the best and the worst fitness values were picked in every generation and the metric encoded in these individuals was evaluated using the test set using kNN classifier.

\subsection{Results}

Both kNN and wFme fitness functions output values in a closed range $[0,1]$. The final kNN classifier used to gauge performance outputs values in the same exact range and so we plot all values in the same graph. Therefore the graph for each generation shows minimal and maximal (both in orange) fitnesses, median fitness (in green) with the area between 25th and 75th percentiles coloured with light green. The test successrate of the individual with the highest fitness in XX and the test successrate of the individual with the lowest fitness in XX.

[each dataset has 4 graphs] %TODO

[explain the graphs] %TODO

[discuss results] %TODO

\cenfig{graphs/fitness/balance-scale}{balance-scale}{balance-scale}
\cenfig{graphs/fitness/breast-cancer}{breast-cancer}{breast-cancer}
\cenfig{graphs/fitness/digits10}{digits10}{digits10}
\cenfig{graphs/fitness/digits6}{digits6}{digits6}
\cenfig{graphs/fitness/gaussians}{gaussians}{gaussians}
\cenfig{graphs/fitness/iris}{iris}{iris}
\cenfig{graphs/fitness/mice-protein}{mice-protein}{mice-protein}
\cenfig{graphs/fitness/pima-indians}{pima-indians}{pima-indians}
\cenfig{graphs/fitness/sonar}{sonar}{sonar}
\cenfig{graphs/fitness/wine}{wine}{wine}


\section{Experiment: Runtimes} \label{chap:exp:runtimes}

\begin{tabbing}
\hspace{50pt}\=\kill
$N$ \> number of samples \\
$D$ \> number of attributes \\ 
$G$ \> number of generations (200) \\
\end{tabbing} 

$\mathcal{O}(N^{2.376})$ matrix multiplication 

\begin{table}[ht] \centering
\begin{tabular}{rll}
\hline
Method & full matrix & diagonal matrix \\
\hline
Covariance & $\mathcal{O}(ND^2)$ & - \\
LMNN & $i*\mathcal{O}()$ & - \\
NCA & $i*\mathcal{O}(N^2D^2)$ & - \\
LFDA & $\mathcal{O}(N^2D)$ & - \\
CMAES.kNN & $\mathcal{O}()$ & - \\
CMAES.fMe & $\mathcal{O}()$ & - \\
jDE.kNN & $\mathcal{O}()$ & - \\
jde.fMe & $\mathcal{O}()$ & - \\ 
\hline
\end{tabular}
\caption{XXX} \label{tab:XXX}
\end{table}


jDE
$P=10*D^2$ population size
$\mathcal{O}(GP*(D^2+F_{eval}))$ for full
$\mathcal{O}(GP*(D+F_{eval}))$ for diagonal

CMAES
$P=4*3\log{D^2}$ population size
$\mathcal{O}(G*(PD+PD^2+PF_{eval}+D+D^2+D^3))$ for diagonal
% generate pop PD, PD^2
% update D, D^2
% eigen decomp D^3

Fitness
$N$ split
$D^2$ fit transformer (for full)
$ND^2$ transform samples (for full)

kNN
$N\log{N}$ build
$N^{1-\frac{1}{D}}+k$ retrieve

wFme ??

% http://stackoverflow.com/questions/9146086/time-complexity-of-genetic-algorithm
% Genetic Algorithms are not chaotic, they are stochastic. The complexity depends on the genetic operators, their implementation (which may have a very significant effect on overall complexity), the representation of the individuals and the population, and obviously on the fitness function. Given the usual choices (point mutation, one point crossover, roulette wheel selection) a Genetic Algorithms complexity is O(g(nm + nm + n)) with g the number of generations, n the population size and m the size of the individuals. Therefore the complexity is on the order of O(gnm)). This is of cause ignoring the fitness function, which depends on the application.

Runtime of each method is an important factor to considered. Some methods are not very scalable for large datasets as already discussed in (@WHERE). As already mentioned (@WHERE) it is not easy to describe most of metric learning algorithms in big O notation because most of them are iterative and therefore it is not clear how many iterations is needed. Plus they can still end up in some local minima.

In table \ref{tab:runtimes} we show run times of every method for the best hyper-parameter combination, which corresponds to best error rate for given dataset as shown in table \ref{tab:errors-small}. Euclidean distance does not need any metric training and therefore has a zero runtime in the table. Covariance metric is the seconds fastest metric, closely followed by LFDA and RCA metrics. On the other hand, LMNN and NCA metrics and all evolution metrics are the slowest methods. NCA did not even finish on mice-protein, pima-indians, ionosphere and wine datasets.

%TODO fix missing values
\input{results/runtimes}

%TODO add graph of dimensionality increase
%TODO add graph of number of samples increase

\section{Experiment: Dimensionality reduction} \label{chap:exp:dimred}

Metric learning can also be used for dimensionality reduction as discussed in @WHERE. This experiment was focused on dimensionality reduction into two dimensions where it is easy to visualise the data.

One of the most common and the simplest unsupervised dimensionality reduction method is Principal Component Analysis @REFERENCE (PCA). PCA picks the dimensions of the data with the highest variance. This corresponds to finding the eigenvalues and eigenvectors of the covariance matrix, the eigenvectors with the largest eigenvalues correspond to the dimensions that have the strongest correlation in the dataset.

[Is Covariance Mahalanobis === PCA??]

LFDA uses the same trick for dimensionality reduction, it calculates the metric matrix and then picks the columns of this matrix with the largest corresponding eigenvalues.

\cenfig{graphs/dimred/gaussians}{Dimensionality reduction of multivariate Gaussian dataset using different algorithms}{fig:dimred:gauss}

See fig \ref{fig:dimred:gauss}

\subsection{Direct comparison with other methods}

In this experiment we compare PCA, LFDA, CMAES.kNN methods for dimensionality reduction into two dimensions.


\subsection{Neural Network transformation}

...

\subsection{Preprocessing for t-SNE}

t-SNE \cite{maaten2008visualizing} is another unsupervised dimensionality reduction method which is getting a lot of attention recently. Compared to PCA, t-SNE has a non-convex objective function and is optimized using gradient descend, which means it may end up in a local minimum. .... recommended to run more times.

[add how tsne works?]

In this experiment we compare pure t-SNE with

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation} \label{chap:impl}

\section{Hierarchical Grid search}

how it works - scheme

how cross-validation is in memory

\section{Metric learning in metric\_learn package}

Description of modules, fitnesses, transformers, ...

\begin{figure}[h!] \label{fig:implementation-modules}
	\centering
    \includegraphics[width=0.2\textwidth]{img/notfound}
    \caption{Interaction between modules for Metric evolution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}

Graphs, performace, caveats, what worked, what didnt work...

LMNN, ITML, SDML take too long to calculate

LMNN and LFDA perform very well

LMNN has too many hyper parameters

Simple Covariance metric was impossible to calculate on certain datasets

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

This worked, this didn't work. What we did.

\chapter*{Future work}

What wasn`t finished and should be.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Bibliography
\include{bibliography}

%%% Figures used in the thesis (consider if this is needed)
\listoffigures

%%% Tables used in the thesis (consider if this is needed)
%%% In mathematical theses, it could be better to move the list of tables to the beginning of the thesis.
\listoftables

%%% Abbreviations used in the thesis, if any, including their explanation
%%% In mathematical theses, it could be better to move the list of abbreviations to the beginning of the thesis.
\chapwithtoc{List of Abbreviations}

\printacronyms[include-classes=abbrev,heading=none] % ,name=Abbreviations

\chapwithtoc{List of Notations}
\input{notation}

%%% Attachments to the master thesis, if any. Each attachment must be
%%% referred to at least once from the text of the thesis. Attachments
%%% are numbered.
%%%
%%% The printed version should preferably contain attachments, which can be
%%% read (additional tables and charts, supplementary text, examples of
%%% program output, etc.). The electronic version is more suited for attachments
%%% which will likely be used in an electronic form rather than read (program
%%% source code, data files, interactive charts, etc.). Electronic attachments
%%% should be uploaded to SIS and optionally also included in the thesis on a~CD/DVD.
\chapwithtoc{Attachments}

\openright
\end{document}
