\subsection{Dimensionality Reduction}
\label{chap:exp:dimred}

In this experiment we compare PCA and t-SNE methods with DML methods supporting dimensionality reduction: NCA, LFDA and CMAES.kNN. We also experimented with evolving fully-connected neural networks for dimensionality reduction using CMAES.kNN. We evolved six different neural networks differing in depth (either one or five hidden layers) and their activation functions (sigm, tanh or ReLU). The shallow networks have one hidden layer with 4 neurons and the deep networks have 16-12-8-6-4 neurons in the hidden layers. To reduce the dimension into 2-dimensional space, all networks always end with an output layer of size 2. The shallow network therefore has $4d+8$ free parameters and the deeper network has $16d+354$ free parameters. No biases were used.

PCA, LFDA, NCA and CMAES.kNN methods are linear and so the dimensionality reduction works by projecting the data onto the 2-dimensional space by a matrix multiplication. The methods only differ in how to find this transformation matrix. We were also interested in whether the CMAES would help t-SNE making a better visualization if used as a preprocessing and we call this method CMAES.kNN+t-SNE, which first trains CMAES.kNN, the data are transformed using the evolved transformation matrix while keeping the original dimensionality, and only then the t-SNE is used to transform the preprocessed data from CMAES.kNN into two dimensions.

The dataset is split to training set and test set using a 10-fold cross validation. The algorithms are trained using the training set. Then the trained algorithms are used to transform both train and test sets into two dimensions. Then, we fit the $k$-NN classifier using this 2-dimensional training set and calculate a success rate on the test set. The idea is that the $k$-NN classifier has a large accuracy if instances from the same class are close together, which indicates a good dimensionality reduction.

We also include the t-SNE in the visualizations, however, as it is a transductive method method, it cannot be used to classify new data. Therefore in this case, the $k$-NN classifier is fit on all the data and the numbers may be overly optimistic. The main reason for including t-SNE is to show how the visualization from CMAES.kNN compare to the popular visualization method.

The results are presented in Table \ref{tab:dim-error-rates}. We show the error rates and standard deviations for each method on each dataset from the 10-fold cross validation. The best inductive method (i.e. of those that do not use t-SNE) for every dataset is in a bold font. Generally, CMAES.kNN works very well for dimensionality reduction and also very fast because only a $d\times2$ matrix is evolved. CMAES.kNN has the lowest error only on \textit{pima-indians}, however the difference between this method and the best method on each dataset is never statistically significant. t-SNE provides nice visualizations for the large datasets, namely \textit{digits6} and \textit{digits10} and the preprocessing with CMAES.kNN helps t-SNE on the majority of the datasets. LFDA and NCA have both also very good results, but NCA runs out of memory on three datasets (marked with "Memory" in the results table).

\input{results/dim-red}

The success rate, however, is only one of the factors that affect good visualization and therefore we also check the projections visually and evaluate the following criteria: the instances with the same label are together; the clusters are well separated; clusters form small blobs and they are not spread out; all instances with the same class are in one big cluster. Due to space limitations, we show the visualizations only for the digits10 and pima-indians datasets. The visualization for the rest of the datasets in available in the supplementary materials. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualized. 

On the \textit{digits10} dataset (Figure~\ref{fig:dimred:digits10}), the t-SNE visualization dominates other methods. Nevertheless, CMAES.kNN+t-SNE still improves the visualization by joining some clusters together. PCA algorithm finds some clusters, but they are merged together and not well separated. CMAES.kNN works better compared to PCA, it manages to separate the clusters more and the clusters themselves are also more uniform. LFDA method fails on this dataset completely, it spreads out just few data instances, but keeps the rest together in one dense cluster. NCA method runs out of memory on this datasets. The \textit{pima-indians} dataset (Figure \ref{fig:dimred:pima-indians}) is hard to visualize, and there is not a clear winner, but NCA, LFDA, CMEAS.kNN and CMAES.kNN+t-SNE were the best at separating the two classes from each other.

Interestingly, the neural networks with sigm and tanh activations tend to create interesting, almost cube-like structures, meanwhile ReLU activation creates more of a light torch structure, where many instances are clustered in a center and the rest is spread out in one major direction. We also found out that sometimes the simpler models (shallow networks, on only a linear mapping) often provide better results than the deeper neural networks. We believe one of the reasons may be the higher number of parameters in the deeper network which means it would take longer to tune them. Increasing the number of generations may a way to improve the performance of the deeper models, however, the training would also take considerably longer.

