\section{Experiment Description}

We performed two experiments to assess the performance of the algorithm presented in this work. In Section \ref{chap:exp:classification} we learn a metric for the $k$-NN classifier and compare classification errors on various datasets. This experiment also serves as a comparison to jDE, which is the most closely related method from the literature. In Section \ref{chap:exp:dimred} we present the main experiment, which shows the dimensionality reduction and visualization obtained by the CMAES.kNN method.

In the experiments, we compare some of the state-of-the-art metric learning algorithms (LMNN, NCA, LFDA). We use their implementation from the Python \textit{metric-learn} library\footnote{\url{https://github.com/all-umass/metric-learn}}. We implemented the jDE-based method similar to~\cite{fukui2013evolutionary} and our algorithm and the implementation is available as a supplementary material and also at GitHub\footnote{\url{https://github.com/svecon/metric-learn/tree/ecml2017}}.

\subsection{Datasets} \label{chap:exp:datasets}

For the experiments we used dataset that were most commonly used in the literature \cite{xing2002distance}, \cite{weinberger2009distance}, \cite{jacobgoldberger2004neighbourhood} and \cite{fukui2013evolutionary}. These are balance-scale, breast-cancer, iris, mice-protein, pima-indians, sonar and wine dataset from the well-known UCI Machine Learning Repository\footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Most of the datasets are rather small, therefore, we added two larger datasets -- digits6 and digits10.

We also created an artificial dataset by sampling from four multinomial Gaussian distributions in order to evaluate the algorithm on highly variable data. Each Gaussian has a different center, defined as a matrix in Equation \eqref{eq:gauss} where each row corresponds to one center. All Gaussians were generated sharing one covariance matrix, also defined in Equation \ref{eq:gauss}. Dimensions are uncorrelated, however the first dimension has a large variability of $10^8$.

\begin{equation} \label{eq:gauss}
centers = \begin{pmatrix}
10 & 0 & 0 & 2 \\
0 & 10 & 0 & -2 \\
0 & 0 & 10 & -2 \\
0 & 0 & -10 & 2 \\
\end{pmatrix}, \,
covariance = \begin{pmatrix}
10^8 & 0 & 0 & 0 \\
0 & 100 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}
Table \ref{tab:datasets} summarizes datasets that we experimented on. The table shows number of samples, dimensionality and number of distinct classes for every dataset. All the datasets are meant for classification.

\begin{table}[t] \centering
\caption{Basic overview of the datasets used in this work} \label{tab:datasets}
\begin{tabular}{llll}
\toprule
Dataset & \#~samples & \#~dimensions & \#~classes \\
\midrule
balance-scale           & 625   & 4    & 3  \\
breast-cancer           & 699   & 9    & 2  \\
digits6                 & 1083  & 64   & 6  \\
digits10                & 1797  & 64  & 10 \\
gaussians               & 400   & 5   & 4  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & 77   & 8  \\
pima-indians            & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
wine                    & 178   & 13   & 3  \\
\bottomrule
\end{tabular}
\end{table}

Some of the datasets have missing attributes. We chose to fill missing values using the mean for any given attribute as we found it behaves well with all the chosen datasets. Another difficulty with the datasets is that many of them do not have normalized attributes and their attribute values are unbounded and highly variable. Therefore, we run two sets of experiments, in one of them, the datasets are not preprocessed any further, in the other, the~datasets are normalized using standardization defined as $\hat{X} = \frac{X-\mu}{\sigma}$, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations. The experiments on preprocessed data are prefixed by "s:" in the tables with the results of the metric learning experiment. The experiment on dimensionality reduction was performed only on standardized data.

In all the experiments, we used 10-fold cross-validation in order to obtain the results. The tables contain the average error rate and the standard deviation of the error rate as computed based on the cross-validation.

\subsection{Hyperparameter tuning}

As every metric learning algorithm has a set of hyperparameters which highly influences the learned metric, we selected sensible values from a reasonable range for each of these parameters and ran a full grid-search on these hyperparameters before the metric learning experiment. The hyperparameters values obtained in this experiment are used even in the experiment on dimensionality reduction.

All sets of hyperparameters for each algorithm are shown in table \ref{tab:hyperparams}. The performance of $k$-NN classifier hugely depends on its number of neighbors hyperparameter. We chose to try values 1, 2, 4, 8, 16, 32, 64, 128 for this parameter. We tried 25, 50, 100, 250 and 1000 iterations for the iterative algorithms (the maximum number of iterations was lowered to 500 for the evolutionary approaches on the larger datasets, as otherwise the computations take too much time). The transformer parameter means whether the algorithm uses the full matrix or whether it is restricted to a diagonal transformation matrix.

The population size in the EAs was set to the recommended values for CMA-ES and jDE ( $4+\floor{3\log{d}}$ and $10d$ respectively). This gives an advantage to jDE in terms of number of fitness evaluations, However, preliminary experiments show that the jDE does not work very well with the lower population size.

\section{Results}
The experiments are divided into two parts. In the first part, we compare the CMA-ES with $k$-NN fitness to a jDE-based algorithm with fME based fitness similar to the one of Fukui \emph{et al.}~\cite{fukui2013evolutionary} in the metric learning setting. In the next part, we compare the evolutionary techniques to the metric learning algorithms for the task of dimensionality reduction. We also test the more general mappings with neural networks. The scripts used to create the results in this section are available at GitHub~\footnote{\url{https://github.com/svecon/metric-learn-playground/tree/ecml2017}}. There are also additional experiments which are not mentioned in this paper.

\input{results/hyperparameters}


\subsection{Metric Learning}
\label{chap:exp:classification}

In the first experiment, we tested how the CMA-ES with $k$-NN fitness compares to the other metric learning algorithms. The CMAES.kNN should be able to obtain better results as the mapping created by the algorithm aims to optimize the performance of the classifier directly. 

The metric learning algorithms that we tested in our experiments are Covariance (original Mahalanobis metric with a covariance matrix), LMNN, NCA and jDE.fME, which is based on the method presented by Fukui \emph{et al.}~\cite{fukui2013evolutionary}. Next, we tested the proposed method (CMAES.kNN). Finally, we tested jDE with the $k$-NN (jDE.kNN) fitness function and CMA-ES with the f-measure (CMAES.fME) fitness function in order to tell, what is the effect of the different optimization algorithm and what is the effect of the different fitness function. The baseline in the experiments is the Euclidean distance.

The error rates and their standard deviations (across 10 runs from the cross validation) of the final $k$-NN classifier for every dataset and every method are presented in Table~\ref{tab:error-rates}. If the method name is prefixed with "s:" it means the dataset was standardized. The best average performance for each dataset is in bold. NCA did not finish on the 3 largest datasets due to the memory limitations and so the corresponding cells are marked with "Memory". Some other methods did not finish because a numerical error occurred, these are marked as "Error".

\input{results/error-rates}

The results indicate that the standardization of the data is generally a good idea, only datasets digits6, digits10 and iris have the lowest error rates when unstandardized, however the standardized versions follow up with such a small margin that it is not a statistically significant difference. On the other hand, standardizing data can have a big impact as can be seen on the gaussians, sonar and wine datasets, where the difference between standardized and unstandardized data can be tens of percents. It shows that metric learning algorithms cannot fully replace the data standardization.

Overall, the LMNN method got the best results on 6 out of the 10 datasets, followed by CMAES.kNN with the best result on 4 of the datasets (2 of these datasets were the same as those for LMNN). However, the differences between these two methods are small and statistically insignificant. This often holds even for the other methods, as the standard deviations are quite large. Comparing the jDE and CMA-ES, we see that the type of fitness (fME or $k$-NN) affects the performance more than the optimization algorithm. However, jDE has an advantage in the population size, therefore we believe CMA-ES is overall better then jDE for this task, as it has similar performance with much lower number of fitness evaluations.

\subsection{Dimensionality Reduction}
\label{chap:exp:dimred}

In this experiment we compare PCA and t-SNE methods with DML methods supporting dimensionality reduction: NCA, LFDA and CMAES.kNN. We also experimented with evolving fully-connected neural networks for dimensionality reduction using CMAES.kNN. We evolved six different neural networks differing in depth (either one or five hidden layers) and their activation functions (sigm, tanh or ReLU). The shallow networks have one hidden layer with 4 neurons and the deep networks have 16-12-8-6-4 neurons in the hidden layers. To reduce the dimension into 2-dimensional space, all networks always end with an output layer of size 2. The shallow network therefore has $4d+8$ free parameters and the deeper network has $16d+354$ free parameters. No biases were used.

PCA, LFDA, NCA and CMAES.kNN methods are linear and so the dimensionality reduction works by projecting the data onto the 2-dimensional space by a matrix multiplication. The methods only differ in how to find this transformation matrix. We were also interested in whether the CMAES would help t-SNE making a better visualization if used as a preprocessing and we call this method CMAES.kNN+t-SNE, which first trains CMAES.kNN, the data are transformed using the evolved transformation matrix while keeping the original dimensionality, and only then the t-SNE is used to transform the preprocessed data from CMAES.kNN into two dimensions.

The dataset is split to training set and test set using a 10-fold cross validation. The algorithms are trained using the training set. Then the trained algorithms are used to transform both train and test sets into two dimensions. Then, we fit the $k$-NN classifier using this 2-dimensional training set and calculate a success rate on the test set. The idea is that the $k$-NN classifier has a large accuracy if instances from the same class are close together, which indicates a good dimensionality reduction.

We also include the t-SNE in the visualizations, however, as it is a transductive method method, it cannot be used to classify new data. Therefore in this case, the $k$-NN classifier is fit on all the data and the numbers may be overly optimistic. The main reason for including t-SNE is to show how the visualization from CMAES.kNN compare to the popular visualization method.

The results are presented in Table \ref{tab:dim-error-rates}. We show the error rates and standard deviations for each method on each dataset from the 10-fold cross validation. The best inductive method (i.e. of those that do not use t-SNE) for every dataset is in a bold font. Generally, CMAES.kNN works very well for dimensionality reduction and also very fast because only a $d\times2$ matrix is evolved. CMAES.kNN has the lowest error only on \textit{pima-indians}, however the difference between this method and the best method on each dataset is never statistically significant. t-SNE provides nice visualizations for the large datasets, namely \textit{digits6} and \textit{digits10} and the preprocessing with CMAES.kNN helps t-SNE on the majority of the datasets. LFDA and NCA have both also very good results, but NCA runs out of memory on three datasets (marked with "Memory" in the results table).

\input{results/dim-red}

The success rate, however, is only one of the factors that affect good visualization and therefore we also check the projections visually and evaluate the following criteria: the instances with the same label are together; the clusters are well separated; clusters form small blobs and they are not spread out; all instances with the same class are in one big cluster. Due to space limitations, we show the visualizations only for the digits10 and pima-indians datasets. The visualization for the rest of the datasets in available in the supplementary materials. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualized. 

On the \textit{digits10} dataset (Figure~\ref{fig:dimred:digits10}), the t-SNE visualization dominates other methods. Nevertheless, CMAES.kNN+t-SNE still improves the visualization by joining some clusters together. PCA algorithm finds some clusters, but they are merged together and not well separated. CMAES.kNN works better compared to PCA, it manages to separate the clusters more and the clusters themselves are also more uniform. LFDA method fails on this dataset completely, it spreads out just few data instances, but keeps the rest together in one dense cluster. NCA method runs out of memory on this datasets. The \textit{pima-indians} dataset (Figure \ref{fig:dimred:pima-indians}) is hard to visualize, and there is not a clear winner, but NCA, LFDA, CMEAS.kNN and CMAES.kNN+t-SNE were the best at separating the two classes from each other.

Interestingly, the neural networks with sigm and tanh activations tend to create interesting, almost cube-like structures, meanwhile ReLU activation creates more of a light torch structure, where many instances are clustered in a center and the rest is spread out in one major direction. We also found out that sometimes the simpler models (shallow networks, on only a linear mapping) often provide better results than the deeper neural networks. We believe one of the reasons may be the higher number of parameters in the deeper network which means it would take longer to tune them. Increasing the number of generations may a way to improve the performance of the deeper models, however, the training would also take considerably longer.

