A metric is ubiquitous in machine learning. For instance, in classification, $k$-NN classifier \cite{cover1967nearest} uses a metric to find the nearest neighbors. In clustering, many clustering algorithms, including wildly used k-Means \cite{hartigan1979algorithm}, calculate distance between data points and groups similar instances together. In information retrieval, when searching for the most relevant document, the documents themselves are ranked according to their similarity scores. Also security systems use a metric for face verification. Obviously, the performance of all these algorithms depends on the quality of the metric defined on the input space. The better the metric judges how close or how similar any two instances in the space are, the more the final machine learning algorithm will behave.

Several general purpose metrics exist, such as Euclidean distance, its generalized Minkowski distance, cosine similarity, Levenshtein distance (also called edit distance) for measuring distances between strings or $\chi^2$ distance used for calculating distances between histograms. These metrics, however, do not take any advantage of a prior knowledge of the task being solved or the structure of the data. Improved results are expected when the metric is designed for a particular task. Nevertheless, manually hand-crafting a metric is a strenuous and tedious process, which requires an expert knowledge of the task and the data. This has led to an introduction of distance metric learning (DML) where the distance metrics or similarities are learned from the data themselves.

The goal of metric learning is to adapt some pairwise metric function to the task at hand by using the training examples. In this work, we extend the ideas from DML and apply them to the problem of dimensionality reduction and visualization. The connection between dimensionality reduction and DML and the motivation for this work is quite simple. Given a generalized Mahalanobis distance $d_{\bm{M}}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})}$ with a positive definite matrix $M$ and its Cholesky decomposition $M=L^TL$, we get that $d_{\bm{M}}(\textbf{x},\textbf{y}) = d_{2}(\bm{L}\textbf{x},\bm{L}\textbf{y})$, where $d_2$ is the Euclidean distance. Therefore, the matrix $L$ is a mapping from an original space $O$ to another space $D$. We can generalize this notion further and look for general mappings $f: O \to D$. An example of such a mapping is a linear mapping. In such a case the matrix $L$ may have a lower rank and thus it maps the points of the original space to a lower-dimensional space. Another example of mapping may be a neural network, which makes a non-linear transformation of $O$ to $D$. The distances of points in $O$ are then computed as Euclidean distances of their projections to $D$. Here, we deal mostly with mapping to low dimensional spaces suitable for visualization, however, most of the ideas can be extended even for mappings to spaces with higher dimension, and thus to dimensionality reduction in general. The techniques described here are supervised, and the mappings we seek optimize the accuracy of the $k$-NN classifier applied in the low dimensional space. The motivation is that in case the $k$-NN classifier has high accuracy, instances from the same class are close together.

\section{Related Work}
The Large Margin Nearest Neighbor (LMNN) method designed by Weinberger and Saul~\cite{weinberger2009distance} is a very popular metric learning method that has been improved upon by other researchers~\cite{weinberger2008fast,parameswaran2010large,kedem2012non}. The goal of this method is to improve the performance of the $k$-NN algorithm and so the idea is that $k$-nearest neighbors with the same class should stay together while separating the impostors (instances with a different class) by a large margin. It works by defining the constraints in a local way: it optimizes only over similar points that are also in a close neighborhood of each other. The neighborhood is usually determined using Euclidean distance. The distance metric is then learned using a convex program. 

Goldberger and Hinton~\cite{jacobgoldberger2004neighbourhood} proposed another method designed to improve the performance of the $k$-NN classifier called Neighborhood Component Analysis (NCA). The algorithm works by calculating the leave-one-out error for a stochastic version of the $k$-NN classifier which uses Mahalanobis distance. Instead of searching for $k$-nearest neighbors, it uses a softmax probability distribution, which is used to weigh all the neighbors while favoring the closer ones. The objective of the NCA algorithm is to maximize the expected number of correctly classified points. The algorithm is also valid for rectangular matrices and therefore the NCA algorithm can also be used for dimensionality reduction.

Local Fisher Discriminant Analysis (LFDA) algorithm~\cite{sugiyama2007dimensionality} is a combination of the LDA method~\cite{fisher1936use} and the LPP method~\cite{he2003locality}. LDA works by maximizing the amount of between-class variance relative to the amount of within-class variance. This might be problem when instances from one class form more clusters because these variances are calculated globally in LDA. LFDA improves upon this by introducing locality in the between-class and within-class variances. The final transformation matrix $\bm{L}$ can be obtained by solving the generalized eigenvalue decomposition of the scatter matrices. Both LDA and LPP methods can be used for dimensionality reduction and so can be LFDA by restricting the matrix $\bm{L}$ to be rectangular.

Evolutionary algorithms (EA) have also been used for distance metric learning. For example the work of Koloseni~\cite{koloseni2012optimized} uses EA to select the best possible metric out of a predefined list to improve a classification error. The work of Fukui \emph{et al.}~\cite{fukui2013evolutionary} also uses EA, but takes a different approach and instead of choosing a metric from a predefined list, they try to evolve a Mahalanobis distance matrix. Differential evolution (DE) is used in which an individual encodes the Mahalanobis matrix, which is symmetric and so it can be encoded as a real-valued vector of length $d(d+1)/2$. DE is a special type of an evolutionary algorithm suitable for solving continuous problems, however it is quite sensitive to settings of some of its parameters, therefore, Fukui \emph{et al.}~\cite{fukui2013evolutionary} use an improved version of DE called jDE~\cite{brest2006self}, which alleviates this problem.

Fukui \emph{et al.} wanted to improve clustering algorithms using their algorithm and so they used the external clustering indexes as the fitness function in their EA. External clustering indexes, compared to internal clustering indexes, use the class labels of the data for the evaluation. In their work they use Purity, f-measure (fME) (harmonic mean of the precision and recall) and Entropy. The fME index performed the best in their experimental results and so in this work, we chose to experiment with this index only. After certain number of generations of the evolution algorithm, the individual with the best fitness values is selected Mahalanobis matrix is decoded from the individual.

Most of the DML methods described above, also offer dimensionality reduction. However, there are also other methods for dimensionality reduction and visualization. One of the most common unsupervised dimensionality reduction method is PCA~\cite{jolliffe2002principal} which decomposes the dataset in a set of successive orthogonal components that explain a maximum amount of the variance inside of the data. Another one is t-SNE~\cite{maaten2008visualizing}, which maps the points from the high dimensional space to a low dimensional one. The mapping aims to minimize the Kullback-Leibler divergence of the distribution of distances between points in the high and low dimensional spaces. While t-SNE is getting a lot of attention recently, it does not provide any explicit mapping between the two spaces, therefore it cannot be used to map new data into the lower-dimensional space without re-running the whole method. However, we also show the visualizations obtained by t-SNE and compare them to those obtained by the method presented in this work in the experiments.

\section{Generalizing Evolutionary Distance Learning}

In this work, we aim to find a mapping from the original space $O$ of the input data, to a (potentially low dimensional) space $D$. The mapping $f: O \to D$ is then used in the distance metric found by the algorithm. The evolved metric is defined as $d_f(x, y) = d_2(f(x), f(y))$, where $d_2$ is the Euclidean metric in the $D$ space. We have already discussed that in case $f$ is represented by a lower triangular matrix $L$, $d_f$ represents a Mahalanobis distance with matrix $L^TL$. However, the mappings may be more general. If the mapping is represented by a rectangular matrix, the technique can be used for dimensionality reduction. Moreover, we can seek a non-linear mapping $f$ which is able to transform the input data in such a way that they are easier for the classifier.

In this work, we consider mappings which can be represented as vectors or matrices of real numbers numbers. Such mappings include both linear transformations and non-linear transformation such as neural networks (which can be represented as vectors of their weights). The goal of the algorithm is to find a mapping that maximizes the performance of the classifier that follows the preprocessing step. In this paper, we consider the $k$-NN classifier, however different classifiers can also be used. The performance of the $k$-NN classifier is used as the fitness in the evolutionary algorithm. One of the reasons is that it is relatively fast, it takes $N\log{N}$ to build the underlying k-d tree structure with $N$ training samples and then $M \log{N}$ to classify $M$ testing samples. Another advantage of using the $k$-NN classifier is that the resulting mappings should also provide nice visualizations of the data. The $k$-NN classifier is local, and therefore works the best in case instances from the same class are close together.

In our experiments we use the most common version $(\mu_W, \lambda)$-CMA-ES, described in \cite{hansen2001completely}, where in each iteration a weighted combination of $\mu$ best new candidates (out of $\lambda$) are used to update the distribution parameters. As the $(\mu_W, \lambda)$-CMA-ES is non-elitist (i.e. it can lose the best individual in the search), we kept the best individual separately in a hall of fame (HOF), and return it after the algorithm finishes.

The high level pseudocode for the evolution of the mapping is shown in Algorithm \ref{pseudo:our-method}. First, all parameters are initialized randomly along with an empty HOF. The individuals are vectors of real numbers, that encode the mapping. In case of evolving the matrix $L$ for the Mahalanobis distance, this is a vector of $d^2$ numbers. In case of evolving the neural networks, the individual contains all the weights from the network. Then the CMA-ES algorithm is run until a stopping criterion is met. In each generation new individuals are sampled and the fitnesses of the individuals are evaluated. The HOF and the internal parameters of CMA-ES are updated. After the evolution is finished, the best individual from the HOF is returned. The individual contains the weights that represent the corresponding mapping.

\begin{algorithm}[t]
\caption{Evolving the mapping using CMA-ES} \label{pseudo:our-method}
\DontPrintSemicolon
\LinesNumbered
\Begin{
  Initialize CMA-ES parameters\;
  Initialize empty HOF\;
  \Repeat{a termination criterion is met}{
    Sample new individuals using the current CMA-ES parameters\;
    Evaluate fitnesses of the new individuals as described in Algorithm \ref{pseudo:ea:knn}\;
    Update best individuals in HOF\;
    Update CMA-ES parameters\;
  }
  \Return the best individual from the HOF\;
}
\end{algorithm} 

The most important step here is the evaluation of the fitness function, which is computed as the success rate of the $k$-NN classifier on the data in the $D$ space. The pseudocode is shown in Algorithm~\ref{pseudo:ea:knn}. First, the labeled samples are split into training and testing sets. It is important to note that the splitting is done for each fitness evaluation and therefore the sets are split differently for each individual. The idea behind this is we would like the evolution to find individuals that generalize well. Secondly, the mapping is decoded from the individual, the instances are mapped to the $D$ space and a $k$-NN classifier with Euclidean metric trained using the training data. Then the classifier is used to predict labels of the testing data and an accuracy of this test is returned as a fitness value of the individual.

\begin{algorithm}[t]
\caption{$k$-NN as fitness function in EA} \label{pseudo:ea:knn}
\DontPrintSemicolon
\LinesNumbered
\KwData{set of labeled instances}
\KwIn{the individual $\bm{i}$ encoding the mapping}
\KwResult{the fitness value of the $\bm{i}$ individual}
\Begin{
  Split instances into train-test sets\;
  Extract the mapping from the individual $\bm{i}$\;
  Map the instances to the $D$ space\;
  Initialize $k$-NN classifier in the $D$ space\;
  Train the $k$-NN classifier using the train set\;
  Perform the classification using the $k$-NN on the test set\;
  \Return the success rate of the classification of the test set\;
}
\end{algorithm} 

To perform a non-linear transformation using a neural network, the activation functions need to be used between the layers. Without activation functions a fully connected neural network essentially reduces to a linear transformation. Typical activation functions used in neural networks are sigmoid function ($\sigma$), hyperbolic tangent (tanh) and rectified linear unit (ReLU), which are defined as $\sigma(x) = 1/(1+e^{-x})$, $\text{tanh}(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} = 2\sigma(2x)-1$ and $\text{ReLU}(x) = \max(0,x)$.

To perform a dimensionality reduction using the neural network, the shape and the activation functions need to be chosen beforehand, the only criteria is that the first layer has the size $d$ and the last layer has the desired size $m$. The individual in the evolution then contains weights corresponding to all parameters of the neural network. We experiment with the activation functions and the depth of the neural network used for visualization in Section \ref{chap:exp:dimred}.

The algorithm described here is similar to the algorithm described by Fukui \emph{et al.}~\cite{fukui2013evolutionary}, however, there are some important differences:
\begin{itemize}
\item Fukui \emph{et al.} focused on improving clustering and therefore they used the f-measure as the fitness function, we deal with classification and therefore we minimize the error rate of the $k$-NN classifier;
\item we evolve the mapping instead of the matrix for the Mahalanobis distance, therefore we do not need to repair the matrix in such a way that it is PD;
\item we use CMA-ES instead of jDE, therefore we can use smaller population size which leads to faster optimization (recommended population size for CMA-ES is only $4+\floor{3\log{d}}$ \cite{hansen2006cma} whereas for jDE it is $10d$~\cite{brest2006self}, where $d$ is number of parameters to evolve).
\end{itemize}

\section{Experiment Description}

We performed two experiments to assess the performance of the algorithm presented in this work. In Section \ref{chap:exp:classification} we learn a metric for the $k$-NN classifier and compare classification errors on various datasets. This experiment also serves as a comparison to jDE, which is the most closely related method from the literature. In Section \ref{chap:exp:dimred} we present the main experiment, which shows the dimensionality reduction and visualization obtained by the CMAES.kNN method.

In the experiments, we compare some of the state-of-the-art metric learning algorithms (LMNN, NCA, LFDA). We use their implementation from the Python \textit{metric-learn} library\footnote{\url{https://github.com/all-umass/metric-learn}}. We implemented the jDE-based method similar to~\cite{fukui2013evolutionary} and our algorithm and the implementation is available as a supplementary material and also at GitHub\footnote{\url{https://github.com/svecon/metric-learn/tree/ecml2017}}.

\subsection{Datasets} \label{chap:exp:datasets}

For the experiments we used dataset that were most commonly used in the literature \cite{xing2002distance}, \cite{weinberger2009distance}, \cite{jacobgoldberger2004neighbourhood} and \cite{fukui2013evolutionary}. These are balance-scale, breast-cancer, iris, mice-protein, pima-indians, sonar and wine dataset from the well-known UCI Machine Learning Repository\footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Most of the datasets are rather small, therefore, we added two larger datasets -- digits6 and digits10.

We also created an artificial dataset by sampling from four multinomial Gaussian distributions in order to evaluate the algorithm on highly variable data. Each Gaussian has a different center, defined as a matrix in Equation \eqref{eq:gauss} where each row corresponds to one center. All Gaussians were generated sharing one covariance matrix, also defined in Equation \ref{eq:gauss}. Dimensions are uncorrelated, however the first dimension has a large variability of $10^8$.

\begin{equation} \label{eq:gauss}
centers = \begin{pmatrix}
10 & 0 & 0 & 2 \\
0 & 10 & 0 & -2 \\
0 & 0 & 10 & -2 \\
0 & 0 & -10 & 2 \\
\end{pmatrix}, \,
covariance = \begin{pmatrix}
10^8 & 0 & 0 & 0 \\
0 & 100 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}
Table \ref{tab:datasets} summarizes datasets that we experimented on. The table shows number of samples, dimensionality and number of distinct classes for every dataset. All the datasets are meant for classification.

\begin{table}[t] \centering
\caption{Basic overview of the datasets used in this work} \label{tab:datasets}
\begin{tabular}{llll}
\toprule
Dataset & \#~samples & \#~dimensions & \#~classes \\
\midrule
balance-scale           & 625   & 4    & 3  \\
breast-cancer           & 699   & 9    & 2  \\
digits6                 & 1083  & 64   & 6  \\
digits10                & 1797  & 64  & 10 \\
gaussians               & 400   & 5   & 4  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & 77   & 8  \\
pima-indians            & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
wine                    & 178   & 13   & 3  \\
\bottomrule
\end{tabular}
\end{table}

Some of the datasets have missing attributes. We chose to fill missing values using the mean for any given attribute as we found it behaves well with all the chosen datasets. Another difficulty with the datasets is that many of them do not have normalized attributes and their attribute values are unbounded and highly variable. Therefore, we run two sets of experiments, in one of them, the datasets are not preprocessed any further, in the other, the~datasets are normalized using standardization defined as $\hat{X} = \frac{X-\mu}{\sigma}$, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations. The experiments on preprocessed data are prefixed by "s:" in the tables with the results of the metric learning experiment. The experiment on dimensionality reduction was performed only on standardized data.

In all the experiments, we used 10-fold cross-validation in order to obtain the results. The tables contain the average error rate and the standard deviation of the error rate as computed based on the cross-validation.

\subsection{Hyperparameter tuning}

As every metric learning algorithm has a set of hyperparameters which highly influences the learned metric, we selected sensible values from a reasonable range for each of these parameters and ran a full grid-search on these hyperparameters before the metric learning experiment. The hyperparameters values obtained in this experiment are used even in the experiment on dimensionality reduction.

All sets of hyperparameters for each algorithm are shown in table \ref{tab:hyperparams}. The performance of $k$-NN classifier hugely depends on its number of neighbors hyperparameter. We chose to try values 1, 2, 4, 8, 16, 32, 64, 128 for this parameter. We tried 25, 50, 100, 250 and 1000 iterations for the iterative algorithms (the maximum number of iterations was lowered to 500 for the evolutionary approaches on the larger datasets, as otherwise the computations take too much time). The transformer parameter means whether the algorithm uses the full matrix or whether it is restricted to a diagonal transformation matrix.

The population size in the EAs was set to the recommended values for CMA-ES and jDE ( $4+\floor{3\log{d}}$ and $10d$ respectively). This gives an advantage to jDE in terms of number of fitness evaluations, However, preliminary experiments show that the jDE does not work very well with the lower population size.

\section{Results}
The experiments are divided into two parts. In the first part, we compare the CMA-ES with $k$-NN fitness to a jDE-based algorithm with fME based fitness similar to the one of Fukui \emph{et al.}~\cite{fukui2013evolutionary} in the metric learning setting. In the next part, we compare the evolutionary techniques to the metric learning algorithms for the task of dimensionality reduction. We also test the more general mappings with neural networks. The scripts used to create the results in this section are available at GitHub~\footnote{\url{https://github.com/svecon/metric-learn-playground/tree/ecml2017}}. There are also additional experiments which are not mentioned in this paper.

\input{results/hyperparameters}


\subsection{Metric Learning}
\label{chap:exp:classification}

In the first experiment, we tested how the CMA-ES with $k$-NN fitness compares to the other metric learning algorithms. The CMAES.kNN should be able to obtain better results as the mapping created by the algorithm aims to optimize the performance of the classifier directly. 

The metric learning algorithms that we tested in our experiments are Covariance (original Mahalanobis metric with a covariance matrix), LMNN, NCA and jDE.fME, which is based on the method presented by Fukui \emph{et al.}~\cite{fukui2013evolutionary}. Next, we tested the proposed method (CMAES.kNN). Finally, we tested jDE with the $k$-NN (jDE.kNN) fitness function and CMA-ES with the f-measure (CMAES.fME) fitness function in order to tell, what is the effect of the different optimization algorithm and what is the effect of the different fitness function. The baseline in the experiments is the Euclidean distance.

The error rates and their standard deviations (across 10 runs from the cross validation) of the final $k$-NN classifier for every dataset and every method are presented in Table~\ref{tab:error-rates}. If the method name is prefixed with "s:" it means the dataset was standardized. The best average performance for each dataset is in bold. NCA did not finish on the 3 largest datasets due to the memory limitations and so the corresponding cells are marked with "Memory". Some other methods did not finish because a numerical error occurred, these are marked as "Error".

\input{results/error-rates}

The results indicate that the standardization of the data is generally a good idea, only datasets digits6, digits10 and iris have the lowest error rates when unstandardized, however the standardized versions follow up with such a small margin that it is not a statistically significant difference. On the other hand, standardizing data can have a big impact as can be seen on the gaussians, sonar and wine datasets, where the difference between standardized and unstandardized data can be tens of percents. It shows that metric learning algorithms cannot fully replace the data standardization.

Overall, the LMNN method got the best results on 6 out of the 10 datasets, followed by CMAES.kNN with the best result on 4 of the datasets (2 of these datasets were the same as those for LMNN). However, the differences between these two methods are small and statistically insignificant. This often holds even for the other methods, as the standard deviations are quite large. Comparing the jDE and CMA-ES, we see that the type of fitness (fME or $k$-NN) affects the performance more than the optimization algorithm. However, jDE has an advantage in the population size, therefore we believe CMA-ES is overall better then jDE for this task, as it has similar performance with much lower number of fitness evaluations.

\subsection{Dimensionality Reduction}
\label{chap:exp:dimred}

In this experiment we compare PCA and t-SNE methods with DML methods supporting dimensionality reduction: NCA, LFDA and CMAES.kNN. We also experimented with evolving fully-connected neural networks for dimensionality reduction using CMAES.kNN. We evolved six different neural networks differing in depth (either one or five hidden layers) and their activation functions (sigm, tanh or ReLU). The shallow networks have one hidden layer with 4 neurons and the deep networks have 16-12-8-6-4 neurons in the hidden layers. To reduce the dimension into 2-dimensional space, all networks always end with an output layer of size 2. The shallow network therefore has $4d+8$ free parameters and the deeper network has $16d+354$ free parameters. No biases were used.

PCA, LFDA, NCA and CMAES.kNN methods are linear and so the dimensionality reduction works by projecting the data onto the 2-dimensional space by a matrix multiplication. The methods only differ in how to find this transformation matrix. We were also interested in whether the CMAES would help t-SNE making a better visualization if used as a preprocessing and we call this method CMAES.kNN+t-SNE, which first trains CMAES.kNN, the data are transformed using the evolved transformation matrix while keeping the original dimensionality, and only then the t-SNE is used to transform the preprocessed data from CMAES.kNN into two dimensions.

The dataset is split to training set and test set using a 10-fold cross validation. The algorithms are trained using the training set. Then the trained algorithms are used to transform both train and test sets into two dimensions. Then, we fit the $k$-NN classifier using this 2-dimensional training set and calculate a success rate on the test set. The idea is that the $k$-NN classifier has a large accuracy if instances from the same class are close together, which indicates a good dimensionality reduction.

We also include the t-SNE in the visualizations, however, as it is a transductive method method, it cannot be used to classify new data. Therefore in this case, the $k$-NN classifier is fit on all the data and the numbers may be overly optimistic. The main reason for including t-SNE is to show how the visualization from CMAES.kNN compare to the popular visualization method.

The results are presented in Table \ref{tab:dim-error-rates}. We show the error rates and standard deviations for each method on each dataset from the 10-fold cross validation. The best inductive method (i.e. of those that do not use t-SNE) for every dataset is in a bold font. Generally, CMAES.kNN works very well for dimensionality reduction and also very fast because only a $d\times2$ matrix is evolved. CMAES.kNN has the lowest error only on \textit{pima-indians}, however the difference between this method and the best method on each dataset is never statistically significant. t-SNE provides nice visualizations for the large datasets, namely \textit{digits6} and \textit{digits10} and the preprocessing with CMAES.kNN helps t-SNE on the majority of the datasets. LFDA and NCA have both also very good results, but NCA runs out of memory on three datasets (marked with "Memory" in the results table).

\input{results/dim-red}

The success rate, however, is only one of the factors that affect good visualization and therefore we also check the projections visually and evaluate the following criteria: the instances with the same label are together; the clusters are well separated; clusters form small blobs and they are not spread out; all instances with the same class are in one big cluster. Due to space limitations, we show the visualizations only for the digits10 and pima-indians datasets. The visualization for the rest of the datasets in available in the supplementary materials. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualized. 

On the \textit{digits10} dataset (Figure~\ref{fig:dimred:digits10}), the t-SNE visualization dominates other methods. Nevertheless, CMAES.kNN+t-SNE still improves the visualization by joining some clusters together. PCA algorithm finds some clusters, but they are merged together and not well separated. CMAES.kNN works better compared to PCA, it manages to separate the clusters more and the clusters themselves are also more uniform. LFDA method fails on this dataset completely, it spreads out just few data instances, but keeps the rest together in one dense cluster. NCA method runs out of memory on this datasets. The \textit{pima-indians} dataset (Figure \ref{fig:dimred:pima-indians}) is hard to visualize, and there is not a clear winner, but NCA, LFDA, CMEAS.kNN and CMAES.kNN+t-SNE were the best at separating the two classes from each other.

Interestingly, the neural networks with sigm and tanh activations tend to create interesting, almost cube-like structures, meanwhile ReLU activation creates more of a light torch structure, where many instances are clustered in a center and the rest is spread out in one major direction. We also found out that sometimes the simpler models (shallow networks, on only a linear mapping) often provide better results than the deeper neural networks. We believe one of the reasons may be the higher number of parameters in the deeper network which means it would take longer to tune them. Increasing the number of generations may a way to improve the performance of the deeper models, however, the training would also take considerably longer.

