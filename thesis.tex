%%% The main file. It contains definitions of basic parameters and includes all other parts.

\include{header}
\include{acronyms}

% Definitions of macros (see description inside)
\include{macros}

% Title page and various mandatory informational pages
\begin{document}
\include{title}

%%% A page with automatically generated table of contents of the master thesis

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Guidelines

% Transformace dat jsou důležitou součástí strojového učení, která výrazně ovlivňuje kvalitu vytvořených modelů. Transformace dat se často používají i pro jejich zobrazení do prostoru s menší dimenzí, kde se dají snáze vizualizovat. Většina metod ale funguje bez učitele a tedy není schopna najít taková zobrazení dat, která by brala v úvahu vlastnosti metod strojového učení, které následují po ní, případně se snažila vizualizaci upravit tak, aby data ze stejné třídy byla blízko u sebe. Některé metody určené přímo pro vizualizaci (jako např. t-SNE) navíc ani neposkytují transformační funkci a nejsou tedy schopny zobrazit nová data bez přepočítání celého zobrazení. Cílem práce je tedy pomocí kombinace evolučních algoritmů a dalších přístupů (např. neuronových sítí) navrhnout metody pro vytvoření transformační funkce, která bude brát v úvahu i označkování dat. 

% Student se seznámí s postupy pro automatické transformace dat. Na základě zjištěných informací implementuje vlastní metody a porovná je s existujícími přístupy. Součástí srovnání bude i vhodnost použité metody pro zobrazení dat do prostoru s malou dimenzí vhodnou pro vizualizaci.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction} \label{chap:intro}
% Introduction
% - What has been done
% - What is known
% - What is unknown
% - Motivation: what and why are we doing this
% - Our goals
% - Structure of the work

% Related works
% - What others have done
% - What is author going to do

%TODO curse of dimensionality
% http://jotterbach.github.io/2016/05/23/TSNE/

A metric is ubiquitous in machine learning. For instance, in classification, \ac{knn} classifier~\citep{cover1967nearest} uses a metric to find the nearest neighbours. In clustering, many clustering algorithms, including popular k-Means~\citep{hartigan1979algorithm} algorithm, calculate distance between data points and group similar instances together. In information retrieval, when searching for the most relevant document, the documents themselves are ranked according to their similarity scores. Also security systems use a metric for face verification. Obviously, the performance of all these algorithms depends on the quality of the metric defined on the input space. The better the metric judges how close or how similar any two instances in the space are, the better the final machine learning algorithm will perform.

Several general purpose metrics exist, such as Euclidean distance, defined in Equation~\ref{eq:euclid}, or its generalised Minkowski distance, defined in Equation~\ref{eq:minkowski} and cosine similarity defined in Equation~\ref{eq:cosine}. These are all used for measuring distances between vectors of real numbers. Levenshtein distance measures edit distance between strings and $\chi^2$ distance (in Equation~\ref{eq:chisq}) measures distances between histograms.

% https://numerics.mathdotnet.com/distance.html
\begin{align}
d_{\mathbf{2}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_2 = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2} \label{eq:euclid} \\
d_{\mathbf{p}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_p = \bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p} \label{eq:minkowski} \\
\mathrm{sim}(\textbf{x}, \textbf{y}) &= \cos{\phi} = \frac{\textbf{x}\textbf{y}}{\|\textbf{x}\|\|\textbf{y}\|} \label{eq:cosine} \\
\chi^2(\textbf{x}, \textbf{y}) &= \sum_{i} \cfrac{(x_i - y_i)^2} {(x_i + y_i)} \label{eq:chisq}
\end{align}

These metrics, however, do not take any advantage of a prior knowledge of the task being solved or the structure of the data. Improved results are expected when the metric is designed for a particular task. Nevertheless, manually hand-crafting a metric is a strenuous and tedious process, which requires expert knowledge of the task and the data. This has led to the introduction of \ac{dml} where the distance metrics or similarities are learnt from the data themselves.

The goal of metric learning is to adapt a pairwise metric function to the task at hand by using the training examples. This process requires almost no prior knowledge about the data and is very fast, compared to hand-crafting a metric function, which may take months to develop. That is why learning a distance metric or a similarity metric directly from the data has obtained a lot of traction in the recent years.

Another important area of machine learning is dimensionality reduction, whose goal is to embed high-dimensional space in a low-dimensional space so that most of the information contained in the data is preserved (such as the variance or local distance measurements). Some metric learning algorithms do not scale well when the data are high-dimensional and so the dimensionality reduction can be used to transform data into a low-dimensional space just to save computational cost. Another dimensionality reduction application is visualisation, where the data are reduced into a 2-dimensional or 3-dimensional space, in which they can be plotted and inspected more easily. The reason why we include dimensionality reduction in this work is because some of the metric learning algorithms search for a projection into a low-dimensional space and thus can also be used for dimensionality reduction.

% http://scikit-learn.org/stable/modules/decomposition.html#pca
% http://scikit-learn.org/stable/modules/lda_qda.html#id4
% http://scikit-learn.org/stable/modules/manifold.html
One of the most common unsupervised dimensionality reduction methods is \ac{pca}~\citep{jolliffe1986principal}, which decomposes the dataset in a set of successive orthogonal components that explain a maximum amount of the variance inside of the data. Other well-known methods are for example \ac{ica}~\citep{comon1994independent} and \ac{lda}~\citep{fisher1936use}.

A specific area of non-linear dimensionality reduction is called manifold learning, which is based on the idea that the dimensionality of many datasets is only artificially high but the intrinsic dimensionality is low and the data lie on an embedded low-dimensional manifold within the higher-dimensional space.

A popular manifold algorithm receiving a lot of traction recently is \ac{tsne}~\citep{maaten2008visualizing}, which tries to preserve a local structure of the data and therefore is especially useful in visualisation. Other known manifold methods are \ac{lle}~\citep{roweis2000nonlinear}, \ac{mds}~\citep{kruskal1978multidimensional} and many others. More dimensionality reduction methods were reviewed by~\citep{fodor2002survey} and~\citep{van2009dimensionality}.

In this thesis, we review existing state-of-the-art methods for learning global Mahalanobis distance metric and propose a new method using evolutionary algorithms (EA). In the experiments the methods are compared on several popular datasets to see how they improve classification. Learning the full Mahalanobis distance means learning $d^2$ parameters, however by restricting the methods to learn only a diagonal of the Mahalanobis distance metric, only $d$ parameters are needed. Moreover, we explore the usage of metric learning algorithms for visualisation of the data (dimensionality reduction to 2 dimensions) and compare them with traditional approaches.

\section{Metric learning} \label{chap:intro:ml}

The origins of metric learning can be traced back to~\citep{short1981optimal}, however, the first work that received a lot of attention was done by~\citep{xing2002distance}, where the metric learning is formulated as a convex optimisation problem. Since then, metric learning has obtained a lot of attention and appeared at ICML 2010\footnote{\url{http://www.icml2010.org/program.html}}, ECCV 2010\footnote{\url{http://www.ics.forth.gr/eccv2010/tutorials.php}} coferences and ICCV 2011\footnote{\url{http://www.iccv2011.org/authors/workshops}}, NIPS 2011\footnote{\url{https://nips.cc/Conferences/2011/Schedule}} and ICML 2013\footnote{\url{http://icml.cc/2013/?page_id=41}} workshops.

The goal of the metric learning is to find a pairwise real valued metric function $d(x,y)$ for the given problem such that it leverages the most information it can using the training samples. Every metric learning algorithm has some defining properties. These properties are well described and categorised in a survey done by~\citep{bellet2013survey}. All these properties and their categories are summarised in Figure~\ref{fig:metric-learning}.

\cenfig{graphs/metric_learning}{Properties of metric learning algorithms. This work focuses on the highlighted categories.}{fig:metric-learning}

\newpage

\begin{description}
\item [Learning paradigm] can be categorized into three categories, depending on what kind of information about the data is avaiable:
\begin{itemize}
\item in \textit{fully supervised} paradigm, every sample of the dataset $z_i=(x_i,y_i) \in \mathcal{X} \times \mathcal{Y}$ is composed of the instance $x_i \in \mathcal{X}$ and its label (class) $x_y \in \mathcal{Y}$, which is a discrete and finite set of $\abs{\mathcal{Y}}$ labels.
\item in \textit{weakly supervised} paradigm, the instances are unlabeled, but they come with a side information, typically in a form of constraints $\mathcal{S}, \mathcal{D}$ defined as % pairs of instances, which are either similar or dissimilar, defined as %Equations~\ref{eq:similar}---\ref{eq:dissimilar}).
\begin{align}
\mathcal{S} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be similar} \rbrace, \label{eq:similar} \\
\mathcal{D} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be dissimilar} \rbrace. \label{eq:dissimilar}
\end{align}
The metric learning algorithm tries to exploit these constraints to find the best parameters of a metric that best agree with these constraints. While obtaining labelled instances can be hard or costly, obtaining similar and dissimilar pairs is easier.
\item in \textit{semi-supervised} paradigm, there is some information (fully supervised or weakly supervised) about a portion of the data, but some (usually large) portion of the data comes with no information.
\end{itemize}

\item [Form of metric] is one of the defining features of a metric learning algorithm. It categorizes the metrics into three main categories: 
\begin{itemize}
\item \textit{Linear metrics} where the distance measures a length of a straight line between two points in Euclidean space. Typical example is Euclidean distance or Mahalanobis distance.
\item \textit{Nonlinear metrics} can capture nonlinear dependencies inside data, typical example is $\chi^2$ histogram distance, which was defined in Equation~\ref{eq:chisq}.
\item \textit{Local metrics} where multiple different metrics are used for different parts of the input space. Local metrics can be linear or nonlinear and all metrics are usually trained in parallel.
\end{itemize}

\item [Scalability] is an important factor to be considered for any machine learning algorithm. Having a great method that is intractable to calculate for large amounts of data is useless. Therefore the algorithms need to scale well with respect to \textit{number of samples}. Also, metric learning algorithms should scale well with respect to \textit{number of dimensions} of the data. However, since many metric learning algorithms are trying to learn a $d \times d$ matrix, designing an algorithm that scales well with an increasing dimensionality is a challenge.

\item [Optimality of the Solution] refers to the ability of the algorithm to find a solution. Ideally the algorithm is capable of finding a \textit{global optimum}, which means that the solution is always the best one of all feasible solutions available. This is usually the case for algorithms defined as convex optimisations. On the other hand, the algorithm might only be able to find \textit{local optimum}, which means that no better solution exists in an immediate neighbourhood of the current solution. This is typical for nonconvex formulations of the algorithms.

\item [Dimensionality reduction] is an interesting ``by-product'' of metric learning algorithms that search for a projection of the data into a low-dimensional space, typically achieved by applying a strong regularisation on the algorithm so that the final solution is low-rank. An advantage of supervised dimensionality reduction via metric learning is that it tries to maximize the separation of labelled data. Dimensionality reduction has numerous applications, ranging from visualisation to more compact representations of the data.
\end{description}

This work is mainly focused on the metric learning algorithms that belong to the categories highlighted with orange background in Figure~\ref{fig:metric-learning}: fully supervised with a linear (Mahalanobis) metric that scale well and are capable of dimensionality reduction. 

\section{Applications} \label{chap:intro:applications}

\Ac{dml} can potentially be beneficial whenever a metric between instances plays an important role. Recently, it has been successfully applied to many different problems, such as link prediction in networks~\citep{shaw2011learning}, state representation in reinforcement learning~\citep{taylor2011metric}, music recommendation~\citep{mcfee2012learning}, identity verification~\citep{ben2012improved}, webpage archiving~\citep{law2012structural}, cartoon synthesis~\citep{yu2012semisupervised}, and assessing the efficacy of acupuncture~\citep{liang2012learning}.

There are three large fields that benefit from \ac{dml}. In \textit{Computer vision} \ac{dml} is not only used to find distances between images and videos, but it has also been successfully applied in problems such as image classification~\citep{mensink2012metric}, object recognition~\citep{frome2007learning};~\citep{verma2012learning}, face recognition~\citep{guillaumin2009you};~\citep{lu2014neighborhood}, visual tracking~\citep{li2012non};~\citep{jiang2012order} or image annotation~\citep{guillaumin2009tagprop}. In \textit{Information retrieval}, the objective is to try to provide the most relevant documents to a specific query, or rank documents and other media. This is achieved by training a metric between the documents or a document and a query. Recent applications of \ac{dml} in this field were done by~\citep{lebanon2006metric},~\citep{lee2008rank},~\citep{mcfee2010metric} and~\citep{lim2013robust}. \textit{Bioinformatics} is another large field where \ac{dml} is used extensively to compare protein or DNA sequences. Typically edit distance is used to measure distance between these sequences, but adapting a metric to a specific problem can greatly improve the results as demonstrated in the works of~\citep{xiong2006kernel},~\citep{saigo2006optimizing},~\citep{kato2010metric} and~\citep{wang2012prodis}.

% https://en.wikipedia.org/wiki/Transduction_(machine_learning)
\textit{Kernel learning} is another research area related to metric learning, but it is outside the scope of this work. Compared to metric learning (for example a Mahalanobis distance) where parameters of a given form of metric are to be learnt, kernel learning is typically nonparametric. The main problem with this approach is that it is transductive, which means that the distances between the pairs of the instances are learnt as values, not functions. Therefore the kernel learning can hardly be applied to new data. A recent survey on kernel learning was done by~\citep{abbasnejad2012survey}.

\textit{\Ac{mkl}}~\citep{gonen2011multiple} is an extension of kernel learning, which is parametric and the goal is to learn a predefined set of kernels. Compared to kernel learning, \ac{mkl} has efficient formulations and can be applied in the inductive setting (reasoning from observed data to make general rules). \Ac{mkl} is very popular because it can select an optimal kernel from a larger set of kernels and it can combine data that have different notion of similarity and require different kernels. \ac{mkl} has been used in many applications such as recognition in video~\citep{chen2013event}, object recognition in images~\citep{bucak2014multiple} or biomedical data fusion~\citep{yu20102}. A recent survey on \ac{mkl} was done by~\citep{gonen2011multiple}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mahalanobis distance} \label{chap:intro:mah}
% http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html
% https://en.wikipedia.org/wiki/Mahalanobis_distance

The problem with the Euclidean distance is that it does not account for variances in different directions and correlations between pairs of dimensions in the data. For normally distributed data, the distance from the mean can be specified by computing the z-score, defined as $z = \frac{x-\mu}{\sigma}$, where $\mu$ is the population mean and $\sigma$ is the population standard deviation. The z-score of $x$ can seen as a dimensionless quantity and can be interpreted as the number of standard deviations of $x$ away from the mean.

Figure~\ref{fig:corrdata} demonstrates this problem in two dimensions: the figure visualises a dataset with both dimensions highly correlated and with the first dimension highly variable. In the plot the coloured ellipses show the probability density function of the Gaussian mixture model, which is high for ellipses near the origin and low for ellipses further away. In the figure there are three red points: 
% \begin{itemize}
origin $O$ at coordinates $[0,0]$,
$A$ at coordinates $[2,0]$,
and $B$ at coordinates $[0,1]$.
% \end{itemize}

\cenfig{graphs/correlated_data}{Correlated data centred around origin}{fig:corrdata}

It is clear that in Euclidean distance $d_2(A,O)>d_2(B,O)$. However, because the first axis has larger variance, it is clear that seeing point $A$ is much more likely in our dataset than it is seeing point $B$ because $A$ lies on the edge of the first ellipsis, meanwhile point $B$ lies beyond the edge of the second ellipsis. Using the terminology of the z-score, point $B$ is more standard deviations away from the origin than point $A$.

% http://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html
The data can be uncorrelated using the covariance matrix $\bm{\Sigma}$, which is guaranteed to be a \ac{psd} matrix. Matrix $\bm{A}$ is \ac{psd} if it fulfils
\begin{equation*}
\bm{x}^\top \bm{A}\bm{x} \geq 0 \quad \forall \bm{x} \in \mathbb{R}^d, \bm{x} \neq 0.
\end{equation*}

Sometimes, however the covariance matrix is even \ac{pd} and thus the Cholesky decomposition $\bm{\Sigma}=\bm{L}^\top\bm{L}$ can be used to obtain a lower triangular matrix $\bm{L}$. The inverse of this triangular matrix $\bm{L}$ can be used to remove the correlation from the data defined by $\bm{\Sigma}$. Figure~\ref{fig:uncorrdata} demonstrates the effect of multiplying the original data by $\bm{L}^{-1}$. In this figure with uncorrelated data, it is now clear that $d_2(A,O)<d_2(B,O)$ even in Euclidean distance.

\cenfig{graphs/uncorrelated_data}{Uncorrelated data using Cholesky decomposition of covariance matrix}{fig:uncorrdata}

Many researchers tackle metric learning by adapting a Mahalanobis distance metric first described by~\citep{mahalanobis1936generalized} and defined in Equation~\ref{eq:maha:orig}, where $\bm{\Sigma}$ is a covariance matrix of the data defined in Equation~\ref{eq:cov}. However this name is now overloaded and Mahalanobis distance now stands for any metric parameterised by a matrix $\bm{M}$ as defined in Equation~\ref{eq:maha}.

\begin{align} \label{eq:maha:orig}
d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^\top\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})}  \\
\bm{\Sigma} &= \mathrm {E} \left[\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)^{\rm {T}}\right] \label{eq:cov}
\end{align}

\begin{equation} \label{eq:maha}
d_{\bm{M}}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^\top\bm{M}(\textbf{x}-\textbf{y})} 
\end{equation}

The problem of correlation between dimensions does not arise in the Mahalanobis distance (Equation~\ref{eq:maha:orig}) because it decorrelates the data intrinsically before calculating the distance itself. If the Cholesky decomposition is applied directly inside of the Equation~\ref{eq:maha:orig}, then the Mahalanobis distance of the original data is equal to the Euclidean distance of the decorrelated data as shown in Equation~\ref{eq:mahalanobis:decorrelation}.

\begin{align}
  d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}(\bm{L}\bm{L}^{T})^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{-T}\bm{L}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})^{T}(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}^{-1}\textbf{x}, \bm{L}^{-1}\textbf{y}) \label{eq:mahalanobis:decorrelation}
\end{align}

Therefore from Equation~\ref{eq:mahalanobis:decorrelation} it is possible to see that the Mahalanobis distance has the following properties:
\begin{enumerate}
\item it automatically accounts for the scaling of the coordinate axes
\item it corrects for correlation between different features
\item it reduces to the Euclidean distance for uncorrelated data with unit variance
\end{enumerate}

The main weakness with the original Mahalanobis distance using the covariance matrix (Equation~\ref{eq:maha:orig}) is that it does not take the labels of the data instances into account (it can be thought of as ``unsupervised'' method). For this reason new metric learning algorithms exploiting additional information about data were developed and the term Mahalanobis distance now refers to not only decorrelation of the data using the Covariance matrix but to any linear transformation using different matrices as defined in Equation~\ref{eq:maha}.

From a mathematical perspective, a metric is a function $d(x,y)$ that defines a distance or dissimilarity between two instances from the input space: $d:\mathcal{X} \times \mathcal{X} \mapsto [0,\inf)$. For a function $d$ to be a metric, it has to fulfil these conditions:

% https://en.wikipedia.org/wiki/Metric_(mathematics)
\begin{align}
d(\textbf{x},\textbf{y}) &\geq 0 & nonnegativity \label{eq:metricdef-1} \\
d(\textbf{x},\textbf{y}) &= 0 \iff \textbf{x}=\textbf{y} & identity \ of \ indiscernibles \label{eq:metricdef-2} \\
d(\textbf{x},\textbf{y}) &= d(\textbf{y},\textbf{x}) & symmetry \label{eq:metricdef-3} \\
d(\textbf{x},\textbf{z}) &\leq d(\textbf{x},\textbf{y}) + d(\textbf{y},\textbf{z}) & triangle \ inequality \label{eq:metricdef-4}
\end{align} 

In order for the Mahalanobis distance (Equation~\ref{eq:maha}) to be a metric, the matrix $\bf{M}$ that parametrizes the distance needs to fulfil the conditions in Equations~\ref{eq:metricdef-1}---\ref{eq:metricdef-4}. A strictly \ac{pd} matrix $\bm{M} \succ 0$ fulfils all conditions and therefore the Mahalanobis distance with a \ac{pd} matrix is a distance metric.

Sometimes, however, it is not easy to guarantee the positive definiteness of a matrix. For this reason the \textit{identity of indiscernibles} condition (Equation~\ref{eq:metricdef-2}) is usually relaxed into a condition of \textit{identity} (Equation~\ref{eq:pseudodef-2}) which allows $d(x,y)=0$ for two distinct values $x \neq y$. Together with the rest of the conditions, this rule forms a new set of conditions (Equations~\ref{eq:pseudodef-1}---\ref{eq:pseudodef-4}) which together define a \textit{pseudo-metric}. Therefore a Mahalanobis distance with a \ac{psd} matrix $\bf{M} \succeq 0$ is a pseudo-metric, which is enough in most applications.

\begin{align}
d(\textbf{x},\textbf{y}) &\geq 0 & nonnegativity \label{eq:pseudodef-1} \\
d(\textbf{x},\textbf{x}) &= 0 & identity \label{eq:pseudodef-2} \\
d(\textbf{x},\textbf{y}) &= d(\textbf{y},\textbf{x}) & symmetry \label{eq:pseudodef-3} \\
d(\textbf{x},\textbf{z}) &\leq d(\textbf{x},\textbf{y}) + d(\textbf{y},\textbf{z}) & triangle \ inequality \label{eq:pseudodef-4}
\end{align} 

If the matrix $\bm{M}$ in the generalised Mahalanobis distance is \ac{pd}, it can be decomposed using a Cholesky decomposition into $\bm{\Sigma}=\bm{L}^\top\bm{L}$ and then the same trick from Equation~\ref{eq:mahalanobis:decorrelation} can be used as shown in Equation~\ref{eq:mahalanobis:transform}. Therefore every Mahalanobis distance with a \ac{pd} matrix can be replaced by a linear transformation of the original space using a matrix $\bm{L}$ and then calculating the Euclidean distance in the transformed space. This interesting feature of Mahalanobis distance allows us to look at metric learning algorithms also as a data transformation or data preprocessing.

\begin{align}
  d_{\bm{M}}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{T}\bm{L}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}\textbf{x}-\bm{L}\textbf{y})^{T}(\bm{L}\textbf{x}-\bm{L}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}\textbf{x}, \bm{L}\textbf{y}) \label{eq:mahalanobis:transform}
\end{align}

Note that if the $\bm{M}$ matrix is low rank, i.e., $rank(\bm{M})=k<d$ then the linear projection using the matrix $\bm{L}$ will transform the data onto a lower-dimensional ($k$-dimensional) space. This allows for a more compact representation of the data and cheaper distance computations inside of the low-dimensional space.

Learning Mahalanobis distance comes with two challenges:
\begin{enumerate}
\item ensuring that the matrix $\bm{M}$ stays \ac{psd} during the learning process. A~simple way to ensure this constraint is to use a projection onto a \ac{psd} cone by setting the negative eigenvalues to zero, which is described in~\citep{qian2015efficient}. Still, the eigenvalue decomposition takes $\mathcal{O}(d^3)$ and therefore is intractable for high-dimensional datasets.
\item learning a low-rank matrix $\bm{M}$ instead of the full-rank one. This would imply having the transformation to the low-dimensional space with all the advantages described earlier. Unfortunately, finding a minimum-rank solution of a semidefinite program is NP-hard as described in~\citep{lemon2016low}.
\end{enumerate}

\section{Outline}
This work extends upon the ideas from DML and applies them to the problem of dimensionality reduction and visualisation. The connection between dimensionality reduction and DML and the motivation for this work is quite simple. Given a generalised Mahalanobis distance $d_{\bm{M}}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})}$ with a \ac{pd} matrix $\bm{M}$ and its Cholesky decomposition $\bm{M}=\bm{L}^\top\bm{L}$, it holds that $d_{\bm{M}}(\textbf{x},\textbf{y}) = d_{2}(\bm{L}\textbf{x},\bm{L}\textbf{y})$, where $d_2$ is the Euclidean distance. Therefore, the matrix $\bm{L}$ is a mapping from an original space $O$ to another space $T$. This notion can be generalised further to look for general mappings $f: O \to T$. An example of such a mapping is a linear mapping. In such a case the matrix $\bm{L}$ may have a lower rank and thus it maps the points of the original space to a lower-dimensional space. Another example of mapping may be a neural network, which makes a non-linear transformation of $O$ to $T$. The distances of points in $O$ are then computed as Euclidean distances of their projections to $T$. Mostly mappings to low-dimensional spaces suitable for visualisation are discussed in this work, however, most of the ideas can be extended to higher dimensions, and thus to dimensionality reduction in general. The techniques described here are supervised, and the mappings seek to optimize the accuracy of the \ac{knn} classifier applied in the low-dimensional space. The motivation is that in case the \ac{knn} classifier has a high accuracy, instances from the same class are close together.

Current approaches to \ac{dml} are described in Chapter~\ref{chap:rw}, the notion of \ac{dml} is generalised to any general mapping and our own method for classification and dimensionality reduction is proposed in Chapter~\ref{chap:our-method}. In Chapter~\ref{chap:exp} our method is compared to other state-of-the-art methods in several different tasks such as classification, visualisation, and time-performance. We conclude our work in Chapter~\ref{chap:conclusion} and suggest some ideas for further investigation and research in Chapter~\ref{chap:future-work}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related work} \label{chap:rw}

As already mentioned in Chapter~\ref{chap:intro}, \ac{dml} has been gaining traction in the recent years and it has found many applications in many fields as discussed in Section~\ref{chap:intro:applications}. The survey from~\citep{bellet2013survey} is a comprehensive review of the supervised Mahalanobis distance learning methods. In the survey categorises \ac{dml} methods into the following categories: 
\begin{itemize}
\item \textit{Linear methods} are the simplest methods, which are easy to optimise and are less prone to overfitting. Typical example is Mahalanobis distance.
\item \textit{Online methods} process the samples piece-by-piece, without having the entire input available from the start.
\item \textit{Multi-Task methods} learn the metric for multiple related tasks at once to improve the performance.
\item \textit{Nonlinear methods} no longer have linear metric; it is typically achieved by kernelisation of a linear method.
\item \textit{Local Metric methods} learn multiple metrics for different parts of the space.
\item \textit{Semi-Supervised methods} in are designed to work well in semi-supervised paradigm, where most of the instances do not have labels.
\end{itemize}
This work mostly focuses on linear \ac{dml} methods because the rest of the categories extend these simpler methods and build on top of them.

The Mahalanobis distance originally proposed by~\citep{mahalanobis1936generalized} (Equation~\ref{eq:maha:orig}) uses a Covariance matrix of the data and can be seen as a first \ac{dml} method. Even though it is unsupervised, it still learns from the data. Since then the term Mahalanobis distance has been generalised and is now used with any \ac{psd} matrix, not just Covariance, discussed in more detail in Section~\ref{chap:intro:mah}. This generalised Mahalanobis distance is now used in many linear \ac{dml} methods.

The individual \ac{dml} methods that were mentioned here will be described in the next sections with more detail.

\section{MMC Xing method} \label{chap:rw:xing}
The work of~\citep{xing2002distance} is the first Mahalanobis distance learning algorithm leveraging information about the data. It is a weakly supervised method, which takes information in form of pairs of similar and dissimilar instances as defined in Equations~\ref{eq:similar} and~\ref{eq:dissimilar}. The authors formulate a convex optimisation problem that tries to maximize the distance between dissimilar pairs of instances while keeping the sum of the distances between similar pairs as low as possible ($\leq 1$):

\begin{align*}
\max_{\bm{M} \in \mathbb{S}^{d}_+} \quad & \sum_{(x_i,x_j)\in \mathcal{D}} d_{\bm{M}}(x_i,x_j) \\
\text{s.t.} \quad & \sum_{(x_i,x_j)\in \mathcal{S}} d_{\bm{M}}^2(x_i,x_j) \leq 1 \\
& \bm{M} \succeq 0
\end{align*}

This convex program is solved by using gradient ascend to maximize the distances between dissimilar instances, followed by projections to ensure the conditions of the program are held (matrix $\bm{M}$ must stay \ac{psd} and the distances between similar pairs must stay low).

\section{k-Nearest Neighbour classifier} \label{alg:knn}

\ac{knn} algorithm is a simple non-parametric method for classification and regression. In classification, an unknown sample is classified by a majority vote of $k$ nearest training samples. The pseudocode for classification is shown in Algorithm~\ref{pseudo:knn}. In regression, the output is an average of the values of $k$ nearest neighbours.

\begin{algorithm}[t]
\caption{Pseudocode for classification using \ac{knn}} \label{pseudo:knn}
\DontPrintSemicolon
\LinesNumbered
\KwData{set of labelled instances $\mathcal{Z}=\{\bm{z}_i=(\bm{x}_i, y_i): \bm{x}_i \in \mathcal{X}, y_i \in \mathcal{Y}\}^n_{i=1}$, where $\mathcal{X}$ are training data and $\mathcal{Y}$ are their class labels}
\KwIn{unknown sample $\bm{x} \in \mathcal{X}$, number of neighbours $k$}
\KwResult{class label of the unknown sample $\bm{x}$}
\Begin{
  \For{$i\leftarrow 1$ \KwTo $m$}{
    \lnl{pseudo:knn:dist} $distances[i]$ $\leftarrow$ compute distance d($\bm{z}_i$, $\bm{x}$)\;
  }
  \Return majority class from the $k$ smallest $distances$\;
}
\end{algorithm} 

The performance of the \ac{knn} largely depends on the metric used. Some \ac{dml} methods are designed to directly improve \ac{knn} classifier effectiveness. Any metric can be used in the algorithm simply by changing the distance metric (Line~\ref{pseudo:knn:dist} of Algorithm~\ref{pseudo:knn}). Modifying the \ac{knn} to use a custom metric is trivial even for faster implementations using tree data structures for nearest neighbour search.

\section{Large Margin Nearest Neighbour} \label{chap:rw:lmnn}

\Acf{lmnn} method designed by~\citep{weinberger2009distance} is a very popular method that has been improved by work of many other researchers, such as Multiple-Metrics \ac{lmnn}~\citep{weinberger2008fast}, Multi-task \ac{lmnn}~\citep{parameswaran2010large} and Gradient-Boosted \ac{lmnn}~\citep{kedem2012non}. The goal of this method is to improve the performance of the \ac{knn} classifier, the idea is that $k$-nearest neighbours with the same class should stay together while separating the imposters (instances with a different class) by a large margin. The algorithm works by defining the constraints in a local way: optimising only over similar points that are also in a close neighbourhood of each other as defined in Equations~\ref{eq:rw:lmnn:similar}---\ref{eq:rw:lmnn:dissimilar}. This neighbourhood is usually determined using Euclidean distance.

\begin{align}
\mathcal{S} &= \lbrace(\bm{x}_i,\bm{x}_j): y_i = y_j \text{ and } \bm{x}_j \text{ is in the }k\text{-neighbourhood of } \bm{x}_i \rbrace \label{eq:rw:lmnn:similar} \\
\mathcal{R} &= \lbrace(\bm{x}_i,\bm{x}_j,\bm{x}_k): (\bm{x}_i,\bm{x}_j) \in \mathcal{S}, y_i \neq y_k \rbrace \label{eq:rw:lmnn:dissimilar}
\end{align}

The distance metric is then learnt using the convex program defined in Equations~\ref{eq:rw:lmnn}---\ref{eq:rw:lmnn:last}. The authors exploited the locality of the constraints and designed their own solver, able to solve the program even with billions of constraints. The $\mu$ parameter controls the pull/push trade-off which influences how much the algorithm will push the imposters away and pull the similar instances together.

\begin{align}
\min_{\bm{M} \in \mathbb{S}^{d}_+} \quad & (1-\mu)\sum_{(\bm{x}_i,\bm{x}_j)\in \mathcal{S}} d_{\bm{M}}^2(\bm{x}_i,\bm{x}_j) + \mu \sum_{i,j,k} \xi_{ijk} \label{eq:rw:lmnn} \\
\text{s.t.} \quad & d_{\bm{M}}^2(\bm{x}_i,\bm{x}_k) - d_{\bm{M}}^2(\bm{x}_i,\bm{x}_j) \geq 1 - \xi_{ijk} \quad \forall(\bm{x}_i,\bm{x}_j,\bm{x}_k) \in \mathcal{R} \label{eq:rw:lmnn:last}
\end{align}

Figure~\ref{fig:rw:lmnn} illustrates how \ac{lmnn} works. The left diagram shows neighbourhood of the points before training and the right diagram after training. The distance metric is optimised so that the points lie inside of a smaller neighbourhood, while the instances with different classes are pushed outside of this neighbourhood.

\cenfig{graphs/lmnn}{Illustration of the input space before and after training using \ac{lmnn}}{fig:rw:lmnn}

% \section{Information Theoretic Metric Learning (ITML)} \label{chap:rw:itml}
%~\citep{davis2007information}

% \section{Sparse Determinant Metric Learning (SDML)} \label{chap:rw:sdml}
%~\citep{qi2009efficient}

% \section{Least Squares Metric Learning (LSML)} \label{chap:rw:lsml}
%~\citep{liu2012metric}

% \section{Relative Components Analysis (RCA)} \label{chap:rw:rca}
%~\citep{shental2002adjustment}

\section{Neighbourhood Components Analysis} \label{chap:rw:nca}
\citep{jacobgoldberger2004neighbourhood} proposed another method designed to improve the performance of the \ac{knn} classifier called \acf{nca}. The algorithm works by calculating the leave-one-out error for a stochastic version of the \ac{knn} classifier which uses Mahalanobis distance. Instead of searching for $k$-nearest neighbours, it uses a softmax probability distribution, defined in Equation~\ref{eq:rw:nca:softmax}, which is used to weight all the neighbours while favouring the closer ones. Note that the hyperparameter $k$ is no longer needed in this stochastic setting.

\begin{equation}
p_{ij} = \frac{ \exp(-\lVert \bm{L}x_i-\bm{L}x_j \rVert_2^2) }{ \sum_{l \neq i} \exp(-\lVert \bm{L}x_i-\bm{L}x_l \rVert_2^2) }, p_{ii}=0 \label{eq:rw:nca:softmax}
\end{equation}

The objective of \ac{nca} is to maximize the expected number of correctly classified points, defined in Equation~\ref{eq:rw:nca:objective}. Thanks to the stochastic definition of the neighbour search, this function is continuous and differentiable. Therefore a gradient descend algorithm can be used to optimize it. The main disadvantage of this method is that the objective function is non-convex and therefore \ac{nca} optimisation can suffer from local minima and the performance depends on the initialisation of matrix $\bm{L}$. 

\begin{equation}
\max_{\bm{L}} \sum_i \sum_{j:y_j=y_i} p_{ij} \label{eq:rw:nca:objective}
\end{equation}
% \bm{L}_{NCA} &= \argmax_{\bm{L} \in \mathbb{R}^{d \times r}} \bigg( \sum_{i=1}^n \sum_{j:y_j=y_i} p_{ij}(\bm{L}\bm{L}^\top) \bigg)

The algorithm is also valid if the matrix $\bm{L}$ is chosen to be rectangular and therefore \ac{nca} can also be used for dimensionality reduction by using the matrix $\bm{L}$ to project the original space into a lower-dimensional Euclidean space.

\section{Local Fisher Discriminant Analysis} \label{chap:rw:lfda}
\acf{lfda}~\citep{sugiyama2007dimensionality} is a combination of \acf{lda} method~\citep{fisher1936use} and \acf{lpp} method~\citep{he2003locality}. \ac{lda} works by maximising the amount of between-class variance relative to the amount of within-class variance. This might be problem when instances from one class form more clusters because these variances are calculated globally in \ac{lda}. \ac{lfda} improves this by introducing locality in the between-class and within-class variances using an affinity matrix from \ac{lpp}. The affinity matrix is a square matrix where $\bm{A}_{i,j} \in [0,1]$ and $\bm{A}_{i,j}$ is small when $\bm{x}_i$ and $\bm{x}_j$ are close and it is large when they are far apart. If $\forall i,j: \bm{A}_{i,j}=1$ then this algorithm degrades to the original \ac{lda}.

The within-class and between-class covariance matrices ($\widetilde{\bm{S}}^{(w)}$ and $\widetilde{\bm{S}}^{(b)}$, respectively) for \ac{lfda} are defined in Equations~\ref{eq:rw:lfda:within} and~\ref{eq:rw:lfda:between}. Note that the affinity matrix $\bm{A}$ is not used to weight the instances from different classes in Equations~\ref{eq:rw:lfda:wwithin} and~\ref{eq:rw:lfda:wbetween} because these instances should be always separated.

\begin{align}
\widetilde{\bm{S}}^{(w)} &= \frac{1}{2} \sum_{i,j=1}^n \widetilde{\bm{W}}_{i,j}^{(w)} (x_i-x_j)(x_i-x_j)^\top \label{eq:rw:lfda:within}
\\
\widetilde{\bm{S}}^{(b)} &= \frac{1}{2} \sum_{i,j=1}^n \widetilde{\bm{W}}_{i,j}^{(b)} (x_i-x_j)(x_i-x_j)^\top \label{eq:rw:lfda:between}
\\
\widetilde{\bm{W}}_{i,j}^{(w)} &= \twopartdef{\bm{A}_{i,j}/n_l}{y_i=y_j=l}{0}{y_i \neq y_j} \label{eq:rw:lfda:wwithin}
\\
\widetilde{\bm{W}}_{i,j}^{(b)} &= \twopartdef{\bm{A}_{i,j}(1/n-1/n_l)}{y_i=y_j=l}{1/n}{y_i \neq y_j} \label{eq:rw:lfda:wbetween}
\end{align}

The final transformation matrix $\bm{L}$ for \ac{lfda} is defined exactly the same way as for \ac{lda} (Equation~\ref{eq:rw:lfda:objective}). This matrix transforms the space so that nearby data pairs with the same class are pulled closer together, meanwhile the pairs with different classes are pushed further apart. The data pairs with the same class but far away from each other in the original space are not forced closer together. The transformation matrix can be obtained by solving the generalised eigenvalue decomposition of the scatter matrices and the authors offer an efficient implementation.

\begin{equation}
\bm{L}_{LFDA} = \argmax_{\bm{L} \in \mathbb{R}^{d \times r}} \bigg[\tr\Big( ( \bm{L}^\top\widetilde{\bm{S}}^{(w)}\bm{L} )^{-1} \bm{L}^\top\widetilde{\bm{S}}^{(b)}\bm{L} \Big)\bigg] \label{eq:rw:lfda:objective}
\end{equation}

Both \ac{lda} and \ac{lpp} methods can be used for dimensionality reduction and so can be \ac{lfda} by restricting the matrix $\bm{L}$ to be rectangular.

\section{Evolutionary distance metric learning} \label{chap:rw:fukui}

%TODO what is the difference between Differential Evolution and Evolution Strategy? DE vs ES???

\Ac{dml} paired together with evolutionary algorithms has also been gaining more attention recently. For example the work of~\citep{koloseni2012optimized} uses evolutionary algorithms to select the best possible metric out of a predefined list to improve a classification error. The work of~\citep{fukui2013evolutionary} also uses evolutionary algorithms, but takes a different approach and instead of choosing a metric from a predefined list, they try to evolve a Mahalanobis distance matrix. Differential evolution is used in which an individual encodes the Mahalanobis matrix, which is symmetric and so it can be encoded as a real-valued vector of length $d(d+1)/2$.

Evolutionary algorithm is typically used when the objective function is non-differentiable and non-continuous. A typical schema of an evolutionary algorithm is shown in Figure~\ref{fig:rw:ea:schema}: a population of individuals is randomly initiated and evaluated using the objective (fitness) function. Then the algorithm breeds new individuals through mutation and crossover, the individuals are evaluated and a new population of the individuals is selected. This whole process is repeated until a termination rule is met.

% Source: http://www.maths.uq.edu.au/MASCOS/Multi-Agent04/Fleetwood.pdf
\cenfig{graphs/ea_scheme}{Schema of an evolutionary algorithm}{fig:rw:ea:schema}

Differential evolution is a special case of an evolutionary algorithm which is a stochastic, population based algorithm suitable for solving a real-valued objective function with many parameters. Differential evolution is an evolutionary algorithm with special mutation and crossover operations. The pseudocode for differential evolution is shown in Algorithm~\ref{pseudo:de}. It follows the schema from Figure~\ref{fig:rw:ea:schema} and the special crossover operation is defined on Line~\ref{pseudo:de:mut}, where three different individuals are combined to create a new offspring.

Differential evolution algorithm has two hyperparameters apart from the size of the population: crossover probability $CR$ and differential weight $F$. Differential evolution can be very sensitive to the choice of these two hyperparameters.~\citep{fukui2013evolutionary} uses an improved version of differential evolution called \acf{jde}~\citep{brest2006self}, which alleviates this problem by encoding the hyperparameters $CR$ and $F$ inside of every single individual. When an offspring is created using the mutation and crossover, the $CR$ and $F$ parameters from the parent individuals are used. Therefore these two hyperparameters do not need to be specified for \ac{jde} and the algorithm itself is more robust.

\begin{algorithm}[t]
\caption{Pseudocode for differential evolution} \label{pseudo:de}
\DontPrintSemicolon
\LinesNumbered
\KwIn{fitness function $f$, crossover probability $CR \in [0,1]$, differential weight $F \in [0,2]$}
\KwResult{individual $\bm{x}$ that maximizes $f$}
Initialise all agents with random positions in the search-space.\;
\Repeat{a termination criterion is met}{
  \For{each agent $\bm{x}$ in the population}{
    Randomly select three distinct agents $\bm{a}, \bm{b}$, $\bm{c}$ from the population\;
    Randomly select index $R \in \{1,\ldots ,n\}$\;
    \For{$i\leftarrow 1$ \KwTo $n$}{
      \eIf{$rand()\equiv U(0,1) \leq CR$ or $i = R $}{$\bm{y}_i=\bm{a}_{i}+F\times (\bm{b}_{i}-\bm{c}_{i})$ \label{pseudo:de:mut}}
      {$\bm{y}_{i}=\bm{x}_{i}$}
    }
    \lIf{$f(\bm{y})<f(\bm{x})$}{replace $\bm{x}$ with $\bm{y}$ in the population}
  }
}
\end{algorithm} 

To ensure that the evolved matrix is \ac{psd} during the evolution, the authors ``repair'' the matrix so that it is diagonally dominant using a rule defined in Equation~\ref{eq:rw:jde:repair}. Diagonally dominant matrix with positive diagonal elements is \ac{psd}.

\begin{equation}
d_{i,j}^{repair} = \frac{m_{i,j}}{\sum_j \abs{m_{i,j}}}, \; (i \neq j)
\label{eq:rw:jde:repair}
\end{equation}

\citep{fukui2013evolutionary} focuses on improving clustering algorithms and thus uses external clustering indexes as the fitness function. External clustering indexes, compared to internal clustering indexes, use the class labels of the data for the evaluation. In their work they use purity, \mbox{F-measure} (harmonic mean of precision and recall of the classes assigned by clustering) and entropy. The authors also use a so called ``neighbour relation'' to improve the performance of the indexes by incorporating a weighted function inside of the indexes. 

\mbox{F-measure} performed the best in their experimental results and so we chose to experiment with this index. \mbox{F-measure} is defined in Equations~\ref{eq:rw:fme}---\ref{eq:rw:fme:last} where $C$ is a cluster set, $Y$ are class labels, $N_{s,i}$ is number of data points with class $s \in Y$ and in $i^{th}$ cluster, $y(k)$ and $c(k)$ denote the class/cluster assignment for $x_k$.

\begin{align}
Fme(s,C_i) &= \frac{2 \cdot Prec(s,C_i) \cdot Rec(s, C_i)}{Prec(s,C_i) + Rec(s, C_i)} \label{eq:rw:fme} \\
Prec(s,C_i) &= N_{s,i}/N_i \\
Rec(s,C_i) &= N_{s,i}/N_s \\
N_{s,i} &= \#\{x_k | y(k)=s, c(k)=C_i\} \\
N_s &= \#\{x_k | y(k)=s \in Y\} \\
N_i &= \#\{x_k | c(k)=C_i\} \label{eq:rw:fme:last}
\end{align}

The weighted versions of the previous definitions are shown in Equations~\ref{eq:rw:wfme}---\ref{eq:rw:wfme:last}. $h_{i,j}$ can be any monotonically decreasing function, the authors used Gaussian function $h_{i,j}=\exp(-r_{i,j}/\sigma)$, where $r_{i,j}$ is inter-cluster distance between $C_i$ and $C_j$, and $\sigma$ is a smoothing radius.

\begin{align}
wFme(C) &= \sum_{s \in Y} \frac{N_s}{N} \max_{C_i \in C} Fme(s,C_i) \label{eq:rw:wfme} \\
Prec(s,C_i) &= \widetilde{N}_{s,i}/\widetilde{N}_i \\
Rec(s,C_i) &= \widetilde{N}_{s,i}/N_s \\
\widetilde{N}_{s,i} &= \sum_{C_j \in C} h_{i,j}N_{s,j} \\
\widetilde{N}_i &= \sum_{s\in Y}\widetilde{N}_{s,i}=\sum_{s\in Y}\sum_{C_j \in C} h_{i,j}N_{s,j} \\
\widetilde{N} &= \sum_{C_i \in C}\widetilde{N}_i = \sum_{C_i \in C}\sum_{s\in Y}\sum_{C_j \in C} h_{i,j}N_{s,j} \label{eq:rw:wfme:last}
\end{align}

After certain number of generations of the evolutionary algorithm, the individual with the best fitness value is selected and the matrix encoded in this individual is used for the final Mahalanobis distance.

\section{Dimensionality Reduction}

Metric learning can also be used for dimensionality reduction as already discussed in Chapter~\ref{chap:intro}. This work mainly focuses on dimensionality reduction into two dimensions, where it is easy to visualise the data. \Ac{nca} and \ac{lfda} were described earlier in this chapter. These algorithms perform dimensionality reduction by restricting the shape of the transformation matrix to be a rectangular with the number of columns corresponding to the dimension of the target space.

One of the most common and the simplest unsupervised dimensionality reduction method is \ac{pca}~\citep{jolliffe2002principal} which decomposes the dataset in a set of successive orthogonal components that explain maximum amount of the variance inside of the data. The first principal component can be obtained by solving Equation~\ref{eq:rw:pca}; it can be shown that the solution is the eigenvector of $\bm{X}^\top\bm{X}$ corresponding to its largest eigenvalue.

\begin{equation} 
\bm{w}_{(1)} = \argmax_{||\bm{w}||=1} \bm{w}^\top\bm{X}^\top\bm{X}\bm{w} \label{eq:rw:pca}
\end{equation}

If all principal components are found and \ac{pca} is used to transform the data while keeping all the components (not performing dimensionality reduction), then the Euclidean distances between the samples in the new \ac{pca}-transformed space will equal their Mahalanobis distances in the observed-variable space.

Another dimensionality reduction method is \ac{tsne}~\citep{maaten2008visualizing}, which maps the points from the high-dimensional space to a low-dimensional one. The mapping aims to minimize the Kullback-Leibler divergence of the distribution of distances between points in the high and low-dimensional spaces.

\ac{tsne} is gaining a lot of attention recently, although it does not provide any explicit mapping between the two spaces, therefore it cannot be used to map new data into the lower-dimensional space without re-running the whole method. In the experiments, however, we also show the visualisations obtained by t-SNE and compare them to those obtained by the method presented in this work.

\newpage

\section{Summary of DML methods}

The most prominent \ac{dml} methods were described in the previous sections. We experimented with many other \ac{dml} methods from the survey by~\citep{bellet2013survey}, however, in our preliminary experiments we found that methods that were summarised here, perform the best.

Table~\ref{tab:rw:summary} shows methods described in this chapter. These methods are used further in our experiments.

\begin{table}[ht] \centering
\begin{tabular}{llllll}
\hline
Name & Year & Supervision & Optimum & Regularizer & Notes \\
\hline
Covariance & 1936 & None & Global & None & Unsupervised \\
MMC & 2002 & Weak & Global & None & — \\
LMNN & 2005 & Full & Global & None & Improving \ac{knn} \\
NCA & 2004 & Full & Local & None & Improving \ac{knn} \\
LFDA & 2007 & Full & Global & None & — \\
% RCA & 2003 & Weak & Global & None & — \\
% ITML & 2007 & Weak & Global & LogDet & Online version \\
% SDML & 2009 & Weak & Global & LogDet+L1 & $n \ll d$ \\
jDE.wFme & 2013 & Full & Local & None & Evolutionary \\
\hline
\end{tabular}
\caption{Summary of related metric learning methods} \label{tab:rw:summary}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Generalising Evolutionary Distance Learning} \label{chap:our-method}

The aim of this work is to find a mapping from the original space $O$ of the input data, to a (potentially low-dimensional) space $T$. The mapping $f: O \to T$ is then used in the distance metric found by the algorithm. The evolved metric is defined as $d_f(x, y) = d_2(f(x), f(y))$, where $d_2$ is the Euclidean metric in the $T$ space. It was already mentioned that in case $f$ is represented by a lower triangular matrix $\bm{L}$, $d_f$ represents a Mahalanobis distance with matrix $\bm{L}^\top\bm{L}$. However, the mappings may be more general. If the mapping is represented by a rectangular matrix, the technique can be used for dimensionality reduction. Moreover, we can also look for a non-linear mapping $f$, which is able to transform the input data in such a way that it is easier for the classifier to make the correct prediction.

Only mappings which can be represented as vectors or matrices of real numbers are considered in this work. Such mappings include both linear transformations and non-linear transformation such as neural networks (which can be represented as vectors containing their weights). The goal of the algorithm is to find a mapping that maximizes the performance of the classifier that follows the preprocessing step. In this work, \ac{knn} classifier is considered, however different classifiers can also be used. The performance of the \ac{knn} classifier is used as the fitness in the evolutionary algorithm. One of the reasons is that it is relatively fast, it takes $N\log{N}$ to build the underlying k-d tree structure with $N$ training samples and then $M \log{N}$ to classify $\bm{M}$ testing samples. Another advantage of using the \ac{knn} classifier is that the resulting mappings should also provide useful visualisations of the data. The \ac{knn} classifier is local, and therefore works best in case instances from the same class are close together.

The \ac{knn} classifier function is not differentiable and so we focus on finding the mapping using evolutionary algorithms. When the evolutionary algorithm is non-elitist (i.e. it can lose the best individual in the search), a concept called \ac{hof} is used, which acts as a storage of the best individuals. It is updated with the best individuals after every generation and only keeps the best ones across all generations.

The high level pseudocode for an evolutionary algorithm evolving the mapping is shown in Algorithm~\ref{pseudo:our-method}. First, all parameters are initialised randomly along with an empty \ac{hof}. The individuals are vectors of real numbers that encode the mapping. In case of evolving the matrix $\bm{L}$ for the Mahalanobis distance, this is a vector of $d^2$ numbers. In case of evolving the neural networks, the individual contains all the weights from the network. Then the evolutionary algorithm is run until a stopping criterion is met. In each generation new individuals are sampled and the fitnesses of the individuals are evaluated. The \ac{hof} and the internal parameters of the evolutionary algorithm are updated. After the evolution, the best individual from the \ac{hof} is returned. The individual contains the weights that represent the corresponding mapping.

\begin{algorithm}[t]
\caption{Evolving a mapping using an evolutionary algorithm} \label{pseudo:our-method}
\DontPrintSemicolon
\LinesNumbered
\Begin{
  Initialise parameters of the algorithm\;
  Initialise empty HOF\;
  \Repeat{a termination criterion is met}{
    Sample new individuals using the current parameters\;
    Evaluate fitnesses of the new individuals (described in Algorithm~\ref{pseudo:ea:knn})\;
    Update best individuals in \ac{hof}\;
    Update the parameters of the evolutionary parameter\;
  }
  \Return the best individual from the \ac{hof}\;
}
\end{algorithm} 

For evolving the mapping, we propose to use \ac{cmaes}, which is an algorithm for non-linear non-convex optimisation problems in continuous domain. \ac{cmaes} is considered to be state-of-the-art evolutionary algorithm for continuous optimisation. It does not require parameter tuning, the only hyperparameter is a population size, which comes with a reasonable default.

In our experiments the most common version $(\mu_W, \lambda)$-\ac{cmaes} is used, as described in~\citep{hansen2001completely}. In each iteration a weighted combination of $\mu$ best new candidates (out of $\lambda$) are used to update the distribution parameters. The reason for choosing \ac{cmaes} is that it only needs a small population to perform well compared to other algorithms. Evaluation of the fitness function is time consuming in our setting and therefore the size of the population directly influences the training time.

Recommended population size for \ac{cmaes} is $4+\floor{3\log{d}}$~\citep{hansen2006cma} whereas for \ac{jde} it is $10d$~\citep{brest2006self}, where $d$ is number of parameters to evolve. This makes \ac{jde} intractable for learning Mahalanobis distance in high-dimensional space, because it has to evaluate $10\cdot D(D+1)/2 \approx 5D^2$ ($D$ is dimensionality of the data) fitnesses in each generation, while \ac{cmaes} only needs to evaluate $4+\floor{3\log{D^2}} \approx 4+\floor{6\log{D}}$ fitnesses. An experiment was performed to verify that \ac{jde} does not perform well with the same population size as \ac{cmaes}. The experiment is discussed the next Chapter.

The most important step here is the evaluation of the fitness function, which is computed as the success rate of the \ac{knn} classifier after transforming the data into space $T$. The pseudocode is shown in Algorithm~\ref{pseudo:ea:knn}.

\begin{algorithm}[t]
\caption{\ac{knn} as a fitness function in an evolutionary algorithm} \label{pseudo:ea:knn}
\DontPrintSemicolon
\LinesNumbered
\KwData{set of labeled instances}
\KwIn{the individual $\bm{i}$ encoding the mapping}
\KwResult{the fitness value of the $\bm{i}$ individual}
\Begin{
  Split instances into train-test sets\;
  Extract the mapping to space $T$ from the individual $\bm{i}$\;
  Map the instances to the space $T$\;
  Initialise the \ac{knn} classifier in the space $T$\;
  Train the \ac{knn} classifier using the train set\;
  Perform the classification using the \ac{knn} on the test set\;
  \Return the success rate of the classification of the test set\;
}
\end{algorithm}

It is important to note that the splitting of the data into training and testing sets is done for each fitness evaluation and therefore the sets are split differently for each individual. The idea behind this is that the evolution should find different individuals that generalise well.

\Ac{dml} algorithms can also be used for dimensionality reduction as already discussed in Chapter~\ref{chap:rw}. Our mapping $f: O \to T$ is very general and can take any form. By evolving transformation matrix $\bm{L} \in \mathcal{R}^{m\times d}$, where $m < d$, a singular transformation matrix is obtained that will transform the original $d$-dimensional space into $m$-dimensional.

To perform a dimensionality reduction using the neural network, the shape and the activation functions need to be chosen beforehand, the only criteria is that the first layer has size $d$ and the last layer has the size of the target dimension $m$. Without activation functions a fully connected neural network essentially reduces to a linear transformation and can therefore be used in Mahalanobis distance. The individuals in the evolution contain weights corresponding to all parameters of the neural network. Typical activation functions used in neural networks are sigmoid function ($\sigma(x)$), \ac{tanh} and \ac{relu}, which are defined in Equations~\ref{eq:nn:sigm}---\ref{eq:nn:relu}. Experiments with the activation functions and the depth of the neural network used for visualisation are described in the next chapter.

\begin{align}
\sigma(x)      &= 1/(1+e^{-x}) \label{eq:nn:sigm} \\
\text{tanh}(x) &= \frac{e^x-e^{-x}}{e^x+e^{-x}} = 2\sigma(2x)-1 \label{eq:nn:tanh} \\
\text{ReLU}(x) &= \max(0,x) \label{eq:nn:relu}
\end{align}

The algorithm described here is similar to the algorithm described by~\citep{fukui2013evolutionary}, however, there are some important differences:
\begin{itemize}
\item~\citep{fukui2013evolutionary} focuses on improving clustering and used the \mbox{F-measure} as the fitness function. On the other hand, we deal with classification and therefore we minimize the error rate of the \ac{knn} classifier;
\item a mapping is evolved instead of the matrix for the Mahalanobis distance, therefore the matrix $\bm{L}$ does not have to be ``repaired'' during the evolution. Matrix $\bm{M}=\bm{L}^\top\bm{L}$ can be used in Mahalanobis distance, because it is \ac{psd}:

\begin{equation}
\bm{x}^\top \bm{L}^\top\bm{L}\bm{x}=(\bm{Lx})^\top(\bm{Lx})=(\bm{Lx})\cdot (\bm{Lx})\geq 0 \quad \forall \bm{x} \in \mathbb{R}^d, \bm{x} \neq 0 \label{eq:mah:proof}
\end{equation}

\item \ac{cmaes} is used instead of \ac{jde}, therefore smaller population size can be used, leading to faster optimisation (recommended population size for \ac{cmaes} is only $4+\floor{3\log{d}}$~\citep{hansen2006cma} whereas for \ac{jde} it is $10d$~\citep{brest2006self}, where $d$ is number of parameters to evolve).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%TODO FIX EVOLVING FULL VS TRIANGULAR

\chapter{Experiments and Examples} \label{chap:exp}
Five different experiments were performed to assess the performance of the metric learning algorithm presented in this work. In Section~\ref{chap:exp:classification} a metric for the \ac{knn} classifier is learnt and the classification errors are compared on various datasets, which are described in Section~\ref{chap:exp:datasets}. In the next experiment in Section~\ref{chap:exp:fitness} different evolutionary algorithms together with various fitnesses  are compared and examined to see how well they evolve and generalise, while in Section~\ref{chap:exp:logjde} the performance of different population sizes of \ac{jde} is compared. In Section~\ref{chap:exp:learning-times} the training times of the \ac{dml} algorithms are measured and in the last experiment in Section~\ref{chap:exp:dimred} metric learning algorithms are used for visualisation and the resulting embeddings are compared with more traditional methods, \ac{pca} and \ac{tsne}.

\section{Experimental settings} 
In our experiments metric learning algorithms listed in Chapter~\ref{chap:rw} (\ac{lmnn}, \ac{nca}, \ac{lfda}) are compared. They are implementated in Python \textit{metric-learn} library\footnote{\url{https://github.com/all-umass/metric-learn}}. We implemented a \ac{jde}-based method similar to~\citep{fukui2013evolutionary}, and our algorithm described in Chapter~\ref{chap:our-method}. The implementation is an extension of \textit{metric-learn} library and the source codes are available in our GitHub repository\footnote{\url{https://github.com/svecon/metric-learn/tree/ecml2017}}, but we hope the source codes will be accepted into the library. The scripts used to perform the experiments and create the results in this chapter are also available on GitHub\footnote{\url{https://github.com/svecon/metric-learn-playground/tree/ecml2017}}.

\subsection{Datasets} \label{chap:exp:datasets}
Datasets that were most commonly found in literature were used for the experiments, particularly in~\citep{xing2002distance},~\citep{weinberger2009distance},~\citep{jacobgoldberger2004neighbourhood} and~\citep{fukui2013evolutionary}. The most common among all these papers were \dataset{balance-scale}, \dataset{breast-cancer}, \dataset{iris}, \dataset{mice-protein}, \dataset{pima-indians}, \dataset{sonar} and \dataset{wine} datasets. All these datasets were obtained from a well-known UCI Machine Learning Repository\footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Among these datasets \dataset{mice-protein} and \dataset{sonar} have high dimensionality, nevertheless, they are small in terms of number of samples. We also decided to add \dataset{digits6} and \dataset{digits10} datasets, also obtained from the same archive, which are relatively larger datasets containing $8\times 8$ pixel images of digits with 6 and 10 classes, respectively.

In order to test the metric learning algorithms on a highly variable data, we also created an artificial dataset \dataset{gaussians} by sampling from four multinomial Gaussian distributions. Each Gaussian has a different centre, defined as a matrix in Equation~\ref{eq:gauss:means} where each row corresponds to one centre. All Gaussians were generated sharing one covariance matrix, defined in Equation~\ref{eq:gauss:cov}. Dimensions are uncorrelated, although, the first dimension has an enormous variability of $10^8$.

\begin{equation} \label{eq:gauss:means}
means = \begin{pmatrix}
10 & 0 & 0 & 2 \\
0 & 10 & 0 & -2 \\
0 & 0 & 10 & -2 \\
0 & 0 & -10 & 2 \\
\end{pmatrix}
\end{equation}
\begin{equation} \label{eq:gauss:cov}
covariance = \begin{pmatrix}
10^8 & 0 & 0 & 0 \\
0 & 100 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}

Table~\ref{tab:datasets} summarizes datasets that were experimented on. The table shows number of samples, dimensionality and number of distinct classes for every dataset. All the datasets are meant for classification task: all of their instances belong to a class. All the datasets have a labelled instances and so they can be used in supervised methods. Most of the attributes of the datasets are real numbers, but some of them also contain integral values. The largest dataset in terms of number of samples and number of classes is \dataset{digits10} dataset with 1797 samples. However, the largest dataset in terms of number of dimensions is \dataset{mice-protein} dataset. Most of the datasets are relatively low in both number of samples, which is under a thousand, and their dimension is usually under 10.

\begin{table}[ht] \centering
\begin{tabular}{lrrr}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Dataset & \#~samples & \#~dimensions & \#~classes \\
\hline
balance-scale           & 625   & 4    & 3  \\
breast-cancer           & 699   & 9    & 2  \\
digits6                 & 1083  & 64   & 6  \\
digits10                & \textbf{1797}  & 64  & \textbf{10} \\
gaussians               & 400   & 5   & 4  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & \textbf{77}   & 8  \\
pima-indians            & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
wine                    & 178   & 13   & 3  \\
%soybean-large           & 307   & 35   & \textbf{19} \\
%ionosphere              & 351   & 34   & 2  \\
%letters                 & 20000 & 16   & 26 \\
%mnist                   & 70000 & 784  & 10 \\
%ofaces                  & 400   & 4096 & 40 \\
\hline
\end{tabular}
\caption{Datasets overall summary} \label{tab:datasets}
\end{table}

\subsection{Data preprocessing} \label{chap:exp:preprocessing}
Some of the datasets have missing attributes. There are several strategies for dealing with missing attributes, such as removing affected samples, filling missing values with a random value or zeros, their mean, median or the most frequent value. We chose to fill missing values using the mean for any given attribute as we found it performs well with all the chosen datasets.

\begin{table}[ht] \centering
\begin{tabular}{lrrrrr}
\hline
Dataset & minimum & maximum & mean & std. deviation \\
\hline
balance-scale           & 1.00  & 5.00    & 3.00  & 1.41 \\
breast-cancer           & 1.00  & 10.00   & 3.13  & 2.88 \\
digits6                 & 0.00  & 16.00   & 4.87  & 6.04 \\
digits10                & 0.00  & 16.00   & 4.88  & 6.02 \\
gaussians               & \textbf{-33870.07} & \textbf{31033.73} & -7.65 & \textbf{5285.16} \\
iris                    & 0.10  & 7.90    & 3.46  & 1.97 \\
mice-protein            & -0.06 & 8.48    & 0.68  & 0.79 \\
pima-indians            & 0.00  & 846.00  & 44.99 & 58.37 \\
sonar                   & 0.00  & 1.00    & 0.28  & 0.28 \\
wine                    & 0.13  & 1680.00 & \textbf{69.13} & 215.75 \\
\hline
\end{tabular}
\caption{Datasets samples statistics} \label{tab:datasets-samples}
\end{table}

Another difficulty with the datasets is that many of them do not have normalised attributes and their attribute values are unbounded and have high variance. From Table~\ref{tab:datasets-samples} it is clear that the most variable dataset is our artificial \dataset{gaussians} dataset, however, \dataset{pima-indians} and \dataset{wine} datasets are also highly variable with  standard deviation of $58.37$ and $215.75$, respectively. The~datasets were normalised using standardisation defined in Equation~\ref{eq:stand}, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations.

\begin{equation} \label{eq:stand}
\hat{X} = \frac{X-\mu}{\sigma}
\end{equation}

\section{Experiment: Classification} \label{chap:exp:classification}

This experiment was designed to test how much a distance metric helps in classification task compared to standard Euclidean distance. The error rates were measured on \ac{knn} classifier. It is one of the simplest and yet powerful classifiers and it is very easy to modify to use a custom metric, discussed in Section~\ref{alg:knn}.

The metric learning algorithms tested in the experiments are 

\begin{itemize}
\item Covariance (original Mahalanobis distance with a covariance matrix defined in Equation~\ref{eq:maha:orig})
\item \ac{lmnn}, described in Section~\ref{chap:rw:lmnn}
\item \ac{nca}, described in Section~\ref{chap:rw:nca}
\item \ac{jdefme}, which is based on the method presented by~\citep{fukui2013evolutionary}

\item \ac{cmaesknn} proposed in Chapter~\ref{chap:our-method}

\item \acf{jdeknn}
\item \acf{cmaesfme}

\item Euclidean distance, acting as a baseline method
\end{itemize}

\ac{cmaesknn} should be able to obtain good results in this experiment as the mapping created by the algorithm aims to optimize the performance of the classifier directly.

\ac{cmaesfme} and \ac{jdeknn} were tested as well, in order to tell, what is the effect of the different optimisation algorithm and what is the effect of the different fitness function.

To assess performance of the metric learning algorithms, the datasets listed in Section~\ref{chap:exp:datasets} were used. The datasets were split into training and testing sets using 10-fold cross validation. For each fold data were first preprocessed and standardised as described in Section~\ref{chap:exp:preprocessing}, then the metric was trained using one of the methods and finally the metric was evaluated using \ac{knn} classifier.

We also wanted to test how standardisation influences the learnt metrics and whether the metric learning algorithms can handle unnormalised data and so all of the experiments were run again, but without standardising the data. The experiments on preprocessed data are prefixed by ``s:'' in the tables with the results of the metric learning experiment.

\subsection{Hyperperameter search} \label{chap:exp:hypsearch}

Every metric learning algorithm has a set of hyperparameters which highly influence the learnt metric. For each hyperparameter, sensible values from a reasonable range were selected. All hyperparameters for each algorithm are shown in Table~\ref{tab:hyperparams}. From the hyperparameter table it is clear that all the \ac{dml} algorithms (except Covariance and \ac{lfda}) are iterative (\textit{max\_iter} and \textit{n\_gen} hyperparameters) and the iterations were limited to 25, 50, 100, 250 and 1000 for all these iterative algorithms (only the maximum number of iterations was lowered to 500 for the evolutionary approaches on the larger datasets, as otherwise the computations take too long to finish). The transformer parameter means whether the algorithm uses the full matrix or it is restricted to a diagonal transformation matrix. Unfortunately the library did not offer learning metrics restricted to a diagonal matrix and so the restricted versions were tested only with the evolutionary methods that we implemented ourselves.

\input{results/hyperparameters}

Even for these sensibly chosen hyperparameter values there are thousands of possible combinations for every single algorithm and thus thousands of different models would have to be trained. Considering \ac{lmnn} as an example: there are 10 folds, 8 possible values for the final \ac{knn} classifier hyperparameter, \ac{lmnn} itself has 4 distinct hyperparameters with 6, 3, 4 and 3 values, which means there would be $10*8*(6*3*4*3)=17280$ different models to train just for \ac{lmnn} if a simple grid search was used. This is a major problem because some of the algorithms take a long time to train even on small datasets and trying that many combinations would take too long to test.

It can be noticed, however, that the metric does not need to be retrained for every hyperparameter of the final \ac{knn} classifier. Instead the metric can be trained first and only then \ac{knn} classifier can be trained and tested for each of the hyperparameters. Therefore, we designed our version of grid search that is hierarchical in a sense that it will train a metric with all combinations of hyperparameters first and only then it will evaluate these metrics using \ac{knn} with its own hyperparameters. There will still be the same total number of models, however, there will be 8 times less evaluations (number of the final classifier hyperparameter combinations) for every metric learning algorithm, which roughly translates to 8 times less computation time as training a \ac{knn} classifier is negligible compared to training \ac{dml} algorithm.

The population sizes for \ac{cmaes} and \ac{jde} were set to the recommended values ($4+\floor{3\log{d}}$ and $10d$, respectively). This gives an advantage to \ac{jde} in terms of number of fitness evaluations, however, in experiment in Section~\ref{chap:exp:logjde} we show that \ac{jde} does not perform well with the smaller population size.

\subsection{Results}

The error rates and their standard deviations (across 10 runs from the cross validation) of the final \ac{knn} classifier for every dataset and every method are presented in Table~\ref{tab:error-rates}. If the method name is prefixed with ``s:'' it means the dataset was standardised as described in Section~\ref{chap:exp:preprocessing}. The best average performance for each dataset is in bold. \ac{nca} did not finish on the 3 largest datasets due to the memory limitations and so the corresponding cells are marked with ``Memory''.

Some other methods did not finish because a numerical error occurred, these are marked as ``Error''. The Covariance method failed for some datasets because the matrix was not \ac{pd} and so it was not possible to use Cholesky decomposition. \ac{lfda} also failed for some datasets because a factorisation of a matrix failed during the calculation. \ac{nca} failed for \dataset{gaussians} and \dataset{mice-protein} datasets because the computation was unstable and the numbers overflowed to infinity during the computation.

\input{results/error-rates}

% https://en.wikipedia.org/wiki/Interquartile_range
The results are also plotted in Figures~\ref{fig:cl:sr1} and~\ref{fig:cl:sr2} using the box plots. Each dataset has its own graph where all the methods that finished correctly are drawn with their specific colour using the box plot graph. The median success rate is marked with a thick black line, the area of the box is an interquartile range (IQR) defined as the third quartile minus first quartile: $IQR = Q_3 - Q_1$. The whiskers extend $1.5*IQR$ to both sides of the box plot and anything beyond whiskers is considered to be an outlier value.

The results in the Table~\ref{tab:error-rates} and Figures~\ref{fig:cl:sr1} and~\ref{fig:cl:sr2} indicate that the standardisation of the data is generally a good idea, only datasets \dataset{digits6}, \dataset{digits10} and \dataset{iris} have the lowest error rates when unstandardised, although the standardised versions follow up with such a small margin that it is not a statistically significant difference. On the other hand, standardising data can have a large impact as can be seen on \dataset{gaussians}, \dataset{sonar} and \dataset{wine} datasets, where the difference between standardised and unstandardised data can be tens of percents. It shows that metric learning algorithms cannot fully replace the data standardisation.

Overall, three methods perform very well across all datasets: \ac{lmnn} with 6 best results, followed by \ac{cmaesknn} with 4 best results and \ac{lfda} with one best result. \ac{nca} method was not best in any dataset, but it closely followed the best methods. The differences between these methods are small and statistically insignificant. The statistical significance tables for all methods are attached in Attachments (Tables~\ref{tab:statsign:classification:balance-scale}---\ref{tab:statsign:classification:wine}). The worst method is clearly Covariance algorithm, possibly because it is an unsupervised method. It is also important to note that the Euclidean distance (a baseline) is outperformed by a metric learning algorithm on almost every dataset. This suggests metric learning algorithms can have a large impact in the machine learning tasks.

When comparing \ac{jde} and \ac{cmaes}, it can be noticed that the type of the fitness (\mbox{F-measure} or \ac{knn}) affects the performance more than the optimisation algorithm. However, \ac{jde} has an advantage in the population size, therefore we believe \ac{cmaes} is overall better than \ac{jde} for this task, as it has similar performance with much lower number of fitness evaluations. It is interesting to note that both \ac{jdefme} and \ac{jdeknn} won on the artificial \dataset{gaussians} dataset, however, they were closely followed by \ac{cmaesknn} method. \ac{cmaesfme} and \ac{jdefme} methods perform the worst on most of the datasets.

\cenfig{graphs/classification/sr_1.pdf}{Success rates for classification experiment on \dataset{balance-scale}, \dataset{breast-cancer}, \dataset{digits6}, \dataset{digits10}, \dataset{gaussians}, and \dataset{iris} datasets}{fig:cl:sr1}

\cenfig{graphs/classification/sr_2.pdf}{Success rates for classification experiment on \dataset{mice-protein}, \dataset{pima-indians}, \dataset{sonar}, and \dataset{wine} datasets}{fig:cl:sr2}

\clearpage

To see how the hyperparameters influence the success rate of \ac{knn} classifier, one hyperparameter is inspected at a time. By fixing this hyperparameter and inspecting its values, the best success rate across the rest of the hyperparameters is searched.

First we look at the $k$ hyperparameter in the final \ac{knn} classifier. In Figure~\ref{fig:cl:kpar} there is one graph for every dataset (\dataset{digits6} dataset is omitted because it performs very similarly to \dataset{digits10} dataset) in which each line corresponds to one algorithm. The best value for this hyperparameter differs for each dataset, however, most of the curves have a tip near $k=8$.

From individual graphs it is possible to see some trends among all the methods, in particular that the lines tend to copy a downwards parabola shape, meaning that $k$ values of 1 and 2 are too small to generalise well and values 64 and 128 are too large for these small datasets.

The best shape for any particular method would be a flat line near the top of the graph, which would imply that the method separated the instances into a well separable and homogeneous clusters in which all instances have the same label.

Some of the methods seem to handle the change of this parameter more gracefully. This is the most apparent with \ac{lmnn} and \ac{nca} methods in \dataset{mice-protein} dataset: these two methods have large success rates even when the $k$ parameter increases, whereas the other methods' success rates drop significantly. This phenomenon is visible in other datasets and even in other methods, for example in \dataset{gaussians} dataset for \ac{cmaesknn} as well as the two already mentioned.

The Covariance method has a bad success rate altogether, but it also drops down the fastest when the $k$ hyperparameter increases.

\cenfig{graphs/classification/sr_knn}{Success rates of \ac{dml} algorithms while fixing $k$ in the final \ac{knn} classifier}{fig:cl:kpar}

\clearpage

The hyperparameters of the individual methods are shown in Figure~\ref{fig:cl:hyp}. For each hyperparameter there is a different graph and each curve in a graph represents one dataset. These hyperparameters are shown in the nine graphs:

\begin{enumerate}
\item $k$ hyperparameter for \ac{lmnn}, which is number of neighbours that the method takes into account when optimising as discussed in Section~\ref{chap:rw:lmnn}. It is clear that this hyperparameter influences the success rate of this method by a large amount and it seems to peak at values between 4 and 16.

\item maximum number of iterations (max\_iter) for \ac{lmnn} from which it is clear that running the algorithm for 250 iterations works very well, but it starts to overfit slightly afterwards.

\item regularisation parameter for \ac{lmnn} also improves the success rate when it is higher for all datasets except \dataset{sonar}.

\item learning rate does not affect the performance of \ac{nca}

\item metric initialization for \ac{lfda} shows that both weighted and orthonormalised intialisations perform equally well.

\item transformer shape for \ac{cmaesknn}, shows a comparison between evolving a diagonal and full Mahalanobis matrix. For all except two datasets (\dataset{pima-indians} and \dataset{mice-protein}) evolving the full matrix worked better. Most likely \ac{cmaes} could not explore the whole $d^2$-dimensional space well because \dataset{pima-indians} is a high-dimensional dataset.

\item number of neighbours in \ac{cmaesknn}, shows it is best to select 8 neighbours.

\item weights in \ac{cmaesknn} do not influence the performace too much.

\item transformer shape for \ac{jdefme}, evolving a full matrix works clearly better for all datasets.
\end{enumerate}

\cenfig{graphs/classification/sr_hyp}{Success rates of \ac{dml} algorithms when fixing given hyperparameters}{fig:cl:hyp}

\section{Experiment: Generalisation of EAs} \label{chap:exp:fitness}

To test out how well solutions from evolutionary \ac{dml} methods evolve and generalise, the methods are trained for 1000 generations and the fitness values, train, and test success rates are measured after every generation. From these values it is possible to see whether the method generalises well or whether it overfits. It is also possible to compare the search capabilities of the methods by looking at the individuals with maximal fitness.

\ac{cmaes} and \ac{jde} were tested with both fitnesses (\ac{knn} and \mbox{F-measure}): \ac{cmaesknn}, \ac{cmaesfme}, \ac{jdeknn}, and \ac{jdefme}. All algorithms were tested with full and diagonal Mahalanobis matrices.

All methods have some hyperparameters and in order to avoid the time consuming hyperparameter search, the hyperparameters for the models in this experiment were selected according to results from the previous experiment in Section~\ref{chap:exp:classification}. Only one set of the hyperparameters was selected for all datasets. The number of nearest neighbours used in the \ac{knn} fitness function was chosen to be 8 and the number of nearest neighbours in the final \ac{knn} classifier was set to 4.

Datasets described in Section~\ref{chap:exp:datasets} were used as well for this experiment. Data were preprocessed and standardised the same way as in previous experiments, both methods are described in Section~\ref{chap:exp:preprocessing}. Data were split into training and testing sets using a 67:33 ratio in a stratified fashion, keeping the sizes of classes balanced between the two splits.

To measure the performance of the fitness function, the fitness values of all individuals were logged in every generation and then the median, minimum, maximum, 25th and 75th percentiles (Q1 and Q3) were calculated in each generation. To see how well the individuals generalise, individuals with the best and the worst fitness values were selected in every generation and the metric encoded in these individuals was evaluated using the test set using the \ac{knn} classifier after every single generation.

\subsection{Results}

Figures~\ref{fig:fitness:balance-scale}---\ref{fig:fitness:wine} show the results for each dataset. In each Figure, the top four methods learn a full matrix, meanwhile the bottom four methods learn a diagonal matrix. Both \ac{knn} and \mbox{F-measure} fitness functions output values in a closed range $[0,1]$. The final \ac{knn} classifier used to measure performance outputs values in the same exact range and all values are plotted in the same graph. Therefore each individual graph shows minimal and maximal (both in orange) fitnesses, median fitness (in green) with the area between 25th and 75th percentiles coloured in light green, the blue line represents the test success rate of the individual with the highest fitness and the pink line represents the test success rate of the individual with the lowest fitness in every generation. A black line representing a test success rate of \ac{knn} using standard Euclidean distance is also added to the graphs. Unfortunately \ac{jde} did not finish on the four largest datasets and those graphs are marked with ``Timeout''.

Important aspect to also keep in mind when looking at the graphs is that the left four graphs all use \ac{knn} for both fitness evaluation as well as test success rate evaluation, meanwhile the right four graphs use \mbox{F-measure} fitness for learning, but the test success rate is still evaluated using the \ac{knn} classifier. This means that the left four graphs have fitnesses and their test success rates directly comparable, meanwhile in the right four graphs only general trends can be followed and the fitness values cannot be directly compared with the test success rates.

\ac{jde} updates the population only when a new individual with a better fitness value is generated, whereas \ac{cmaes} regenerates the entire population from scratch every generation according to its internal covariance matrix. This is clearly visible on all the graphs: for \ac{jde}, the orange and blue lines representing fitnesses never drop, they only grow. On the other hand, \ac{cmaes}' fitness lines are more unpredictable. Therefore, in order to smooth the lines and make the graphs more readable, one-dimensional Gaussian filter with a standard deviation 2.0 is used.

\ac{cmaesknn} (full) methods performs very well on most datasets. From the graphs it is evident that it generalises also very well, which can be deduced from the fact that the blue and pink lines representing test success rate closely follow the green fitness value lines. \ac{cmaesknn} (full) can overfit and plummet after some number of generations which can be seen in Figure~\ref{fig:fitness:iris} on \dataset{iris} dataset, where the test success rate first goes up to 94\% and then starts dropping and finishes at 86\% success rate. This suggests that there should be an early stopping mechanism, which would prevent overfitting and reduce the computation time because the algorithm would not have to finish a fixed number of generations. The full version should outperform the diagonal version. However the diagonal version performs better on some datasets, namely on \dataset{breast-cancer} (Figure~\ref{fig:fitness:breast-cancer}), \dataset{digits10} (Figure~\ref{fig:fitness:digits10}) and \dataset{iris} (Figure~\ref{fig:fitness:iris}). This might be because the algorithm was better at exploring the $d$-dimensional space compared to $d^2$-dimensional space in the case of the full matrix. Nevertheless, the full matrix outperforms the diagonal matrix on \dataset{balance-scale} (Figure~\ref{fig:fitness:balance-scale}), \dataset{mice-protein} (Figure~\ref{fig:fitness:mice-protein}), \dataset{sonar} (Figure~\ref{fig:fitness:sonar}) and \dataset{wine} (Figure~\ref{fig:fitness:wine}) datasets. Moreover, in this experiment we are more interested in general trends of the evolution, not absolute values of the fitness functions.

If \ac{jdeknn} is compared with \ac{cmaesknn}, it generalises worse even though its fitness values are higher than fitness values of \ac{cmaesknn}. In every single dataset the blue and pink lines representing the test success rates are far below the green fitness line and on most datasets even far below the lower orange fitness line representing the minimal fitness in the given generation. This problem concerns evolving both full and diagonal Mahalanobis matrices. Even though it does not generalise well and the fitness values tend to overestimate the success rate, the testing success rates are relatively comparable, although slightly worse, to those using \ac{cmaesknn}.

The fitness values of \ac{cmaesfme} have a very large range on all datasets, for examples on \dataset{breast-cancer} dataset (Figure~\ref{fig:fitness:breast-cancer}), the fitness values range from nearly 0\% to around 97\%. This highly oscillating range suggests it is hard for the method to generate good individuals, however the test success rates perform relatively well. The issue with \ac{cmaesfme} is that the fitness does not grow over time and therefore the test success rates do not grow either. This might be caused by the large fitness variability and because \mbox{F-measure} does not say anything about the structure of the clusters and together with the fact that \ac{cmaes} regenerates the individuals in each generation, the generation does not improve the individuals but tends to be random. \ac{jdefme} overcomes this problem, because it does not regenerate the population but rather only replaces the individuals with better ones. For this reason the fitness values of \ac{jdefme} and \ac{jdeknn} are only rising. The problem with \ac{jdefme} method is that the test success rates do not follow the growth of the fitness values, but they tend to stagnate as can be seen on \dataset{pima-indians} (Figure~\ref{fig:fitness:pima-indians}) and \dataset{wine} datasets (Figure~\ref{fig:fitness:wine}). This might be a problem of \mbox{F-measure} not being suitable for representing the fitness of the clusters as \ac{cmaesfme} suffered from the same problem. This concerns evolving both the full and the diagonal matrices.

\cenfig{graphs/fitness/balance-scale}{Fitness evolution and generalisation on \dataset{balance-scale} dataset}{fig:fitness:balance-scale}
\cenfig{graphs/fitness/breast-cancer}{Fitness evolution and generalisation on \dataset{breast-cancer} dataset}{fig:fitness:breast-cancer}
\cenfig{graphs/fitness/digits6}{Fitness evolution and generalisation on \dataset{digits6} dataset}{fig:fitness:digits6}
\cenfig{graphs/fitness/digits10}{Fitness evolution and generalisation on \dataset{digits10} dataset}{fig:fitness:digits10}
\cenfig{graphs/fitness/gaussians}{Fitness evolution and generalisation on \dataset{gaussians} dataset}{fig:fitness:gaussians}
\cenfig{graphs/fitness/iris}{Fitness evolution and generalisation on \dataset{iris} dataset}{fig:fitness:iris}
\cenfig{graphs/fitness/mice-protein}{Fitness evolution and generalisation on \dataset{mice-protein} dataset}{fig:fitness:mice-protein}
\cenfig{graphs/fitness/pima-indians}{Fitness evolution and generalisation on \dataset{pima-indians} dataset}{fig:fitness:pima-indians}
\cenfig{graphs/fitness/sonar}{Fitness evolution and generalisation on \dataset{sonar} dataset}{fig:fitness:sonar}
\cenfig{graphs/fitness/wine}{Fitness evolution and generalisation on \dataset{wine} dataset}{fig:fitness:wine}

\section{Experiment: jDE with log population} \label{chap:exp:logjde}
In previous experiments, different population sizes for \ac{cmaes} and \ac{jde} were used: $4+3\log{d^2}$ and $10d^2$, respectively ($d$ is the dimensionality of the problem, a full matrix was evolved). These population sizes for \ac{cmaes} and \ac{jde} are suggested in~\citep{hansen2001completely} and~\citep{brest2006self}, respectively. The size of the population directly translates into the performance of the evolutionary algorithm, which has better exploration capabilities with a larger population and is also more robust.

In experiments in Sections~\ref{chap:exp:classification} and~\ref{chap:exp:fitness}, \ac{cmaes} outperforms \ac{jde}~\citep{fukui2013evolutionary} even with imbalanced population sizes. The main drawback of a larger population is increased computational time, especially when the evaluation of the fitness function is expensive.

This experiment is designed to test whether such a large population for \ac{jde} is necessary and the generalisation performance of \ac{jde} with both population sizes, $4+3\log{d^2}$ and $10d^2$, is tested. The setting of this experiment is the same as for the experiment in Section~\ref{chap:exp:fitness}.

\subsection{Results}

Figures~\ref{fig:fitnesslogjde:digits10}---\ref{fig:fitnesslogjde:wine} use the same type of graphs as in the previous experiment. Each figure shows minimal, mean and maximal fitnesses for each generation and also test success rates using best and worst individuals in each generation. We chose to show only datasets where the differences between the population sizes are the most prominent.

Figures~\ref{fig:fitnesslogjde:digits10} and~\ref{fig:fitnesslogjde:mice-protein} show that \ac{jde} with smaller population ``(log)\ac{jde}'' has lower maximal fitness and also lower test success rates, which suggests the algorithm is not able to explore the space as good as with a larger population size. In Figures~\ref{fig:fitnesslogjde:gaussians} and~\ref{fig:fitnesslogjde:wine}, the test success rates of (log)\ac{jde} with the \ac{knn} fitness oscillate and drop in multiple generations, which suggests that the algorithm overfits.

We conclude that the population size has a big influence on the performance of an evolutionary algorithm. \ac{jde} with logarithmic population size performs worse on some datasets than the same algorithm with a population size linear to the dimensionality of the individuals. However, \ac{cmaes} with logarithmic population beats \ac{jde} even with a larger population as could be seen in the previous experiments.

%\cenfig{graphs/fitnesslogjde/balance-scale}{jDE with linear and log population size on \dataset{balance-scale} dataset}{fig:fitnesslogjde:balance-scale}
%\cenfig{graphs/fitnesslogjde/breast-cancer}{jDE with linear and log population size on \dataset{breast-cancer} dataset}{fig:fitnesslogjde:breast-cancer}
%\cenfig{graphs/fitnesslogjde/digits6}{jDE with linear and log population size on \dataset{digits6} dataset}{fig:fitnesslogjde:digits6}
\cenfig{graphs/fitnesslogjde/digits10}{jDE with linear and log population size on \dataset{digits10} dataset}{fig:fitnesslogjde:digits10}
\cenfig{graphs/fitnesslogjde/gaussians}{jDE with linear and log population size on \dataset{gaussians} dataset}{fig:fitnesslogjde:gaussians}
%%\cenfig{graphs/fitnesslogjde/iris}{jDE with linear and log population size on \dataset{iris} dataset}{fig:fitnesslogjde:iris}
\cenfig{graphs/fitnesslogjde/mice-protein}{jDE with linear and log population size on \dataset{mice-protein} dataset}{fig:fitnesslogjde:mice-protein}
%\cenfig{graphs/fitnesslogjde/pima-indians}{jDE with linear and log population size on \dataset{pima-indians} dataset}{fig:fitnesslogjde:pima-indians}
%%\cenfig{graphs/fitnesslogjde/sonar}{jDE with linear and log population size on \dataset{sonar} dataset}{fig:fitnesslogjde:sonar}
\cenfig{graphs/fitnesslogjde/wine}{jDE with linear and log population size on \dataset{wine} dataset}{fig:fitnesslogjde:wine}

\section{Experiment: Learning time} \label{chap:exp:learning-times}

Learning time of each method is an important factor. Some methods are not scalable to large datasets as already discussed in Chapter~\ref{chap:rw}. Moreover, some of the methods can still end up in a local minima thus multiple runs may be necessary for optimal results.

In this experiment the learning times of all the methods from previous experiments were measured: Covariance, \ac{lmnn}, \ac{nca}, \ac{lfda} and both evolution strategies with both evolution fitnesses: \ac{cmaesknn}, \ac{cmaesfme}, \ac{jdeknn} and \ac{jdefme}. For the evolutionary methods, learning only a diagonal transformation matrix was also measured. All methods were run 10 times on every dataset with a set of the hyperparameters which corresponds to best error rate for given dataset from the first experiment in Section~\ref{chap:exp:classification}. Euclidean distance does not need any training and is therefore omitted from this experiment altogether. The experiment was performed on a single computer so that the results are comparable. Having this restriction, the maximal time for the method to finish was set to six hours; after this period the experiment was interrupted. The methods that did not finish are marked as ``Timeout''.

\subsection{Results}

The results are in Table~\ref{tab:learning-times}, where the learning times shown are averaged across all 10 runs. Calculating the Covariance metric is the fastest, closely followed by \ac{lfda}. Some of the datasets were so small that these two methods would take a less than a hundredth of a second. On the other hand, \ac{lmnn} and \ac{nca} methods and all the evolutionary methods take a lot of time to learn. Generally, \ac{jde} should be much faster compared to \ac{cmaes} strategy because calculating a covariance matrix inside \ac{cmaes} is expensive, however, \ac{jde} needs a much larger population and the fitness evaluation is very expensive in our case as discussed in Section~\ref{chap:rw:fukui}. Therefore, \ac{jde} is clearly much slower compared to \ac{cmaes} strategy. It is also clear that \mbox{F-measure} fitness is much slower compared to \ac{knn} fitness as discussed in Section~\ref{alg:knn} when \ac{cmaesknn} is compared with \ac{cmaesfme} and \ac{jdeknn} with \ac{jdefme}. This makes the method \ac{jdefme} described in Section~\ref{chap:rw:fukui} the slowest method in all of the benchmarked methods and is even several times slower than \ac{nca}. \ac{cmaesknn} performs comparable to \ac{lmnn} in terms of learning time. The restricted versions of the evolutionary algorithms to a diagonal transformation matrix learn much faster thanks to smaller individuals, which directly encode the Mahalanobis matrix and also the smaller populations as also discussed in Chapter~\ref{chap:our-method}. What is important to note, however, is that differences between learning a full and a diagonal matrices using \ac{cmaes} are much smaller than the differences between learning a full and a diagonal matrices using \ac{jde}, where for \ac{cmaes} the difference is about 2 times, but for \ac{jde} the difference is much more significant, even 10 times slower on some datasets.

\input{results/learning-times}

To further investigate how well metric learning methods scale, two additional experiments were also designed to investigate the learning times. In the first experiment the dimensionality of the data was fixed to 5 and the learning times were measured for the number of samples in the range from 100 to 1500 with 100 increments. In the second experiment the number of samples was fixed to 500 and the learning times were measured for all methods with the increasing dimension in the range from 2 to 10 with an increment of 1. All these measurements were done using \dataset{digits10} dataset from which the required number of samples was sampled using the stratified sampling and then the dimension of the samples was reduced using \ac{pca} to a required dimension.

Both experiments are represented in Figures~\ref{fig:learning-times:samples} and~\ref{fig:learning-times:dimensions}. Most of the methods merge together in the graph so two graphs are showed for each experiment: one with linear and one with logarithmic scale applied on y axis. Covariance and \ac{lfda} are not included in these two figures because they perform so fast on this experiment that the results are not interpretable due to random fluctuations in the measurements.

From Figure~\ref{fig:learning-times:samples} it is clear that \ac{jde} performs the worst and does not scale well with an increasing number of samples with both \ac{knn} and \mbox{F-measure} fitnesses. \ac{nca} performs also very bad with an increasing number of samples. On the other hand the rest of the methods perform relatively well and are indistinguishable on the linearly scaled axis. \ac{lmnn} and \ac{cmaes} have a similar performance, however \ac{lmnn} also performs much slower with more and more instances. In contrast, \ac{cmaes} scales very well and the number of samples hardly influences the performance. Looking at the evolutionary methods restricted to a diagonal matrix, it is clear that \ac{cmaesknn} (diag) is the fastest among all of them, however, it is not much slower compared to its unrestricted version \ac{cmaesknn} (full). Restricted version of \ac{jde} is much faster compared to its unrestricted counterpart, but even the restricted version is slower and scales worse than \ac{cmaesknn} (full).

\cenfig{graphs/learning-times/samples}{Learning times for increasing number of samples with a fixed dimension (linear and log scale)}{fig:learning-times:samples}

Similarly, \ac{jde} scales very poorly with an increasing dimensionality of the dataset as can be seen from Figure~\ref{fig:learning-times:dimensions}. The order of the methods in the graphs remained the same as in the previous experiment, however, there are some notable changes in the shapes of the curves. \ac{lfda} scales very well with an increasing dimensionality; \ac{nca} and \ac{lmnn} are not as steep as in the previous experiment, but they still scale worse compared to \ac{cmaes}. It is clear that restricted versions of all evolutionary algorithms scale much better in regards of the increasing dimensionality because the dimensionality directly reflects in the individuals size: for the restricted version the individuals have size $D$, but for the unrestricted version the whole matrix has $D^2$ parameters. The dimensionality of the individuals directly reflect in the population size as discussed. Therefore the restricted versions scale better with the increasing dimension and their graphs are less steep. When the steepness of \ac{cmaes} and \ac{jde} is directly compared, \ac{jde} scales worse even in the restricted version. That is due to the large population size that \ac{jde} uses by default, compared to \ac{cmaes}, which only has a population of a logarithmic size as discussed in Chapter~\ref{chap:our-method}.

\cenfig{graphs/learning-times/dimensions}{Learning times for increasing dimension with a fixed number of samples (linear and log scale)}{fig:learning-times:dimensions}


\section{Experiment: Dimensionality reduction} \label{chap:exp:dimred}

%TODO visualise learnt transformation? PCA vs CMAES vs LFDA vs NCA

In this experiment, \ac{pca} and \ac{tsne} methods are compared with \ac{dml} methods supporting dimensionality reduction: \ac{nca}, \ac{lfda} and \ac{cmaesknn}. Apart from evolving a transformation matrix, we also experimented with evolving fully-connected neural networks for dimensionality reduction using \ac{cmaesknn}. Six different neural networks were evolved differing in depth (one or five hidden layers) and their activation functions (sigmoid, \ac{tanh} or \ac{relu}). The shallow networks have one hidden layer with 4 neurons and the deep networks have 16-12-8-6-4 neurons in the hidden layers. To reduce the dimension into 2-dimensional space, all networks always end with an output layer with 2 neurons. Therefore, the shallow network has $4d+8$ free parameters and the deeper network has $16d+354$ free parameters. No biases were used.

\ac{pca}, \ac{lfda}, and \ac{nca} methods are linear and so the dimensionality reduction works by projecting the data onto the 2-dimensional space by a matrix multiplication. The methods only differ in how to find this transformation matrix.

The same datasets, listed in Table~\ref{tab:datasets}, are used in this experiments. The data are standardised as described in Section~\ref{chap:exp:preprocessing}. The 10-fold cross validation is used to split the dataset to train and test sets. After training the algorithms, they are used to transform both train and test sets into two dimensions. To compare the dimensionality reduction, the \ac{knn} classifier is fit using this 2-dimensional training set and calculate a success rate on the test set. The idea is that the \ac{knn} classifier has a large accuracy if instances from the same class are close together, which indicates a good dimensionality reduction. 

\ac{tsne} is also included in the visualisations, however, as it is a transductive method, it cannot be used to classify new data. Therefore in this case, the \ac{knn} classifier is fit on all the data and the numbers may be overly optimistic. The main reason for including \ac{tsne} is to show how the visualisation from \ac{cmaesknn} compare to this popular visualisation method.

We were also interested in whether \ac{cmaes} would help \ac{tsne} make a better visualisation if used as a preprocessing and we call this method \ac{cmaesknn}+\ac{tsne}, where \ac{cmaesknn} is trained first and data are transformed using the evolved transformation matrix keeping the original dimensionality, and only then \ac{tsne} is used to transform the preprocessed data from \ac{cmaes} into two dimensions.

\subsection{Results}

The results are presented in Table~\ref{tab:dim-error-rates}. The error rates and standard deviations are shown for each method on each dataset from the 10-fold cross validation. The best inductive method (i.e. of those that do not use t-SNE) for every dataset is in a bold font. The best method among methods that have a transformation function (all except \ac{tsne}) is emphasised in italics.

Generally, \ac{cmaesknn} works very well for dimensionality reduction and also very fast because only a $d\times2$ matrix is evolved. \ac{cmaesknn} has the lowest error only on \dataset{pima-indians}, however the difference between this method and the best method on each dataset is never statistically significant. Again, the statistical significance tables for this experiment are provided in Attachments (Tables~\ref{tab:statsign:dimred:balance-scale}---\ref{tab:statsign:dimred:wine}). \ac{tsne} works the best on large datasets, namely \dataset{digits6} and \dataset{digits10} and the preprocessing with \ac{cmaes} helps \ac{tsne} on the majority of the datasets. \ac{lfda} and \ac{nca} have both also very good results, but \ac{nca} runs out of memory on three datasets (marked with ``Memory'' in the results table).

\input{results/dim-red}

The success rate, however, is only one of the factors that imply a good visualisation and therefore we also check the projections visually by plotting both the training and testing sets using a scatter plot. The following criteria are used to evaluate the visualisation: the instances with the same label are together; the clusters are well separated; clusters form small blobs and they are not spread out; all instances with the same class are in one large cluster. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualised.

In each scatter plot the instances are coloured according to their label. The training instances are highly opaque without any border. The testing instances are less opaque and they have a black or red border. Black border signifies that the instance was classified correctly using the \ac{knn} classifier and red border means the instance was classified incorrectly. The instances that were classified incorrectly are also visualised as slightly larger. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualised. The success rates of \ac{tsne} are prefixed with a tilde to signify it is an optimistic value.

The results of the dimensionality reduction for all methods are in Figures~\ref{fig:dimred:balance-scale}---\ref{fig:dimred:wine}. One figure represents one dataset and each figure contains a scatter plot for each method. \ac{pca} is also plotted in all figures and acts as the baseline method that other methods are compared to. 

On \dataset{balance-scale} (Figure~\ref{fig:dimred:balance-scale}) dataset \ac{pca} finds a good projection of the data and it is possible to see some intrinsic relationships between the classes. Nevertheless, \ac{nca}, \ac{lfda} and \ac{cmaesknn} all find much better projection in which the blue class forms a barrier between the orange and the green classes. \ac{tsne} by itself completely hides these relationships between the data: the green class is split into two clusters and the blue class is scattered around. Preprocessing the data using \ac{cmaesknn} first before using \ac{tsne} fixes both these problems, the green class stays clustered together and the blue class forms a cluster. The evolved neural networks are also able to separate the classes from each other, but the deep neural network with \ac{tanh} activation makes two well separated clusters, but spreads out the third class.

On \dataset{breast-cancer} (Figure~\ref{fig:dimred:breast-cancer}) dataset \ac{nca} and \ac{cmaesknn} separate the clusters more strictly. \ac{tsne} works well by itself, but preprocessing the data with \ac{cmaes} first improves the visualisation again. The neural networks also separate the classes very well. The networks with sigmoid and \ac{tanh} activations tend to keep the instances in long chains, but \ac{relu} activation function tends to cluster one class together and spreads out the other one.

When looking at \dataset{digits6} and \dataset{digits10} datasets (Figures~\ref{fig:dimred:digits6} and~\ref{fig:dimred:digits10}), \ac{tsne} visualisations outperforms other methods. Nevertheless, \ac{cmaesknn}+\ac{tsne} still improves the visualisation by joining some clusters together on both datasets. \ac{pca} finds some clusters, but they are merged together and not well separated. \ac{cmaesknn} works better compared to \ac{pca}, it manages to separate the clusters more and the clusters themselves are also more uniform. \ac{lfda} method fails on this dataset completely, it spreads out just few data instances, but keeps the rest together in one dense cluster. \ac{nca} method runs out of memory on these datasets.

All methods perform well on \dataset{gaussians} dataset (Figure~\ref{fig:dimred:gaussians}). The supervised methods, however, find more compact clustering than \ac{pca}. \ac{tsne} splits the blue class into two clusters and this is where \ac{cmaesknn}+\ac{tsne} improves on again. Very similar observation can be made about \dataset{iris} dataset (Figure~\ref{fig:dimred:iris}). The neural networks again visualise and cluster the data almost as chains.

Both \dataset{mice-protein} (Figure~\ref{fig:dimred:mice-protein}) and \dataset{pima-indians} datasets (Figure~\ref{fig:dimred:pima-indians}) are hard to visualise. Regarding \dataset{mice-protein} dataset, \ac{pca} does not find any nice intrinsic properties and just makes one large heterogeneous cluster. \ac{nca} ran out of memory and \ac{lfda} only separated out the grey class. \ac{cmaesknn} was able to separate out the most classes, but some classes are still mixed together and they are not well-separated. The best visualisation, at least in regards to success rate, was achieved using \ac{tsne}, but it was not able to capture the classes into homogeneous clusters, but it created many small fragmented clusters. On \dataset{pima-indians} there is not a clear winner, but \ac{nca}, \ac{lfda}, \ac{cmaesknn} and \ac{cmaesknn}+\ac{tsne} were the best at separating the two classes from each other.

Datasets \dataset{sonar} (Figure~\ref{fig:dimred:sonar}) and \dataset{wine} (Figure~\ref{fig:dimred:wine}) have very few data instances and that might be the reason why \ac{tsne} and even \ac{cmaesknn}+\ac{tsne} failed completely. \dataset{sonar} is a high-dimensional dataset and \ac{pca} failed to separate the clusters as well on this dataset. On the other hand \ac{nca}, \ac{lfda} and \ac{cmaesknn} were able to separate the classes into a homogeneous clusters on both datasets. The neural network methods have high success rate which signifies that the dataset probably has some non-linear intrinsic structure.

Interestingly, the neural networks with sigmoid and \ac{tanh} activations tend to create interesting, almost cube-like structures, meanwhile \ac{relu} activation creates more of a light torch structure, where many instances are clustered in a centre and the rest is spread out in one major direction. We also found out that sometimes the simpler models (shallow networks, on only a linear mapping) often provide better results than the deeper neural networks. One of the reasons may be the deeper network has more parameters which means it would take longer to tune them. Increasing the number of generations may improve the performance of the deeper models, however, the training would also take considerably longer.

\cenfig{graphs/dimred/balance-scale}{Visualisation of \dataset{balance-scale} dataset with \ac{dml} methods}{fig:dimred:balance-scale}
\cenfig{graphs/dimred/breast-cancer}{Visualisation of \dataset{breast-cancer} dataset with \ac{dml} methods}{fig:dimred:breast-cancer}
\cenfig{graphs/dimred/digits6}{Visualisation of \dataset{digits6} dataset with \ac{dml} methods}{fig:dimred:digits6}
\cenfig{graphs/dimred/digits10}{Visualisation of \dataset{digits10} dataset with \ac{dml} methods}{fig:dimred:digits10}
\cenfig{graphs/dimred/gaussians}{Visualisation of \dataset{gaussians} dataset with \ac{dml} methods}{fig:dimred:gaussians}
\cenfig{graphs/dimred/iris}{Visualisation of \dataset{iris} dataset with \ac{dml} methods}{fig:dimred:iris}
\cenfig{graphs/dimred/mice-protein}{Visualisation of \dataset{mice-protein} dataset with \ac{dml} methods}{fig:dimred:mice-protein}
\cenfig{graphs/dimred/pima-indians}{Visualisation of \dataset{pima-indians} dataset with \ac{dml} methods}{fig:dimred:pima-indians}
\cenfig{graphs/dimred/sonar}{Visualisation of \dataset{sonar} dataset with \ac{dml} methods}{fig:dimred:sonar}
\cenfig{graphs/dimred/wine}{Visualisation of \dataset{wine} dataset with \ac{dml} methods}{fig:dimred:wine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\chapter{Implementation} \label{chap:impl}
%
%\section{Hierarchical Grid search}
%
%how it works - scheme
%
%how cross-validation is in memory
%
%\section{Metric learning in metric\_learn package}
%
%Description of modules, fitnesses, transformers, ...
%
%\begin{figure}[h!] \label{fig:implementation-modules}
%	\centering
%    \includegraphics[width=0.2\textwidth]{img/notfound}
%    \caption{Interaction between modules for Metric evolution}
%\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\chapter{Discussion}

%Graphs, performace, caveats, what worked, what didnt work...

%LMNN, ITML, SDML take too long to calculate

%LMNN and LFDA perform very well

%LMNN has too many hyper parameters

%Simple Covariance metric was impossible to calculate on certain datasets

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion} \label{chap:conclusion}
% \addcontentsline{toc}{chapter}{Conclusion}

We presented an evolutionary algorithm for metric learning and dimensionality reduction with a focus on visualisation. The algorithm extends previous ideas of~\citep{fukui2013evolutionary} and improves on its results. The main idea of the algorithm is to use a \ac{knn}-based fitness function and \ac{cmaes} in order to find distance metrics and mappings to lower-dimensional spaces. We have further extended the idea by replacing the usual linear transformation with a more complex non-linear mapping represented as neural networks.

Our algorithm was compared to standard Euclidean distance and state-of-the-art \ac{dml} algorithms: \ac{lmnn}, \ac{nca}, \ac{lfda} and \ac{jdefme}. In the metric learning experiment focused on improving classification, our algorithm was able to find metrics better than other algorithms in four out of the ten cases, while in the remaining cases the results were comparable to those obtained by the best algorithms. The \ac{knn} classifier used as a fitness function performs better than using \mbox{F-measure} index.

In the dimensionality reduction experiment, our algorithm obtained the best results on five out of ten datasets. On the rest of the datasets, the other methods (especially \ac{nca}) obtained better results. However, these methods sometimes failed completely due to their complexity or numerical instability, while our proposed method does not suffer from these limitations.

Lastly, an experiment was performed to measure the generalisation performance of the evolutionary algorithms and their fitnesses. We found that \ac{jde} needs a much larger population to perform well, compared to \ac{cmaes}. We also found that \ac{cmaes} generalises well, but can overfit in some cases. 

We also measured the scalability and time-performance of all algorithms and found that the proposed algorithm \ac{cmaesknn} scales very well with number of samples and also scales well with an increasing dimensionality of the dataset. \ac{cmaesknn} is $10-20\times$ faster than the competing evolutionary \ac{jdefme} algorithm, described by~\citep{fukui2013evolutionary}, due to the population size. When compared to non-evolutionary methods, \ac{cmaesknn} also performs several times faster than \ac{nca}, nevertheless, \ac{lmnn} and \ac{lfda} are faster.


\chapter{Future work} \label{chap:future-work}

% Inspired by http://researchers.lille.inria.fr/abellet/talks/metric_learning_tutorial_ECML_PKDD.pdf

In spite of our best effort, not everything was finished and there is space for further extensions and research. We recommend focusing on these areas:

\begin{itemize}
\item Evolutionary algorithms are stochastic and multiple runs might improve the results in both classification and dimensionality reduction experiments.

\item Our evolutionary method is sometimes overfitting the metric and not generalising well. This problem might be mitigated using an early stopping technique.

\item Another approach to overfitting might be adding a regularising technique to the fitness function, such as Frobenius Norm~\citep{schultz2003learning} or LogDet divergence~\citep{davis2007information}.

\item Our work focused mainly on learning linear metrics, but further extensions to non-linear metrics are possible using the kernel trick~\citep{chatpatanasiri2010new} or multiple local metrics as discussed in~\citep{bellet2013survey}. We also experimented with fully connected neural networks, however, more experiments can be conducted using other neural network architectures.

\item Instead of choosing a single individual with the best fitness across all generations for the final transformation, multiple individuals can be combined together to form the final transformation.

\item Additional objectives can be added to the fitness function together with the \ac{knn} classifier to improve the generalisation. Some clustering indexes (e.g. purity, homogeneity) could perform well with \ac{knn} in describing the final embedding. \ac{cmaes} can be naturally extended to handle multi-objective optimisation~\citep{igel2007covariance}.

\item In our work we evolved a transformation matrix that could be used to form Mahalanobis distance. We focused on evolving a full and a diagonal matrix, however, evolving a triangular matrix could be a good compromise between model performance and computation feasibility.

\item It might be possible to evaluate the fitness on subsamples of the dataset to speed up the evolution, similarly to the way how batches are used in stochastic gradient descend to improve the speed of the algorithm.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Bibliography
\include{bibliography}

%%% Figures used in the thesis (consider if this is needed)
\listoffigures

%%% Algorithms used in the thesis 
\listofalgorithmes

%%% Tables used in the thesis (consider if this is needed)
%%% In mathematical theses, it could be better to move the list of tables to the beginning of the thesis.
\listoftables

%%% Abbreviations used in the thesis, if any, including their explanation
%%% In mathematical theses, it could be better to move the list of abbreviations to the beginning of the thesis.
\chapwithtoc{List of Abbreviations}

\printacronyms[include-classes=abbrev,heading=none] % ,name=Abbreviations

\chapwithtoc{List of Notations}
\input{notation}

%%% Attachments to the master thesis, if any. Each attachment must be
%%% referred to at least once from the text of the thesis. Attachments
%%% are numbered.
%%%
%%% The printed version should preferably contain attachments, which can be
%%% read (additional tables and charts, supplementary text, examples of
%%% program output, etc.). The electronic version is more suited for attachments
%%% which will likely be used in an electronic form rather than read (program
%%% source code, data files, interactive charts, etc.). Electronic attachments
%%% should be uploaded to SIS and optionally also included in the thesis on a~CD/DVD.
\chapwithtoc{Attachments} \label{chap:attachments}

The tables showing statistical significance between individual methods are attached below, one table per dataset. Tables~\ref{tab:statsign:classification:balance-scale}---\ref{tab:statsign:classification:wine} correspond to experiment from Section~\ref{chap:exp:classification} and Tables~\ref{tab:statsign:dimred:balance-scale}---\ref{tab:statsign:dimred:wine} correspond to experiment from Section~\ref{chap:exp:dimred}. All experiments were performed using a 10-fold cross validation and then the t-test was used to calculate the $p$-value. The p-values are not shown in the tables directly, but they are encoded in asterisks: the smaller the $p$-value, the larger the significance between the models. More asterisks signify larger significance.

\input{results/stat-significance_classification}
\input{results/stat-significance_dimred}

\openright
\end{document}
