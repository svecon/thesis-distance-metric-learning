%%% The main file. It contains definitions of basic parameters and includes all other parts.

%% Settings for single-side (simplex) printing
% Margins: left 40mm, right 25mm, top and bottom 25mm
% (but beware, LaTeX adds 1in implicitly)
\documentclass[12pt,a4paper]{report}
\setlength\textwidth{145mm}
\setlength\textheight{247mm}
\setlength\oddsidemargin{15mm}
\setlength\evensidemargin{15mm}
\setlength\topmargin{0mm}
\setlength\headsep{0mm}
\setlength\headheight{0mm}
% \openright makes the following text appear on a right-hand page
\let\openright=\clearpage
\renewcommand{\baselinestretch}{1.25}

%% Settings for two-sided (duplex) printing
% \documentclass[12pt,a4paper,twoside,openright]{report}
% \setlength\textwidth{145mm}
% \setlength\textheight{247mm}
% \setlength\oddsidemargin{14.2mm}
% \setlength\evensidemargin{0mm}
% \setlength\topmargin{0mm}
% \setlength\headsep{0mm}
% \setlength\headheight{0mm}
% \let\openright=\cleardoublepage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Character encoding: usually latin2, cp1250 or utf8:
\usepackage[utf8]{inputenc}

%% Further useful packages (included in most LaTeX distributions)
\usepackage{amsmath}        % extensions for typesetting of math
\usepackage{amsfonts}       % math fonts
\usepackage{amsthm}         % theorems, definitions, etc.
\usepackage{bbding}         % various symbols (squares, asterisks, scissors, ...)
\usepackage{bm}             % boldface symbols (\bm)
\usepackage{graphicx}       % embedding of pictures
\usepackage{fancyvrb}       % improved verbatim environment
\usepackage{natbib}         % citation style AUTHOR (YEAR), or AUTHOR [NUMBER]
\usepackage[nottoc]{tocbibind} % makes sure that bibliography and the lists
			    % of figures/tables are included in the table
			    % of contents
\usepackage{dcolumn}        % improved alignment of table columns
\usepackage{booktabs}       % improved horizontal lines in tables
\usepackage{paralist}       % improved enumerate and itemize
\usepackage[usenames]{xcolor}  % typesetting in color

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% User-defined packages
\usepackage{amssymb}
\usepackage[inline]{enumitem}
\usepackage{adjustbox}
\usepackage{dcolumn}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{morefloats}
% \usepackage{capt-of}
% \sloppy

% http://ftp.cvut.cz/tex-archive/macros/latex/contrib/algorithm2e/doc/algorithm2e.pdf
% https://en.wikibooks.org/wiki/LaTeX/Algorithms
\usepackage[]{algorithm2e}

% http://ftp.cvut.cz/tex-archive/macros/latex/contrib/acro/acro_en.pdf
% \acsetup{first-style=short}
\usepackage{acro}
\include{acronyms}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Basic information on the thesis
% Thesis title in English (exactly as in the formal assignment)
\def\ThesisTitle{Evolutionary Algorithms for Data Transformation}

% Author of the thesis
\def\ThesisAuthor{Bc. Ondřej Švec}

% Year when the thesis is submitted
\def\YearSubmitted{2017}

% Name of the department or institute, where the work was officially assigned
% (according to the Organizational Structure of MFF UK in English,
% or a full name of a department outside MFF)
\def\Department{Department of Theoretical Computer Science and Mathematical Logic}

% Is it a department (katedra), or an institute (ústav)?
\def\DeptType{Department}

% Thesis supervisor: name, surname and titles
\def\Supervisor{Mgr. Martin Pilát, Ph.D.}
\def\SupervisorAvast{Ing. Martin Vejmelka, Ph.D.}

% Supervisor's department (again according to Organizational structure of MFF)
\def\SupervisorsDepartment{Department of Theoretical Computer Science and Mathematical Logic}

% Study programme and specialization
\def\StudyProgramme{Computer Science}
\def\StudyBranch{Artificial Intelligence}

% An optional dedication: you can thank whomever you wish (your supervisor,
% consultant, a person who lent the software, etc.)
\def\Dedication{%
I would like to thank my team at Avast Software, especially \SupervisorAvast, for inspiring me to investigate this part of machine learning and his guidance from the very beginning of my research. Additionally, I would like to acknowledge the academic and technical support of the Avast Software, namely for the provided hardware used for evaluating various deep learning methods. 

I would like to thank my supervisor, \Supervisor, for his guidance, the considerable time spent on consultations, proof reading, and for inspiring me to investigate the combination of metric learning and evolutionary algorithms.

I also need to thank my family for their continued support and encouragement during my Master studies and especially during the time spent working on this thesis.
}

% Abstract (recommended length around 80-200 words; this is not a copy of your thesis assignment!)
\def\Abstract{%
Abstract. %TODO
}

% 3 to 5 keywords (recommended), each enclosed in curly braces
\def\Keywords{%
{Metric Learning} {Mahalanobis Distance} {Dimensionality Reduction} {Evolution Strategy} {Visualisation} %TODO
}

%% The hyperref package for clickable links in PDF and also for storing
%% metadata to PDF (including the table of contents).
\usepackage[pdftex,unicode]{hyperref}   % Must follow all other packages
% \usepackage[hidelinks]{hyperref}
\hypersetup{breaklinks=true}
\hypersetup{pdftitle={\ThesisTitle}}
\hypersetup{pdfauthor={\ThesisAuthor}}
\hypersetup{pdfkeywords=\Keywords}
\hypersetup{urlcolor=blue}

% Definitions of macros (see description inside)
\include{macros}

% Title page and various mandatory informational pages
\begin{document}
\include{title}

%%% A page with automatically generated table of contents of the master thesis

\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Guidelines

% Transformace dat jsou důležitou součástí strojového učení, která výrazně ovlivňuje kvalitu vytvořených modelů. Transformace dat se často používají i pro jejich zobrazení do prostoru s menší dimenzí, kde se dají snáze vizualizovat. Většina metod ale funguje bez učitele a tedy není schopna najít taková zobrazení dat, která by brala v úvahu vlastnosti metod strojového učení, které následují po ní, případně se snažila vizualizaci upravit tak, aby data ze stejné třídy byla blízko u sebe. Některé metody určené přímo pro vizualizaci (jako např. t-SNE) navíc ani neposkytují transformační funkci a nejsou tedy schopny zobrazit nová data bez přepočítání celého zobrazení. Cílem práce je tedy pomocí kombinace evolučních algoritmů a dalších přístupů (např. neuronových sítí) navrhnout metody pro vytvoření transformační funkce, která bude brát v úvahu i označkování dat. 

% Student se seznámí s postupy pro automatické transformace dat. Na základě zjištěných informací implementuje vlastní metody a porovná je s existujícími přístupy. Součástí srovnání bude i vhodnost použité metody pro zobrazení dat do prostoru s malou dimenzí vhodnou pro vizualizaci.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{preface}
%\include{chap01_competition}
%\include{chap01}
%\include{chap02}
%\include{chap03_mahalanobis}
%\include{epilog}

\chapter{Introduction} \label{chap:intro}
\addcontentsline{toc}{chapter}{Introduction}

A metric is ubiquitous in machine learning. For instance, in classification, \ac{knn} classifier \citep{cover1967nearest} uses a metric to find the nearest neighbours. In clustering, many clustering algorithms, including wildly used k-Means \citep{hartigan1979algorithm}, calculate distance between data points and groups similar instances together. In information retrieval, when searching for the most relevant document, the documents themselves are ranked according to their similarity scores. Also security systems use a metric for face verification.

It is obvious that performance of all these algorithms depends on the quality of the metric defined on the input space. The better the metric judges how close or how similar any two instances in the space are, the more the final machine learning algorithm will behave.

Several general purpose metrics exist, such as Euclidean distance, defined in Equation \ref{eq:euclid}, or its generalized Minkowski distance, defined in Equation \ref{eq:minkowski}, and cosine similarity, defined in Equation \ref{eq:cosine} used for vectors of real numbers, Levenshtein distance (also called edit distance) for measuring distances between strings or $\chi^2$ distance used for calculating distances between histograms in Equation \ref{eq:chisq}.

% https://numerics.mathdotnet.com/distance.html
\begin{align}
d_{\mathbf{2}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_2 = \sqrt{\sum_{i=1}^{n} (x_i-y_i)^2} \label{eq:euclid} \\
d_{\mathbf{p}}(\textbf{x}, \textbf{y}) &= \|\textbf{x}-\textbf{y}\|_p = \bigg(\sum_{i=1}^{n} |x_i-y_i|^p\bigg)^\frac{1}{p} \label{eq:minkowski} \\
\mathrm{sim}(\textbf{x}, \textbf{y}) &= \cos{\phi} = \frac{\textbf{x}\textbf{y}}{\|\textbf{x}\|\|\textbf{y}\|} \label{eq:cosine} \\
\chi^2(\textbf{x}, \textbf{y}) &= \sum_{i} \cfrac{(x_i - y_i)^2} {(x_i + y_i)} \label{eq:chisq}
\end{align}

These metric, however, do not take any advantage of a prior knowledge of the task being solved or the structure of the data. Improved results are expected when the metric is designed for a particular task. Nevertheless, manually hand-crafting a metric is a strenuous and tedious process, which requires an expert knowledge of the task and the data. This has led to an introduction of \ac{dml} where the distance metrics or similarities are learnt from the data themselves.

Therefore the goal of metric learning is to adapt some pairwise metric function to the task at hand by using the training examples. This process requires almost no prior knowledge about the data and is very fast compared to hand-crafting a metric function, which may take months of experimenting. That is why metric learning algorithms that learn a distance metric or similarity directly from the data have been getting more and more traction in the recent years.

Another important area of machine learning is dimensionality reduction whose goal is to embed high-dimensional space in a low-dimensional space so that most of the information contained in the data is preserved (such as the variance or local distance measurements) in the low-dimensional space. Some metric learning algorithms do not scale well when the data are high-dimensional and so the dimensionality reduction can be used to transform data into a low-dimensional space just to save computational time. Another dimensionality reduction application is visualisation where the data are reduced into a 2-dimensional or 3-dimensional space in which they can be plotted and inspected more easily. The reason why we include dimensionality reduction in this work is because some of the metric learning algorithms search for a projection into a low-dimensional space and thus can also be used for dimensionality reduction.

% http://scikit-learn.org/stable/modules/decomposition.html#pca
% http://scikit-learn.org/stable/modules/lda_qda.html#id4
% http://scikit-learn.org/stable/modules/manifold.html
One of the most common unsupervised dimensionality reduction method is \ac{pca} \citep{jolliffe2002principal} which decomposes the dataset in a set of successive orthogonal components that explain a maximum amount of the variance inside of the data. Other well-known methods are for example \ac{ica} and \ac{lda}. A specific area of non-linear dimensionality reduction is called manifold learning which is based on the idea that the dimensionality of many datasets is only artificially high but the intrinsic dimensionality is low and the data lie on an embedded low-dimensional manifold within the higher-dimensional space. A popular manifold algorithm receiving a lot of traction recently is \ac{tsne} \citep{maaten2008visualizing} that tries to preserve a local structure of the data which is especially useful in visualisation. Other known manifold methods are \ac{lle}, \ac{mds} and many others. The surveys reviewing the dimensionality reduction were done by \cite{fodor2002survey} and \cite{van2009dimensionality}.

In this thesis, we review existing state-of-the-art methods for learning global Mahalanobis distance metric and we propose a new method using evolution algorithms. We compare all the methods on several popular datasets to see how they improve classification. Learning the full Mahalanobis metric means learning $d^2$ parameters and so we also try to restrict the existing methods to learn only a diagonal of the Mahalanobis distance metric with $d$ parameters. Additionally we explore the usage of metric learning algorithms for visualisation of the data (dimensionality reduction to 2 dimensions) and compare them with traditional approaches.

\section{Metric learning} \label{chap:intro:ml}

The origins of metric learning can be traced back to \cite{short1981optimal}, however the first paper that received a lot of attention was \cite{xing2002distance} where the metric learning is formulated as a convex optimization problem. Since then metric learning has been getting increasing amount of attention and appeared at conferences ICML 2010\footnote{\url{http://www.icml2010.org/program.html}}, ECCV 2010\footnote{\url{http://www.ics.forth.gr/eccv2010/tutorials.php}} and workshops at ICCV 2011\footnote{\url{http://www.iccv2011.org/authors/workshops}}, NIPS 2011\footnote{\url{https://nips.cc/Conferences/2011/Schedule}} and ICML 2013\footnote{\url{http://icml.cc/2013/?page_id=41}}.

The goal of the metric learning is to find a pairwise real valued metric function $d(x,y)$ for the given problem such that it leverages the most information it can using the training samples. Every metric learning algorithm has some defining properties. These properties are well described and categorized in the survey done by Bellet \citep{bellet2013survey}. According to the survey the most important properties are: learning paradigm, form of the metric, scalability, optimality of the solution and if it supports dimensionality reduction. All these properties and its categories are summarized in figure \ref{fig:metric-learning} which is also inspired by the same survey.

\cenfig{graphs/metric_learning}{Metric learning properties. We focus on the highlighted categories.}{fig:metric-learning}

\begin{description}
\item [Learning paradigm] can be \textit{fully supervised}, where every sample of the dataset $z_i=(x_i,y_i) \in \mathcal{X} \times \mathcal{Y}$ is composed of the instance $x_i \in \mathcal{X}$ and its label (class) $x_y \in \mathcal{Y}$, which is a discrete and finite set of $\abs{\mathcal{Y}}$ labels. If the algorithm is \textit{weakly supervised} it does not have access to the labels of the instances, it only gets access to some side information, typically in the form of constraints $\mathcal{S}, \mathcal{D}$ defined as pairs of instances which are either similar \ref{eq:similar} or dissimilar \ref{eq:dissimilar}. Metric learning algorithm then tries to exploit these constraints to find the best parameters of a metric that best agrees with these constraints. Whereas getting labelled instances can be very hard or costly to obtain, obtaining similar and dissimilar pairs is easier. In \textit{semi-supervised} paradigm, we have some information (fully supervised or weakly supervised) about part of the data, but some (usually large) portion of the data comes with no information whatsoever.

\begin{align}
\mathcal{S} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be similar} \rbrace \label{eq:similar} \\
\mathcal{D} &= \lbrace(x_i,x_j): x_i \text{ and } x_j \text{ should be dissimilar} \rbrace \label{eq:dissimilar}
\end{align}

\item [Form of metric] is one of the defining features of a metric learning algorithm. Form of the metric categorizes the metric function into three main categories: \textit{Linear metrics} where the distance measures a length of a straight line between two points in Euclidean space. Typical example is Euclidean distance or Mahalanobis distance. \textit{Nonlinear metrics} can capture nonlinear dependencies inside data, typical example is $\chi^2$ histogram distance defined earlier in Equation \ref{eq:chisq}. Metric learning algorithm can also learn multiple \textit{local metrics} where different metric is used for different parts of the input space. The local metrics can be linear or nonlinear and all the metrics are usually trained in parallel.

\item [Scalability] is an important factor to be considered during any machine learning. Having a great method that is intractable to calculate for large amounts of data is useless. Therefore the algorithms need to scale well with respect to \textit{number of samples}. Also, metric learning algorithms should scale well with respect to \textit{number of dimensions} of the data. However, since many metric learning algorithms are trying to learn a $d \times d$ matrix, designing an algorithm that scales well with an increasing dimensionality is a challenge.

\item [Optimality of the Solution] refers to the ability of the algorithm to find a solution. Ideally the algorithm is capable of finding a \textit{global optimum} which means that the solution is always the best one of all feasible solutions available. This is usually the case for algorithms defined as convex optimizations. On the other hand the algorithm might only be able to find \textit{local optimum}, which means that no better solution exists in an immediate neighbourhood of the current solution. This is typical for nonconvex formulations of the algorithms.

\item [Dimensionality reduction] is an interesting "by-product" of metric learning algorithms that search for a projection of the data into a low-dimensional space, typically achieved by applying a strong regularization on the algorithm so that the final solution is low-rank. An advantage of combination of supervised dimensionality reduction via metric learning is that it tries to maximize the separation of labelled data. Dimensionality reduction has a numerous applications: from visualisation to more compact representations of the data.
\end{description}

In this work we mainly focus on the metric learning algorithms that belong to the categories highlighted with orange background in figure \ref{fig:metric-learning}: fully supervised with a linear (Mahalanobis) metric that scale well and are capable of dimensionality reduction. 

\section{Applications} \label{chap:intro:applications}

\acl{dml} can potentially be beneficial whenever a metric
between instances plays an important role. Recently, it has been successfully applied to many different problems, such as link prediction in networks \citep{shaw2011learning}, state representation in reinforcement learning \citep{taylor2011metric}, music recommendation \citep{mcfee2012learning}, identity verification \citep{ben2012improved}, webpage archiving \citep{law2012structural}, cartoon synthesis \citep{yu2012semisupervised} and assessing the efficacy of acupuncture \citep{liang2012learning}.

There are three large fields that benefit from \acl{dml}. In \textit{Computer vision} \ac{dml} is not only used to find distances between images and videos, but it has also been successfully applied in problems such as image classification \citep{mensink2012metric}, object recognition \citep{frome2007learning}; \citep{verma2012learning}, face recognition \citep{guillaumin2009you}; \citep{lu2014neighborhood}, visual tracking \citep{li2012non}; \citep{jiang2012order} or image annotation \citep{guillaumin2009tagprop}. In \textit{Information retrieval}, the objective is to try to provide with the most relevant documents to a specific query, or rank documents and other media. This is achieved by training a metric between the documents or a document and a query. Recent applications of \acl{dml} in this field were done by \cite{lebanon2006metric}, \cite{lee2008rank}, \cite{mcfee2010metric} and \cite{lim2013robust}. \textit{Bioinformatics} is another large field where \acl{dml} is used extensively to compare protein or DNA sequences. Typically edit distance is used to measure distance between these sequences, but adapting a metric to a specific problem can greatly improve the results as is demonstrated in the works of \cite{xiong2006kernel}, \cite{saigo2006optimizing}, \cite{kato2010metric} and \cite{wang2012prodis}.

Other research topic that is related to metric learning but outside the scope of this work is \textit{Kernel Learning}. Compared to metric learning where we try to find parameters of a given form of metric (for example a Mahalanobis distance), kernel learning is typically nonparametric, which means that the kernel matrix without any assumption on the form of the kernel that generated it. The main problem with this approach is that it is transductive, which means that the distances between the pairs of the instances are learnt as values, not functions. Therefore the kernel learning can hardly be applied to new data.  Induction on the other hand is reasoning from observed instances to general rules (which are later applied to the test instances). A recent survey on kernel learning was done by \cite{abbasnejad2012survey}.

An extension to kernel learning is \textit{\ac{mkl}} which compared to traditional kernel learning is parametric and the goal is to learn a predefined set of kernels. Compared to kernel learning, \ac{mkl} has efficient formulations and can be applied in the inductive setting. \Ac{mkl} is very popular because it can select an optimal kernel and from a larger set of kernels and it can combine data that have different notion of similarity and require different kernels. \ac{mkl} has been used in many applications such as recognition in video \citep{chen2013event}, object recognition in images \citep{bucak2014multiple} or biomedical data fusion \citep{yu20102}. A recent survey on \ac{mkl} was done by \cite{gonen2011multiple}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Mahalanobis metric} \label{chap:intro:mah}
% http://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html
% https://en.wikipedia.org/wiki/Mahalanobis_distance

The problem with the Euclidean distance is that it does not account for variances in different directions and correlations between pairs of dimensions in the data. For normally distributed data, the distance from the mean can be specified by computing the z-score, defined as $z = \frac{x-\mu}{\sigma}$, where $\mu$ is the population mean and $\sigma$ is the population standard deviation. The z-score of $x$ can be looked at as a dimensionless quantity and can be interpreted as the number of standard deviations of $x$ away from the mean.

The figure \ref{fig:corrdata} demonstrates this problem in two dimensions: the figure visualises a dataset with both dimensions highly correlated and with the first dimension highly variable. In the plot the coloured ellipses show the probability density of the Gaussian mixture model. The probability density is high for ellipses near the origin and low for ellipses further away. In the figure there is $[0,0]$ (origin) and two other red points: $[0,1]$ and $[2,0]$. In Euclidean distance the point $[0,1]$ is twice closer to the origin compared to the point $[2,0]$. However, because the first axis has larger variance, it is clear that seeing point $[2,0]$ is much more probable in our dataset than it is seeing point $[0,1]$ because the point $[2,0]$ lies on the edge of the first ellipsis, meanwhile point $[0,1]$ lies beyond the edge of the second ellipsis. Using the terminology of the z-score, the point $[0,1]$ is more standard deviations away from the origin than the point $[2,0]$.

\cenfig{graphs/correlated_data}{Correlated data centered around origin}{fig:corrdata}

% http://blogs.sas.com/content/iml/2012/02/08/use-the-cholesky-transformation-to-correlate-and-uncorrelate-variables.html
The data can be uncorrelated using the covariance matrix $\bm{\Sigma}$, which is a \ac{psd} matrix and therefore the Cholesky decomposition $\bm{\Sigma}=\bm{L}\bm{L}^T$ can be used to obtain a lower triangular matrix $\bm{L}$. The inverse of this triangular matrix $\bm{L}$ can be used to remove the correlation from the data defined by $\bm{\Sigma}$. The figure \ref{fig:uncorrdata} demonstrates the effect of multiplying the original data by $\bm{L}^{-1}$. In this figure, it is now clear that the point at original coordinates $[2,0]$ is much closer to the origin in Euclidean distance than the the point with original coordinates $[0,1]$.

\cenfig{graphs/uncorrelated_data}{Uncorrelated data using Cholesky decomposition of covariance matrix}{fig:uncorrdata}

Many researchers tackle metric learning by adapting a Mahalanobis distance metric first described in \cite{mahalanobis1936generalized} in \ref{eq:maha:orig}, where $\bm{\Sigma}$ is a covariance matrix of the data defined in equation \ref{eq:cov}. However this name is now overloaded and Mahalanobis distance now stands for any metric parametrized by a matrix $\bm{M}$ as defined in equation \ref{eq:maha}.

\begin{align} \label{eq:maha:orig}
d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^T\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})}  \\
\bm{\Sigma} &= \mathrm {E} \left[\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)\left(\mathbf {X} -\mathrm {E} [\mathbf {X} ]\right)^{\rm {T}}\right] \label{eq:cov}
\end{align}

\begin{equation} \label{eq:maha}
d_{\bm{M}}(\textbf{x},\textbf{y}) = \sqrt{(\textbf{x}-\textbf{y})^T\bm{M}(\textbf{x}-\textbf{y})} 
\end{equation}

The problem of correlation between dimensions does not arise in the Mahalanobis distance \eqref{eq:maha:orig} because it decorrelates the data intrinsically before calculating the distance itself. If the Cholesky decomposition is applied directly inside the equation \ref{eq:maha:orig}, we get that the Mahalanobis distance of the original data is equal to the Euclidean distance of the decorrelated data as shown in \ref{eq:mahalanobis:decorrelation}.

\begin{align}
  d_{Mahalanobis}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{\Sigma}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}(\bm{L}\bm{L}^{T})^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{-T}\bm{L}^{-1}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})^{T}(\bm{L}^{-1}\textbf{x}-\bm{L}^{-1}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}^{-1}\textbf{x}, \bm{L}^{-1}\textbf{y}) \label{eq:mahalanobis:decorrelation}
\end{align}

Therefore from the \ref{eq:mahalanobis:decorrelation} it is possible to see the Mahalanobis distance has the following properties:
\begin{enumerate}
\item it automatically accounts for the scaling of the coordinate axes
\item it corrects for correlation between different features
\item it reduces to the Euclidean distance for uncorrelated data with unit variance
\end{enumerate}

The main weakness with the original Mahalanobis distance using the covariance matrix \ref{eq:maha:orig} is that it does not take the labels of the data instances into account (it can be thought of as "unsupervised" method). For this reason new metric learning algorithms exploiting additional information about data were developed and the term Mahalanobis distance now refers to not only decorrelation of the data using the Covariance matrix but any linear transformation using different matrices as defined in \ref{eq:maha}.

Not all matrices $\bm{M}$ in the generalized form of the Mahalanobis distance defined in equation \ref{eq:maha} would define a distance metric and one needs to be careful of what matrices $\bm{M}$ are used in the Mahalanobis distance so that it remains a distance metric. From a mathematical perspective, a metric is a function $d(x,y)$ that defines a distance or dissimilarity between two instances from some input space: $d:\mathcal{X} \times \mathcal{X} \mapsto [0,\inf)$. For a function to be a metric it has to follow 4 conditions: nonnegativity, identity of indiscernibles, symmetry and triangle inequality as defined in equations \ref{eq:metricdef-1} --- \ref{eq:metricdef-4}.

% https://en.wikipedia.org/wiki/Metric_(mathematics)
\begin{align}
d(\textbf{x},\textbf{y}) &\geq 0 & nonnegativity \label{eq:metricdef-1} \\
d(\textbf{x},\textbf{y}) &= 0 \iff \textbf{x}=\textbf{y} & identity \ of \ indiscernibles \label{eq:metricdef-2} \\
d(\textbf{x},\textbf{y}) &= d(\textbf{y},\textbf{x}) & symmetry \label{eq:metricdef-3} \\
d(\textbf{x},\textbf{z}) &\leq d(\textbf{x},\textbf{y}) + d(\textbf{y},\textbf{z}) & triangle \ inequality \label{eq:metricdef-4}
\end{align} 

In order for the Mahalanobis metric from equation \eqref{eq:maha} to be a metric, the matrix $\bf{M}$ that parametrizes the distance needs to pass the metric conditions \ref{eq:metricdef-1} --- \ref{eq:metricdef-4}. It turns out that a strictly \ac{pd} matrices $\bm{M} \succ 0$ do pass the conditions and therefore the Mahalanobis distance with a \ac{pd} matrix is a distance metric.

Sometimes, however, it is not easy to guarantee the positive definiteness of a matrix and we have to settle with a \ac{psd} matrix $\bf{M} \succeq 0$. For \ac{psd} matrices the identity of indiscernibles condition \ref{eq:metricdef-2} is not met and therefore the Mahalanobis distance with a \ac{psd} matrix is not a metric. For this reason the condition is usually relaxed into a condition of identity \ref{eq:pseudodef-2} which allows $d(x,y)=0$ for two distinct values $x \neq y$ whereas this is impossible using the identity of indiscernibles \ref{eq:pseudodef-2}. Together with the rest of the conditions, this rule forms a new set of conditions \ref{eq:pseudodef-1} ---\ref{eq:pseudodef-4}, which together defines a pseudo-metric. Therefore the Mahalanobis distance with a \ac{psd} matrix is a pseudo-metric.

\begin{align}
d(\textbf{x},\textbf{y}) &\geq 0 & nonnegativity \label{eq:pseudodef-1} \\
d(\textbf{x},\textbf{x}) &= 0 & identity \label{eq:pseudodef-2} \\
d(\textbf{x},\textbf{y}) &= d(\textbf{y},\textbf{x}) & symmetry \label{eq:pseudodef-3} \\
d(\textbf{x},\textbf{z}) &\leq d(\textbf{x},\textbf{y}) + d(\textbf{y},\textbf{z}) & triangle \ inequality \label{eq:pseudodef-4}
\end{align} 

For the generalized Mahalanobis distance the same trick from the \ref{eq:mahalanobis:decorrelation} can be used for any general matrix $\bm{M}$ as can be seen in \ref{eq:mahalanobis:transform}. Cholesky decomposition is defined for any \ac{pd} matrix: $\bm{M}=\bm{L}\bm{L}^T$ where $\bm{L} \in \mathbb{R}^{k \times d}$ where $k$ is the rank of $\bm{M}$. Thus every Mahalanobis distance with a \ac{pd} matrix can be instead replaced by some linear transformation using a matrix $\bm{L}$ and simply calculating the Euclidean distance as usual. This interesting feature of Mahalanobis distance allows us to look at metric learning algorithms also as a data transformation or data preprocessing. Decorrelating the data using the Covariance matrix is also called data whitening.

\begin{align}
  d_{\bm{M}}(\textbf{x},\textbf{y}) &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{M}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\textbf{x}-\textbf{y})^{T}\bm{L}^{T}\bm{L}(\textbf{x}-\textbf{y})} \nonumber\\
         &= \sqrt{(\bm{L}\textbf{x}-\bm{L}\textbf{y})^{T}(\bm{L}\textbf{x}-\bm{L}\textbf{y})} \nonumber\\
         &= d_{\bm{2}}(\bm{L}\textbf{x}, \bm{L}\textbf{y}) \label{eq:mahalanobis:transform}
\end{align}

Note that if the $\bm{M}$ matrix is low rank, i.e., $rank(\bm{M})=k<d$ then the linear projection using the matrix $\bm{L}$ will transform the data into a lower-dimensional ($k$-dimensional) space. This allows for a more compact representation of the data and cheaper distance computations inside of the low-dimensional space. The effect is more prominent the bigger the difference between the original dimension $d$ and the new dimension $k$.

Learning Mahalanobis distance comes with two challenges. The first challenge is ensuring that the matrix $\bm{M}$ stays \ac{psd} during the learning process. A simple way to ensure this constraint is to use a projection onto the \ac{psd} cone by setting the negative eigenvalues to zero, which is described in \cite{qian2015efficient}. Still, the eigenvalue decomposition takes $\mathcal{O}(d^3)$ and therefore is intractable for high-dimensional datasets. The second challenge is to learn a low-rank matrix $\bm{M}$ instead of the full-rank one. This would imply having the transformation to the low-dimensional space with all the advantages described earlier. Unfortunately, finding a minimum-rank solution of a \ac{sdp} is NP-hard as described in \cite{lemon2016low}.

%TODO
% Úvod do problematiky
% - Co už se udělalo
% - Co je známo
% - Co není známo
% - Co a pro Co a proč chceme naší prací objasnit naší prací objasnit MOTIVATION
% - Jasná definice cíle práce GOALS
% - Struktura práce

% Related works
% - Co už udělali jiní
% - Co nového hodlá udělat autor

\section{Outline}
%TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Related work} \label{chap:rw}
%TODO [Scalability in N and D] 

As already mentioned in Chapter \ref{chap:intro}, \acl{dml} is rapidly gaining traction in the recent years and it has found many applications in many fields as discussed in Section \ref{chap:intro:applications}. The survey from \cite{bellet2013survey} is a comprehensive review of the supervised Mahalanobis distance learning methods. In the survey in addition to \textit{Linear methods} that we present here, they also review \textit{Online methods} where the metric is updated after every instance; \textit{Multi-Task methods} in which the metric is trained for multiple related tasks at once to improve performance; \textit{Nonlinear methods} where the metric is no longer linear, typically achieved by kernelization of some linear method; \textit{Local Metric methods} where multiple metrics are learnt for different parts of the space and methods for \textit{Semi-Supervised Setting} in which most of the instances do not have labels. In this work we mostly focus on the Linear metric learning methods because the rest of the categories extend these simpler methods and build on top of them.

Most of the research has been focused on learning a metric from labelled training instances (supervised setting), nevertheless, researchers also focus on weakly-supervised tasks, where the labels of the instances are unknown, but the information about the data comes in the form of similar and dissimilar pairs (Equations \ref{eq:similar}, \ref{eq:dissimilar}) and semi-supervised setting has received some attention as well.

The Mahalanobis distance that was originally proposed by \cite{mahalanobis1936generalized} (Equation \ref{eq:maha:orig}) used a Covariance matrix of the data and can be looked at as a first \acl{dml} method, even though it is unsupervised, it still learns from the data. Since then the term Mahalanobis distance has been generalized and is now used with any \ac{psd} matrix, not just Covariance, this was discussed in more detail in Section \ref{chap:intro:mah}. This generalized Mahalanobis distance is now used in many linear \acl{dml} methods.

We have tested many methods from the \ac{dml} survey: Covariance (the original Mahalanobis distance), \ac{itml}, \ac{lmnn}, \ac{lsml}, \ac{nca}, \ac{rca}, \ac{sdml}. In our tests we found that \ac{lmnn} and \ac{nca} work well and so we focused on those. We also found a prominent method called \ac{lfda} and described in \cite{sugiyama2007dimensionality}. \Acl{dml} algorithms that are trained using evolutionary algorithms are also receiving some attention. We chose the work of \cite{fukui2013evolutionary} who uses an adaptive evolution strategy to evolve a Mahalanobis distance. In Table \ref{tab:rw:summary} you can see methods that we think are the most prominent are we are going to use those further in our experiments. In the next sections we will describe the individual \acl{dml} methods in more detail.

\begin{table}[ht] \centering
\begin{tabular}{llllll}
\hline
Name & Year & Supervision & Optimum & Regularizer & Notes \\
\hline
Covariance & 1936 & None & Global & None & Unsupervised \\
% MMC & 2002 & Weak & Global & None & — \\
LMNN & 2005 & Full & Global & None & For k-NN \\
NCA & 2004 & Full & Local & None & For k-NN \\
LFDA & 2007 & Full & Global & None & — \\
% RCA & 2003 & Weak & Global & None & — \\
% ITML & 2007 & Weak & Global & LogDet & Online version \\
% SDML & 2009 & Weak & Global & LogDet+L1 & $n \ll d$ \\
jDE.wFme & 2013 & Full & Local & None & Evolution \\
\hline
\end{tabular}
\caption{Summary of related metric learning methods} \label{tab:rw:summary}
\end{table}

\section{k-Nearest Neighbour classifier} \label{alg:knn}

Some \acl{dml} methods were designed to directly improve \acl{knn} classifier effectiveness. The performance of the \ac{knn} classifier largely depends on the metric used. In Figure \ref{pseudo:knn} you can see the pseudocode for classifying an unknown sample using \ac{knn} classifier. Any metric can be used in the algorithm simply by changing the distance metric on Line \ref{pseudo:knn:dist}. Modifying the \ac{knn} algorithm to use a custom metric is trivial even for faster implementations using trees for nearest neighbour search.

\begin{figure}
\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwData{set of labelled instances $\mathcal{Z}=\{\bm{z}_i=(\bm{x}_i, y_i): \bm{x}_i \in \mathcal{X}, y_i \in \mathcal{Y}\}^n_{i=1}$, where $\mathcal{X}$ are training data and $\mathcal{Y}$ are their class labels}
\KwIn{unknown sample $\bm{x} \in \mathcal{X}$, number of neighbours $k$}
\KwResult{class label of the unknown sample $\bm{x}$}
\Begin{
  \For{$i\leftarrow 1$ \KwTo $m$}{
    \lnl{pseudo:knn:dist} $distances[i]$ $\leftarrow$ compute distance d($\bm{z}_i$, $\bm{x}$)\;
  }
  \Return majority class from the $k$ smallest $distances$\;
}
\end{algorithm} 
\caption{Pseudocode for k-NN classification} \label{pseudo:knn}
\end{figure}

\section{MMC Xing method} \label{chap:rw:xing}
The work of \cite{xing2002distance} is the first Mahalanobis distance learning algorithm leveraging some information about the data. It is a weakly supervised method that takes information in a form of pairs of similar and dissimilar instances as defined in Equations \ref{eq:similar} and \ref{eq:dissimilar}. They formulate a convex optimisation problem that tries to maximize the distance between dissimilar pairs of instances while keeping the sum of the distances between similar pairs as low as possible ($\leq 1$). The convex program is shown in Equation \ref{eq:rw:xing}.

\begin{align*}
\max_{\bm{M} \in \mathbb{S}^{d}_+} \quad & \sum_{(x_i,x_j)\in \mathcal{D}} d_{\bm{M}}(x_i,x_j) \\
\text{s.t.} \quad & \sum_{(x_i,x_j)\in \mathcal{S}} d_{\bm{M}}^2(x_i,x_j) \leq 1 \\
& \bm{M} \succeq 0 \numberthis \label{eq:rw:xing}
\end{align*}

They solve this convex program using gradient ascend to maximize the distances between dissimilar instances, followed by projections to ensure the conditions of the program are held (matrix $\bm{M}$ must stay \ac{psd} and the distances between similar pair must stay low).

\section{Large Margin Nearest Neighbor} \label{chap:rw:lmnn}
The \acf{lmnn} method designed by \cite{weinberger2009distance} is a very popular method that has been improved upon by many other researchers, such as Multiple-Metrics \ac{lmnn} \citep{weinberger2008fast}, Multi-task \ac{lmnn} \citep{parameswaran2010large} and Gradient-Boosted \ac{lmnn} \citep{kedem2012non}. The goal of this method is to improve the performance of the \ac{knn} algorithm and so the idea is that $k$-nearest neighbours with the same class should stay together while separating the imposters (instances with a different class) by a large margin. It works by defining the constraints in a local way: it optimizes only over similar points that are also in a close neighbourhood of each other as defined in Equation \ref{eq:rw:lmnn:similar}. This neighbourhood is usually determined using Euclidean distance.

\begin{align}
\mathcal{S} &= \lbrace(\bm{x}_i,\bm{x}_j): y_i = y_j \text{ and } \bm{x}_j \text{ is in the }k\text{-neighbourhood of } \bm{x}_i \rbrace  \label{eq:rw:lmnn:similar} \\
\mathcal{R} &= \lbrace(\bm{x}_i,\bm{x}_j,\bm{x}_k): (\bm{x}_i,\bm{x}_j) \in \mathcal{S}, y_i \neq y_k \rbrace
\end{align}

The distance metric is then learnt using the convex program defined in Equation \ref{eq:rw:lmnn}. The authors exploited the locality of the constraints and designed their own solver that is able to solve the program even with billions of constraints. The $\mu$ parameter controls the pull/push trade-off, which influences how much the algorithm will push the imposters away and pull the similar instances.

\begin{align*}
\min_{\bm{M} \in \mathbb{S}^{d}_+} \quad & (1-\mu)\sum_{(\bm{x}_i,\bm{x}_j)\in \mathcal{S}} d_{\bm{M}}^2(\bm{x}_i,\bm{x}_j) + \mu \sum_{i,j,k} \xi_{ijk} \\
\text{s.t.} \quad & d_{\bm{M}}^2(\bm{x}_i,\bm{x}_k) - d_{\bm{M}}^2(\bm{x}_i,\bm{x}_j) \geq 1 - \xi_{ijk} \quad \forall(\bm{x}_i,\bm{x}_j,\bm{x}_k) \in \mathcal{R} \numberthis \label{eq:rw:lmnn}
\end{align*}

The Figure \ref{fig:rw:lmnn} illustrates how the \ac{lmnn} algorithm works. The left diagram shows neighbourhood of the points before training and the right diagram after training. The distance metric is optimized so that the points lie inside of a smaller neighbourhood, while the instances with different classes are pushed outside of this neighbourhood.

\cenfig{graphs/lmnn}{LMNN algorithm illustration before and after training}{fig:rw:lmnn}

% \section{Information Theoretic Metric Learning (ITML)} \label{chap:rw:itml}
% \cite{davis2007information}

% \section{Sparse Determinant Metric Learning (SDML)} \label{chap:rw:sdml}
% \cite{qi2009efficient}

% \section{Least Squares Metric Learning (LSML)} \label{chap:rw:lsml}
% \cite{liu2012metric}

% \section{Relative Components Analysis (RCA)} \label{chap:rw:rca}
% \cite{shental2002adjustment}

\section{Neighborhood Components Analysis} \label{chap:rw:nca}
\cite{jacobgoldberger2004neighbourhood} proposed another method designed to improve the performance of the \ac{knn} classifier called \acf{nca}. The algorithm works by calculating the leave-one-out error for a stochastic version of the \ac{knn} classifier which uses Mahalanobis distance. Instead of searching for $k$-nearest neighbours, it uses a softmax probability distribution, defined in Equation \ref{eq:rw:nca:softmax}, which is used to weigh all the neighbours while favouring the closer ones. Note that the hyperparameter $k$ is no longer needed in this stochastic setting.

\begin{equation}
p_{ij} = \frac{ \exp(-\lVert \bm{L}x_i-\bm{L}x_j \rVert_2^2) }{ \sum_{l \neq i} \exp(-\lVert \bm{L}x_i-\bm{L}x_l \rVert_2^2) }, p_{ii}=0 \label{eq:rw:nca:softmax}
\end{equation}

The objective of the \ac{nca} algorithm is to maximize the expected number of correctly classified points, defined in Equation \ref{eq:rw:nca:objective}. Thanks to the stochastic definition of the neighbour search, this function is continuous and differentiable. Therefore a gradient descend algorithm can be used to optimize it. The main disadvantage of this method is that the objective function is non-convex and therefore \ac{nca} optimization can suffer from local minima and the performance depends on the initialization of the matrix $\bm{L}$. 

\begin{equation}
\max_{\bm{L}} \sum_i \sum_{j:y_j=y_i} p_{ij} \label{eq:rw:nca:objective}
\end{equation}
% \bm{L}_{NCA} &= \argmax_{\bm{L} \in \mathbb{R}^{d \times r}} \bigg( \sum_{i=1}^n \sum_{j:y_j=y_i} p_{ij}(\bm{L}\bm{L}^\top) \bigg)

The algorithm is also valid if the matrix $\bm{L}$ is chosen to be rectangular and therefore the \ac{nca} algorithm can also be used for dimensionality reduction by using the matrix $\bm{L}$ to project the original space into a lower dimensional Euclidean space.

\section{Local Fisher Discriminant Analysis} \label{chap:rw:lfda}
\acf{lfda} algorithm \citep{sugiyama2007dimensionality} is a combination of \acf{lda} method \cite{fisher1936use} and \acf{lpp} method \cite{he2003locality}. \ac{lda} works by maximizing the amount of between-class variance relative to the amount of within-class variance. This might be problem when instances from one class form more clusters because these variances are calculated globally in \ac{lda}. \ac{lfda} improves upon this by introducing locality in the between-class and within-class variances using an affinity matrix from \ac{lpp}. The affinity matrix a square matrix where $\bm{A}_{i,j} \in [0,1]$ and $\bm{A}_{i,j}$ is small when $\bm{x}_i$ and $\bm{x}_j$ are close and it is big when they are far apart. If $\forall i,j: \bm{A}_{i,j}=1$ then we get the original \ac{lda} algorithm.

The within-class and between-class covariance matrices ($\widetilde{\bm{S}}^{(w)}$ and $\widetilde{\bm{S}}^{(b)}$ respectively) for \ac{lfda} are defined in Equations \ref{eq:rw:lfda:within} and \ref{eq:rw:lfda:between}. Note that the affinity matrix $\bm{A}$ is not used to weight the instances from different classes in Equations \ref{eq:rw:lfda:wwithin} and \ref{eq:rw:lfda:wbetween} because these instances should be separated irrespective of the closeness.

\begin{align}
\widetilde{\bm{S}}^{(w)} &= \frac{1}{2} \sum_{i,j=1}^n \widetilde{\bm{W}}_{i,j}^{(w)} (x_i-x_j)(x_i-x_j)^\top \label{eq:rw:lfda:within}
\\
\widetilde{\bm{S}}^{(b)} &= \frac{1}{2} \sum_{i,j=1}^n \widetilde{\bm{W}}_{i,j}^{(b)} (x_i-x_j)(x_i-x_j)^\top \label{eq:rw:lfda:between}
\\
\widetilde{\bm{W}}_{i,j}^{(w)} &= \twopartdef{\bm{A}_{i,j}/n_l}{y_i=y_j=l}{0}{y_i \neq y_j} \label{eq:rw:lfda:wwithin}
\\
\widetilde{\bm{W}}_{i,j}^{(b)} &= \twopartdef{\bm{A}_{i,j}(1/n-1/n_l)}{y_i=y_j=l}{1/n}{y_i \neq y_j} \label{eq:rw:lfda:wbetween}
\end{align}

The final transformation matrix $\bm{L}$ for \ac{lfda} is defined exactly the same way as for \ac{lda}, in Equation \ref{eq:rw:lfda:objective}. This matrix transforms the space so that nearby data pairs with the same class are pulled closer, the data pairs with different classes are pushed further apart and the data pairs with the same class but far away from each other in the original space are not constrained to become closer. The transformation matrix can be obtained by solving the generalized eigenvalue decomposition of the scatter matrices and the authors offer an efficient implementation.

\begin{equation}
\bm{L}_{LFDA} = \argmax_{\bm{L} \in \mathbb{R}^{d \times r}} \bigg[\tr\Big( ( \bm{L}^\top\widetilde{\bm{S}}^{(w)}\bm{L} )^{-1} \bm{L}^\top\widetilde{\bm{S}}^{(b)}\bm{L} \Big)\bigg] \label{eq:rw:lfda:objective}
\end{equation}

Both \ac{lda} and \ac{lpp} methods can be used for dimensionality reduction and so can be \ac{lfda} by restricting the matrix $\bm{L}$ to be rectangular.

\section{Evolutionary distance metric learning} \label{chap:rw:fukui}

%TODO what is the difference between Differential Evolution and Evolution Strategy? DE vs ES???

%TODO DE, ES as acronyms??!
The work of \cite{fukui2013evolutionary} takes a different approach to learning Mahalanobis distance. They use a Differential Evolution strategy in which they encode the Mahalanobis matrix into an individual. The matrix is symmetric and therefore they encode it as a real valued vector of length $d(d+1)/2$. The Differential Evolution is a special case of an Evolution algorithm which is a stochastic, population based algorithm that tries to optimize a real-valued objective function with many parameters. The objective function is typically non-differentiable and non-continuous. A typical schema of the evolution algorithm is shown in Figure \ref{fig:rw:ea:schema}: a population of individuals is randomly initiated and evaluated using the objective (fitness) function. Then the algorithm breeds new individuals through mutation and cross-over, the individuals are evaluated and a new population of the individuals is selected. This whole process is repeated until some termination rule is met. The Differential Evolution has a special type of mutation and cross-over operations. The pseudocode for Differential Evolution is shown in Figure \ref{pseudo:de}. The DE algorithm has two hyperparameters apart from the size of the population: crossover probability $CR$ and differential weight $F$. The DE can be very sensitive to the choice of these two hyperparameters. \cite{fukui2013evolutionary} uses an improved version of the Differential Evolution called \acf{jde} \citep{brest2006self}, which encodes the hyperparameters $CR$ and $F$ inside of every individual and therefore \ac{jde} does not need these two hyperparameters to be specified.

%TODO add figure: evolution algorithm schema!!
% http://ktiml.mff.cuni.cz/~neruda/eva1-16en.pdf [schema]

%TODO https://en.wikipedia.org/wiki/CMA-ES
%TODO https://en.wikipedia.org/wiki/Evolutionary_algorithm
%TODO https://en.wikipedia.org/wiki/Differential_evolution
%TODO https://www.quora.com/What-is-the-difference-between-Genetic-algorithm-and-differential-evolution
%TODO http://www.maths.uq.edu.au/MASCOS/Multi-Agent04/Fleetwood.pdf

\begin{figure}
\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwIn{fitness function $f$, crossover probability $CR \in [0,1]$, differential weight $F \in [0,2]$}
\KwResult{individual $\bm{x}$ that maximizes $f$}
Initialize all agents with random positions in the search-space.\;
\Repeat{a termination criterion is met}{
  \For{each agent $\bm{x}$ in the population}{
    Pick distinct agents $\bm{a}, \bm{b}$, $\bm{c}$ from the population at random\;
    Pick a random index $R \in \{1,\ldots ,n\}$\;
    \For{$i\leftarrow 1$ \KwTo $n$}{
      \lIf{$rand()\equiv U(0,1) \leq CR$ or $i = R $}{$\bm{y}_i=\bm{a}_{i}+F\times (\bm{b}_{i}-\bm{c}_{i})$}
      \lElse{$\bm{y}_{i}=\bm{x}_{i}$}
    }
    \lIf{$f(\bm{y})<f(\bm{x})$}{replace $\bm{x}$ with $\bm{y}$ in the population}
  }
}
\end{algorithm} 
\caption{Pseudocode for Differential Evolution algorithm} \label{pseudo:de}
\end{figure}

%TODO validity measures
%http://www.cs.kent.edu/~jin/DM08/ClusterValidation.pdf
% internal (cohesion, separation, silhouette coeficient), external (entropy, purity), ...

\subsection{Evolution strategies} \label{chap:intro:es} \label{alg:jde}
Quick intro

Describe EAs, ... crossover, mutation, selection (+schema)

\subsection{jDE} 
move to related work

TODO podkapitola na jDE s log populaci
jDE ma vetsi vypocetni silu diky vetsim generacim
->> dat stejnou vypocetni silu


\section{Dimensionality Reduction}

Metric learning can also be used for dimensionality reduction as discussed in mentioned in Section \ref{chap:intro}. This experiment was focused on dimensionality reduction into two dimensions where it is easy to visualise the data.

One of the most common and the simplest unsupervised dimensionality reduction method is \ac{pca} \citep{jolliffe2002principal}. \ac{pca} picks the dimensions of the data with the highest variance. This corresponds to finding the eigenvalues and eigenvectors of the covariance matrix, the eigenvectors with the largest eigenvalues correspond to the dimensions that have the strongest correlation in the dataset.

[Is Covariance Mahalanobis === PCA??]

LFDA uses the same trick for dimensionality reduction, it calculates the metric matrix and then picks the columns of this matrix with the largest corresponding eigenvalues.

[Preprocessing for t-SNE]

t-SNE \cite{maaten2008visualizing} is another unsupervised dimensionality reduction method which is getting a lot of attention recently. Compared to PCA, t-SNE has a non-convex objective function and is optimized using gradient descend, which means it may end up in a local minimum. .... recommended to run more times.

[add how tsne works?]

PCA is connected to Mahalanobis!!
% http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues
% http://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance/62147#62147
->
formally explain that evolving 2*D matrix corresponds to some eigen value decomposition???

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Improving metric evolution} \label{chap:our-method}

main difference: we evolve L (no adjustments needed!)
proof that $L^TL$ is PSD matrix  (using SVD)
% http://www.deeplearningbook.org/contents/linear_algebra.html

CMAES + kNN

full matrix, diagonal matrix, neural network shape

k-Means for dimensionality reduction

% http://math.stackexchange.com/questions/158219/is-a-matrix-multiplied-with-its-transpose-something-special
% Moreover if AA is regular, then AATAAT is also positive definite, since
xTAATx=(ATx)T(ATx)>0

\section{CMA-ES strategy} \label{alg:cmaes}

simple evolution, jDE, CMAES, ...

compare sizes of populations for full and diagonal matrices... and with jDE

\begin{tabbing}
\hspace{50pt}\=\kill
$N$ \> number of samples \\
$D$ \> number of attributes \\ 
$G$ \> number of generations (200) \\
\end{tabbing} 

$\mathcal{O}(N^{2.376})$ matrix multiplication 

\begin{table}[ht] \centering
\begin{tabular}{rll}
\hline
Method & full matrix & diagonal matrix \\
\hline
Covariance & $\mathcal{O}(ND^2)$ & - \\
LMNN & $i*\mathcal{O}()$ & - \\
NCA & $i*\mathcal{O}(N^2D^2)$ & - \\
LFDA & $\mathcal{O}(N^2D)$ & - \\
CMAES.kNN & $\mathcal{O}()$ & - \\
CMAES.fMe & $\mathcal{O}()$ & - \\
jDE.kNN & $\mathcal{O}()$ & - \\
jde.fMe & $\mathcal{O}()$ & - \\ 
\hline
\end{tabular}
\caption{XXX} \label{tab:XXX}
\end{table}


jDE
$P=10*D^2$ population size
$\mathcal{O}(GP*(D^2+F_{eval}))$ for full
$\mathcal{O}(GP*(D+F_{eval}))$ for diagonal

CMAES
$P=4*3\log{D^2}$ population size
$\mathcal{O}(G*(PD+PD^2+PF_{eval}+D+D^2+D^3))$ for diagonal
% generate pop PD, PD^2
% update D, D^2
% eigen decomp D^3

Fitness
$N$ split
$D^2$ fit transformer (for full)
$ND^2$ transform samples (for full)

kNN
$N\log{N}$ build
$N^{1-\frac{1}{D}}+k$ retrieve

wFme ??

% http://stackoverflow.com/questions/9146086/time-complexity-of-genetic-algorithm
% Genetic Algorithms are not chaotic, they are stochastic. The complexity depends on the genetic operators, their implementation (which may have a very significant effect on overall complexity), the representation of the individuals and the population, and obviously on the fitness function. Given the usual choices (point mutation, one point crossover, roulette wheel selection) a Genetic Algorithms complexity is O(g(nm + nm + n)) with g the number of generations, n the population size and m the size of the individuals. Therefore the complexity is on the order of O(gnm)). This is of cause ignoring the fitness function, which depends on the application.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Dimensionality Reduction} \label{chap:dr}

%TODO mention curse of dimensionality

\section{Linear} \label{chap:dr:linear}

\section{Neural network transformation} \label{chap:dr:nn}

how, where, what...

sigm, tanh, relu...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments and Examples}
We performed four different experiments to assess the performance of metric learning algorithms. In section \ref{chap:exp:classification} we use learnt metric in \ac{knn} classificator and compare classification errors on various datasets, which are described in \ref{chap:exp:datasets}. In the next experiment in section \ref{chap:exp:fitness} we compare different evolutionary algorithms together with various fitnesses and show how well they evolve and generalize. In section \ref{chap:exp:learning-times} we measure the training times of the algorithms and in the last experiment in section \ref{chap:exp:dimred} we use metric learning algorithms for visualisation and we compare the resulting embeddings.

\section{Experimental settings} 
In our experiments we compare metric learning algorithms listed in section \ref{chap:rw}. \ac{lmnn}, \ac{nca}, \ac{lfda} metric learning algorithms were already implemented in an open-sourced Python \textit{metric-learn} library \footnote{\url{https://github.com/all-umass/metric-learn}}. Only our evolutionary method described in section \ref{chap:our-method} and \ac{jdefme} from  \cite{fukui2013evolutionary} were missing in this library and thus we designed a modular interface and implemented these two methods as a part of the \textit{metric-learn} library. The~implementation is described in chapter \ref{chap:impl}.

\subsection{Datasets} \label{chap:exp:datasets}
We chose to experiment on classification datasets that were most common among all related works, particularily in \cite{xing2002distance}, \cite{weinberger2009distance}, \cite{jacobgoldberger2004neighbourhood} and \cite{fukui2013evolutionary}. We found that the most common among all these papers were balance-scale, breast-cancer, iris, mice-protein, pima-indians, sonar and wine dataset. All these datasets were obtained from a well-known UCI Machine Learning Repository\footnote{\url{https://archive.ics.uci.edu/ml/datasets/}}. Even among these datasets mice-protein and sonar  have high dimensionality, however they are small in terms of number of samples. We also added digits6 and digits10 datasets, also obtained from the same archive, which are relatively larger datasets containing $8\times 8$ pixel images of digits with 6 and 10 classes respectively.

In order to test the metric learning algorithms on a highly variable data, we created an artificial dataset of four multinomial Gaussians. Each Gaussian has a different center, defined as a matrix in equation \eqref{eq:gauss:means} where each row corresponds to one center. All Gaussians were generated sharing one covariance matrix, defined in equation \eqref{eq:gauss:cov}. Dimensions are uncorrelated, however the first dimension has an enormous variability of $10^8$.

\begin{equation} \label{eq:gauss:means}
means = \begin{pmatrix}
10 & 0 & 0 & 2 \\
0 & 10 & 0 & -2 \\
0 & 0 & 10 & -2 \\
0 & 0 & -10 & 2 \\
\end{pmatrix}
\end{equation}
\begin{equation} \label{eq:gauss:cov}
covariance = \begin{pmatrix}
10^8 & 0 & 0 & 0 \\
0 & 100 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\end{equation}

Table \ref{tab:datasets} summarizes datasets that we experimented on. The table shows number of samples, dimensionality and number of distinct classes for every dataset. All the datasets are meant for classification task, meaning that all of their instances belong to some class. All the datasets have a labelled instances and so they can be used in a supervised methods. Most of the attributes of the datasets are real numbers, but some of them also contain integral values. The largest dataset in terms of number of samples and number of classes is digits10 dataset with 1797 samples. However the largest dataset in terms of number of dimensions is mice-protein dataset. Most of the datasets are relatively low in both number of samples, which is under a thousand, and their dimensionality is usually under 10.

\begin{table}[ht] \centering
\begin{tabular}{lrrr}
\hline
% \multicolumn{4}{c}{Item} \\
% \cline{1-2}
Dataset & \#~samples & \#~dimensions & \#~classes \\
\hline
balance-scale           & 625   & 4    & 3  \\
breast-cancer           & 699   & 9    & 2  \\
digits6                 & 1083  & 64   & 6  \\
digits10                & \textbf{1797}  & 64  & \textbf{10} \\
gaussians               & 400   & 5   & 4  \\
iris                    & 150   & 4    & 3  \\
mice-protein            & 1080  & \textbf{77}   & 8  \\
pima-indians            & 768   & 8    & 2  \\
sonar                   & 208   & 60   & 2  \\
wine                    & 178   & 13   & 3  \\
%soybean-large           & 307   & 35   & \textbf{19} \\
%ionosphere              & 351   & 34   & 2  \\
%letters                 & 20000 & 16   & 26 \\
%mnist                   & 70000 & 784  & 10 \\
%ofaces                  & 400   & 4096 & 40 \\
\hline
\end{tabular}
\caption{Datasets overall summary} \label{tab:datasets}
\end{table}

\subsection{Preprocessing data} \label{chap:exp:preprocessing}
Some of the datasets have missing attributes. There are several strategies for dealing with missing attributes, such as removing affected samples, filling missing values with a random value or zeros, their mean, median or the most frequent value. We chose to fill missing values using the mean for any given attribute as we found it behaves well with all the chosen datasets.

\begin{table}[ht] \centering
\begin{tabular}{lrrrrr}
\hline
Dataset & minimum & maximum & mean & std. deviation \\
\hline
balance-scale           & 1.00  & 5.00    & 3.00  & 1.41 \\
breast-cancer           & 1.00  & 10.00   & 3.13  & 2.88 \\
digits6                 & 0.00  & 16.00   & 4.87  & 6.04 \\
digits10                & 0.00  & 16.00   & 4.88  & 6.02 \\
gaussians               & \textbf{-33870.07} & \textbf{31033.73} & -7.65 & \textbf{5285.16} \\
iris                    & 0.10  & 7.90    & 3.46  & 1.97 \\
mice-protein            & -0.06 & 8.48    & 0.68  & 0.79 \\
pima-indians            & 0.00  & 846.00  & 44.99 & 58.37 \\
sonar                   & 0.00  & 1.00    & 0.28  & 0.28 \\
wine                    & 0.13  & 1680.00 & \textbf{69.13} & 215.75 \\
\hline
\end{tabular}
\caption{Datasets samples statistics} \label{tab:datasets-samples}
\end{table}

Another difficulty with the datasets is that many of them do not have normalized attributes and their attribute values are unbounded and highly variable. From table \ref{tab:datasets-samples} it is clear that the most variable dataset is our artificial gaussians dataset, however pima-indians and wine datasets are also highly variable with a standard deviation of $58.37$ and $215.75$ respectively. The~datasets were normalized using standardization defined in equation \ref{eq:stand}, where $\mu$ is a mean vector for of each of the attributes and $\sigma$ is a vector of their standard deviations.

\begin{equation} \label{eq:stand}
\hat{X} = \frac{X-\mu}{\sigma}
\end{equation}

\section{Experiment: Classification} \label{chap:exp:classification}

In this experiment we wanted to see how much does learnt metric help in classification task compared to standard Euclidean distance. We chose \ac{knn} classifier, which is one of the simplest and yet powerful classifiers and it is very easy to modify to use a custom metric as discussed in Section \ref{alg:knn}.

The metric learning algorithms that we tested in our experiments are Covariance (original Mahalanobis metric with a covariance matrix defined in Equation \ref{eq:maha:orig}), \ac{lmnn}, \ac{nca} and \ac{jdefme}. All these methods were described in Chapter \ref{chap:rw}. Next, we tested \ac{cmaesknn} that we proposed in Chapter \ref{chap:our-method}. We were also curious if the \ac{knn} fitness function would improve \ac{jde} strategy and how would \ac{cmaes} perform with \ac{fme} fitness function so we also tested all these combinations: \ac{cmaesknn}, \ac{cmaesfme}, \ac{jdefme} and \ac{jdeknn}, where the first part until the dot describes evolution strategy used in the algorithm and second part describes its fitness function. Finally, we also tested all datasets against the Euclidean distance which acted as our baseline method.

To assess performance of the metric learning algorithms we used datasets listed in section \ref{chap:exp:datasets} and used 10-fold cross validation for each algorithm. For each fold data were first preprocessed and standardized as described in section \ref{chap:exp:preprocessing}, then the metric was trained using one of the methods and finally the metric was evaluated using \ac{knn} algorithm.

We also wanted to test, how standardization influences the learnt metrics and whether the metric learning algorithms can handle unnormalized data and so we also tried all of the experiments again, but without standardizing the data.

\subsection{Hyperperameter search} \label{chap:exp:hypsearch}

Every metric learning algorithm has a set of hyperparameters which highly influences the learnt metric. For each hyperparameter we selected sensible values from a reasonable range. All hyperparameters for each algorithm are shown in table \ref{tab:hyperparams}. The performance of \ac{knn} classifier hugely depends on its number of neighbours hyperparameter. We chose to try values 1, 2, 4, 8, 16, 32, 64, 128 for this parameter. From the hyperparameter table we can see that all the algorithms except Covariance and \ac{lfda} are iterative (max\_iter and n\_gen hyperparameters) and we tried 50, 100, 250 and 1000 iterations for all these iterative algorithms. The transformer parameter means whether the algorithm uses the full matrix or it is restricted to a diagonal transformation matrix. Unfortunately the library did not provide means for trying this and so we tested this only with the evolution methods that we implemented ourselves. The rest of the hyperparameters is self-explanatory and has been described in Chapters \ref{chap:rw} and \ref{chap:our-method}.

\input{results/hyperparameters}

Even for these sensibly chosen hyperparameter values there are thousands of possible combinations for every single algorithm and thus we have to fit thousands of different models. Let's consider \ac{lmnn} algorithm: there are 10 folds, 8 possible values for the final \ac{knn} classifier hyperparameter, \ac{lmnn} itself has 4 distinct hyperparameters with 6, 3, 4 and 3 values, which means there would be $10*8*(6*3*4*3)=17280$ different models to train just for \ac{lmnn} algorithm if a simple grid search was used. This is a major problem because some of the algorithms take a long time to train even on small datasets and trying that many combinations would take too long to test.

We can notice, however, that we do not need to retrain our metric for every hyperparameter of the final \ac{knn} classifier. Instead we can train a metric model first and then train and test \ac{knn} classifier with all hyperparameter options. Therefore, we designed our version of grid search that is hierarchical in a sense that it will train a metric with all combinations of hyperparameters first and only then it will evaluate these metrics using \ac{knn} with its own hyperparameters. There will still be the same total number of models, however there will be 8 times less evaluations of every metric learning algorithm, which roughly translates to 8 times less computation time as training a \ac{knn} classifier is negligible compared to a metric model.

\subsection{Results}

In table \ref{tab:error-rates} you can see the error rates and their standard deviation (across 10 runs from the cross validation) of the final \ac{knn} classifier for every dataset and every method. If the method name is prefixed with "s:" it means the dataset was standardized as described in Section \ref{chap:exp:preprocessing}. The bold numbers signify that it is the best result for the given dataset. \ac{nca} did not finish on the 3 largest datasets and so the corresponding cells are marked as a "Timeout". Some other methods did not finish because an error occurred and those cells are marked with an "Error" text. The Covariance method failed for some datasets because the matrix was not \ac{pd} and so it was not possible to use Cholesky decomposition. \ac{lfda} also failed for some datasets because of a factorization of a matrix during the calculation. \ac{nca} failed for gaussians and mice-protein datasets because the numbers overflew to infinity during the computation.

\input{results/error-rates}

% https://en.wikipedia.org/wiki/Interquartile_range
The results are also plotted in figures \ref{fig:cl:sr1} and \ref{fig:cl:sr2} using the box plots. Each dataset has its own graph where all the methods that finished correctly are drawn with their specific colour using the box plot graph. The median success rate is marked with a thick black line, the area of the box is the \ac{iqr} defined as the third quartile minus first quartile: $IQR = Q_3 - Q_1$. The whiskers extend $1.5*IQR$ to both sides of the box plot and anything beyond whiskers is considered to be an outlier value.

%TODO zacit odstavcem, ktery vypichne nejlepsi metody (uderni shrnuti)... teprve pak nejake zabrednuti  ??  ---- to by mel by tento nasledujici

%TODO "\ac{knn} behaves much better as a fitness" Pilat: to se da trochu cekat, pro to knn totiz optimalizuje primo to, co te zajima; fme pocita trochu neco jineho  --- to asi jo, oni to delali spis pro clustering
From the results in the table \ref{tab:error-rates} or the figures \ref{fig:cl:sr1} and \ref{fig:cl:sr2} we can see that the standardization of the data is generally a good idea, only datasets digits6, digits10 and iris have the lowest error rates when unstandardized, however the standardized versions follow up with such a small margin that it is not a statistically significant difference. On the other hand, standardizing data can have a big impact as can be seen on the gaussians, sonar and wine datasets, where the difference between standardized and unstandardized data can be tens of percents difference. Our original hypothesis that the metric learning algorithms can replace data standardization therefore failed, it is better to use metric learning algorithms on the standardized data.

Overall three methods behave very well across all datasets: \ac{lmnn} with 6 best results, \ac{cmaesknn} with 4 best results and \ac{lfda} with one best result. \ac{nca} method was not best in any dataset, but it closely followed the best methods. It is interesting to note that both \ac{jdefme} and \ac{jdeknn} won on the artificial gaussians dataset, however they were closely followed by \ac{cmaesknn} method. The worst method is clearly Covariance algorithm, possibly because it is the unsupervised method. \ac{cmaesfme} and \ac{jdefme} methods perform the worst on most of the datasets which suggests that \ac{fme} fitness function does not work very well, whereas \ac{knn} behaves much better as a fitness function when compared to \ac{cmaesknn} and \ac{jdeknn}. It is also important to note that the Euclidean distance that acts as a baseline is outperformed by a metric learning algorithm on every dataset, except digits6 where it ties with \ac{lmnn} method. This suggests metric learning algorithms can have a big impact in the machine learning tasks.

\cenfig{graphs/classification/sr_1.pdf}{Boxplots of successrates from 10-fold crossvalidation for the first 6 datasets}{fig:cl:sr1}

\cenfig{graphs/classification/sr_2.pdf}{Boxplots of successrates from 10-fold crossvalidation for the next 4 datasets}{fig:cl:sr2}

To see how the hyperparameters influence the success rate of the \ac{knn} classifier we choose one hyperparameter and for each of its values we look at the best possible success rate among the rest of the hyperparameters.

First we look at the $k$ hyperparameter in the final \ac{knn} classifier. In figure \ref{fig:cl:kpar} there is one graph for every dataset (digits6 dataset is omitted because it behaves very similarly to digits10 dataset) in which each line corresponds to one algorithm. The best value for this hyperparameter differs for each dataset, however most of the curves have a tip near $k=8$. From individual graphs it is possible to see some trends among all the methods, in particular that the lines tend to copy a downwards parabola shape, meaning that $k$ values of 1 and 2 are too small to generalize well and values 64 and 128 are too big for these small datasets. The best shape for any particular method would be a flat line near the top of the graph, which would imply that the method separated the instances into a well separable and homogeneous clusters in which all instances have the same label. Unfortunately none of the methods behave so great that they would have flat curve. Nevertheless, what is important to notice, is that some of the methods seem to handle the change of this parameter more gracefully. This is the most prominent with \ac{lmnn} and \ac{nca} methods in the mice-protein dataset: these two methods have large success rates even when the $k$ parameter increases, whereas the other method success rates drop significantly. This fenomenon is visible in other datasets and even other methods, for example in gaussians dataset for the \ac{cmaesknn} as well as the two already mentioned. The Covariance method has a bad success rate altogether, but it also drops down the fastest when the $k$ hyperparameter increases.

\cenfig{graphs/classification/sr_knn}{Successrates of individual algorithms when fixing `k` in the final kNN classifier}{fig:cl:kpar}

Next we look at the hyperparameters of the individual methods shown in figure \ref{fig:cl:hyp}. For each hyperparameter there is a different graph and each curve in a graph represents one dataset. The first graph shows the $k$ hyperparameter for the \ac{lmnn} method, which is number of neighbours that the method takes into account when optimizing as discussed in section \ref{chap:rw:lmnn}. It is clear that this hyperparameter influences the success rate of this method by a big amount and it seems to peak at values between 4 and 16. Second graph represents maximum number of iterations (max\_iter) for the \ac{lmnn} algorithm from which we can see that running the algorithm for 250 iterations works very well, but it starts to overfit slightly afterwards. Regularization parameter also improves the success rate when it is higher for all datasets except the sonar, shown in the third graph. Fourth graph shows that learning rate is not that important for \ac{nca} algorithm. Weighted metric works slightly better than orthonormalized for the \ac{lfda} algorithm. Sixth graphs shows a comparison between evolving a diagonal and full Mahalanobis matrix in \ac{cmaes} algorithm. For all except two datasets (pima-indians and mice-protein) evolving the full matrix worked better. Most likely the \ac{cmaes} strategy could not explore the whole $d^2$-dimensional space well because \textit{pima-indians} is a high-dimensional dataset. The same hyperparameter for the \ac{jdefme} method is shown in the last graph and evolving a full matrix works clearly better for all datasets. In the seventh and eighth graphs there are two more hyperparameters for the \ac{cmaesknn} method, more precisely for its \ac{knn} fitness method, where we can see that the best combination is to have 8 neighbours and uniform weights.

\cenfig{graphs/classification/sr_hyp}{Successrates of individual algorithms when fixing some of their hyperparameters}{fig:cl:hyp}

\section{Experiment: Generalization of EAs} \label{chap:exp:fitness}

To test out how well do solutions from evolution algorithms evolve and generalize we performed another series of tests. Using \ac{cmaes} and \ac{jde} evolution strategies described in sections \ref{alg:cmaes} and \ref{alg:jde}, combined with \ac{knn} and \ac{fme} fitnesses described in Sections \ref{alg:knn} and \ref{chap:rw:fukui} respectively. Both strategies with both fitnesses were tested, giving us 4 different algorithms: \ac{cmaesknn}, \ac{cmaesfme}, \ac{jdeknn} and \ac{jdefme}. Both full and diagonal Mahalanobis matrices were experimented on during this experiment, totalling to 8 different models.

All methods have some hyperparameters and in order to avoid the time consuming hyperparameter search, the hyperparameters for the models in this experiment were picked according to results from the previous experiment in section \ref{chap:exp:classification}. Only one set of hyperparameters was picked for all datasets. The number of nearest neighbours in the \ac{knn} classifier used in the fitness function was chosen to be 8 and the number of nearest neighbours in the final \ac{knn} classifier was set to 4. All methods were evolved for a full thousand generations.

Datasets described in section \ref{chap:exp:datasets} were used as well for this experiment. Data were preprocessed and standardized the same way as previous experiment, both methods are described in section \ref{chap:exp:preprocessing}. Data were split into training and testing sets using a 67:33 ratio in a stratified fashion, keeping the sizes of classes balanced between the two splits.

To gauge the performance of the fitness function we logged the fitness values of all individuals in every generation and then calculated median, minimum, maximum, 25th and 75th percentiles (Q1 and Q3) in each generation. To see how well the individuals generalize, individuals with the best and the worst fitness values were picked in every generation and the metric encoded in these individuals was evaluated using the test set using \ac{knn} classifier after every single generation.

\subsection{Results}

In figures \ref{fig:fitness:balance-scale}, \ref{fig:fitness:breast-cancer}, \ref{fig:fitness:digits6}, \ref{fig:fitness:digits10}, \ref{fig:fitness:gaussians}, \ref{fig:fitness:iris}, \ref{fig:fitness:mice-protein}, \ref{fig:fitness:pima-indians}, \ref{fig:fitness:sonar}, \ref{fig:fitness:wine} we can see the results for each dataset. In each figure there are 8 different graphs, the top four are methods learning a full matrix, the bottom four are methods restricted to a diagonal matrix. Both \ac{knn} and \ac{fme} fitness functions output values in a closed range $[0,1]$. The final \ac{knn} classifier used to gauge performance outputs values in the same exact range and so we plot all values in the same graph. Therefore each individual graph shows minimal and maximal (both in orange) fitnesses, median fitness (in green) with the area between 25th and 75th percentiles coloured in light green, the blue line represents the test success rate of the individual with the highest fitness and the pink line represents the test success rate of the individual with the lowest fitness in every generation. A black line representing a test success rate of the \ac{knn} using standard Euclidean distance is also added to the graphs. Unfortunately \ac{jde} algorithm did not finish on the four largest datasets and we mark those graphs with a "Timeout" text.

Important thing to also keep in mind when looking at the graphs is that the left four graphs all use \ac{knn} for both fitness evaluation as well as test success rate evaluation, meanwhile the right four graphs use \ac{fme} fitness for learning, but the test success rate is still evaluated using \ac{knn} classifier. This means that the left four graphs have fitnesses and their test success rates directly comparable, meanwhile in the right four graphs we can only look at the general trends and can't directly compare fitness values with test success rates.

The first thing to notice is a completely different nature of \ac{cmaes} compared to \ac{jde} strategy. The \ac{jde} strategy updates the population only when a new individual with a better fitness values comes along, whereas the \ac{cmaes} strategy regenerates the entire population from scratch every generation according to its internal covariance matrix. This is clearly visible on all the graphs: for \ac{jde}, the orange and blue lines representing fitnesses never drop, they only grow. On the other hand, \ac{cmaes} strategy fitness lines are much more chaotic than that and so we use a one dimensional Gaussian filter with a standard deviation 2.0 in order to smooth the lines and make the graphs more readable. 

Now we look at the performance of the individual methods. \ac{cmaesknn} (full) methods performs very well on most datasets. From the graphs we can see that it generalizes also very well, which can be deduced from the fact that the blue and pink lines representing test success rate closely follow the green fitness value lines. \ac{cmaesknn} (full) can overfit and plummet after some number of generations which can be seen in figure \ref{fig:fitness:iris} on the iris dataset, where the test success rate first goes up to 94\% and then starts dropping and finishes at 86\% success rate. This suggests that there should be an early stopping mechanism in place inside of the algorithm. That would help with both overfitting and it would also cut down learning time because the algorithm would not have to finish a fixed number of generations. If we compare the full version with the diagonal version of this algorithm, the full version should dominate in terms of the success rate. However the diagonal version behaves  better on some datasets, namely on breast-cancer (figure \ref{fig:fitness:breast-cancer}), digits10 (figure \ref{fig:fitness:digits10}) and iris (figure \ref{fig:fitness:iris}). This might be because the algorithm was better at exploring the $d$-dimensional space compared to $d^2$-dimensional space in the case of the full matrix. Nevertheless, the full matrix dominates the diagonal matrix on balance-scale (figure \ref{fig:fitness:balance-scale}), mice-protein (figure \ref{fig:fitness:mice-protein}), sonar (figure \ref{fig:fitness:sonar}) and wine (figure \ref{fig:fitness:wine}) datasets. Moreover, in this experiment we are more interested in general trends of the evolution, not absolute values of the fitness functions.

If we compare \ac{cmaesknn} with \ac{jdeknn}, the \ac{jdeknn} algorithm generalizes worse even though its fitness values are higher than fitness values of \ac{cmaesknn} algorithm. In every single dataset the blue and pink lines representing the test success rates are far below the green fitness line and on most datasets even far below the lower orange fitness line representing the minimal fitness in the given generation. This problem concerns evolving both full and diagonal Mahalanobis matrices. Even though it does not generalize well and the fitness values tend to overestimate the success rate, the testing success rates are relatively comparable, although slightly worse, to those using \ac{cmaesknn}.

Now let's have a look at the methods with the \ac{fme} fitness function. The fitness values of \ac{cmaesfme} have a very large range on all datasets, for examples on the breast-cancer dataset (figure \ref{fig:fitness:breast-cancer}) the fitness values range from nearly 0\% to around 97\%. This highly variable range would suggest it would be hard for the method to generate individuals, however the test success rates perform relatively well, but generally worse when compared to previously mentioned methods with the \ac{knn} fitness function. The biggest problem for the \ac{cmaesfme} is that the fitness does not grow over time and therefore the test success rates do not grow either. This might be caused by the large fitness variability and because the \ac{fme} does not say anything about the structure of the clusters and together with the fact that the \ac{cmaes} strategy regenerates the individuals in each generation, the generation does not improve the individuals but tends to be very random. This problem is overcome with the \ac{jdefme} method, which does not regenerate the population but rather only replaces the individuals with better ones. For this reason the fitness values of \ac{jdefme} are only rising, same behaviour as for the \ac{jdeknn}. The problem with \ac{jdefme} method is that the test success rates do not follow the growth of the fitness values, but they tend to stagnate as you can see on the pima-indians (figure \ref{fig:fitness:pima-indians}) and wine datasets (figure \ref{fig:fitness:wine}). This might be a problem of the \ac{fme} not being suitable to represent the fitness of the clusters as the \ac{cmaesfme} suffered from the same problem. This concerns evolving both the full and the diagonal matrices.

\cenfig{graphs/fitness/balance-scale}{Fitness evolution and generalization on `balance-scale` dataset}{fig:fitness:balance-scale}
\cenfig{graphs/fitness/breast-cancer}{Fitness evolution and generalization on `breast-cancer` dataset}{fig:fitness:breast-cancer}
\cenfig{graphs/fitness/digits6}{Fitness evolution and generalization on `digits6` dataset}{fig:fitness:digits6}
\cenfig{graphs/fitness/digits10}{Fitness evolution and generalization on `digits10` dataset}{fig:fitness:digits10}
\cenfig{graphs/fitness/gaussians}{Fitness evolution and generalization on `gaussians` dataset}{fig:fitness:gaussians}
\cenfig{graphs/fitness/iris}{Fitness evolution and generalization on `iris` dataset}{fig:fitness:iris}
\cenfig{graphs/fitness/mice-protein}{Fitness evolution and generalization on `mice-protein` dataset}{fig:fitness:mice-protein}
\cenfig{graphs/fitness/pima-indians}{Fitness evolution and generalization on `pima-indians` dataset}{fig:fitness:pima-indians}
\cenfig{graphs/fitness/sonar}{Fitness evolution and generalization on `sonar` dataset}{fig:fitness:sonar}
\cenfig{graphs/fitness/wine}{Fitness evolution and generalization on `wine` dataset}{fig:fitness:wine}


\section{Experiment: Learning time} \label{chap:exp:learning-times}

Learning time of each method is an important factor to be considered. Some methods are not scalable for large datasets as already discussed in chapter \ref{chap:rw}. It is not easy to describe the metric learning algorithms in the big O notation because most of the methods are iterative and it is unclear how many iterations is needed for them to converge. Moreover, some of them can still end up in a local minima thus multiple runs may be necessary for optimal results.

In this experiment we measured the learning times of all the methods from previous experiments: Covariance, \ac{lmnn}, \ac{nca}, \ac{lfda} and both evolution strategies with both evolution fitnesses: \ac{cmaesknn}, \ac{cmaesfme}, \ac{jdeknn} and \ac{jdefme}. For the evolution methods we also measured learning only a restricted diagonal transformation matrix. All methods were run 10 times on every dataset with a set of the hyper-parameters which corresponds to best error rate for given dataset from the first experiment in section \ref{chap:exp:classification}. Euclidean distance does not need any training and therefore is omitted from this experiment altogether. The experiment was performed on a single computer so that the results would be comparable. Having this restriction, the maximal time for the method to finish was set to three hours, after this period the experiment was interrupted. The methods that did not finish are marked as "Timeout".

\subsection{Results}

The results are in table \ref{tab:learning-times} where the learning times shown are averaged across all 10 runs. Calculating the Covariance metric is the fastest, closely followed by \ac{lfda} algorithm. Some of the datasets were so small that these two methods would take a less than a hundredth of a second. On the other hand, \ac{lmnn} and \ac{nca} metrics and all the evolution metrics take a lot of time to learn. Generally, \ac{jde} evolution strategy should be much faster compared to \ac{cmaes} strategy because calculating a covariance matrix inside \ac{cmaes} is expensive, however the \ac{jde} needs a much bigger population and the fitness evaluation is very expensive in our case as discussed in Section \ref{alg:jde}. Therefore, \ac{jde} is clearly much slower compared to \ac{cmaes} strategy in the results table. It is also clear that the \ac{fme} fitness is much slower compared to \ac{knn} fitness as discussed in \ref{alg:knn} when we compare \ac{cmaesknn} with \ac{cmaesfme} and \ac{jdeknn} with \ac{jdefme}. This makes the method \ac{jdefme} described in Section \ref{chap:rw:fukui} the slowest method in all of our benchmarked methods and is even several times slower than \ac{nca} algorithm. The \ac{cmaesknn} performs comparable to \ac{lmnn} in terms of learning time. The restricted versions of the evolutionary algorithms to a diagonal transformation matrix learn much faster thanks to smaller individuals which directly encode the Mahalanobis matrix and also the smaller populations as also discussed in \ref{alg:cmaes}. What is important to note, however, is that differences between learning a full and a diagonal matrices using \ac{cmaes} are much smaller than the differences between learning a full and a diagonal matrices using \ac{jde} algorithm, where for \ac{cmaes} the difference is about 2 times, but for \ac{jde} the difference is much more significant, even 10 times slower on some datasets.

\input{results/learning-times}

To further investigate how well do metric learning methods scale, we also designed two additional experiments to investigate the learning times. In the first experiment we kept the dimensionality of the data fixed to 5 and measured the learning times for the number of samples in the range from 100 to 1500 with 100 increments. In the second experiment we fixed the number of samples to 500 and measured the learning times for all methods with the increasing dimension in the range from 2 to 10 with an increment of 1. All these measurements were done using the digits10 dataset from which the required number of samples was sampled using the stratified sampling and then the dimension of the samples was reduced using \ac{pca} to a required dimension.

Both experiments are represented in figures \ref{fig:learning-times:samples} and \ref{fig:learning-times:dimensions}. Most of the methods merge together in the graph so we show two graphs for each experiment: one with linear and one with logarithmic scale applied on y axis. Covariance and \ac{lfda} algorithms are not included in these two figures because they perform so fast on this experiment that the results are not interpretable due to random fluctuations in the measurements.

From figure \ref{fig:learning-times:samples} it is clear that the \ac{jde} algorithm performs the worst and scales poorly with an increasing number of samples with both \ac{knn} and \ac{fme} fitnesses. \ac{nca} performs also very bad with an increasing number of samples. On the other hand the rest of the methods perform relatively well and are indistinguishable on the linearly scaled axis. \ac{lmnn} and \ac{cmaes} algorithms have a similar performance, however \ac{lmnn} also performs much slower with more and more instances. In contrast, \ac{cmaes} algorithm scales very well and the number of samples hardly influences the performance. Looking at the evolutionary methods restricted to a diagonal matrix, it is clear that \ac{cmaesknn} (diag) is the fastest among all of them, however it is not much slower compared to its unrestricted version \ac{cmaesknn} (full). Restricted version of \ac{jde} algorithms are much faster compared to their unrestricted counterparts, but even these restricted versions are slower and scale worse than \ac{cmaesknn} (full).

\cenfig{graphs/learning-times/samples}{Learning times for increasing number of samples with a fixed dimension (linear and log scale)}{fig:learning-times:samples}

Similarly, from figure \ref{fig:learning-times:dimensions} we can see that \ac{jde} scales very poorly with an increasing dimensionality of the dataset as well. The order of the methods in the graphs remained the same as in the previous experiment, however there are some notable changes in the shapes of the curves. \ac{lfda} algorithm scales very well with an increasing dimensionality; \ac{nca} and \ac{lmnn} are not as steep as in the previous experiment, but they still scale worse compared to \ac{cmaes} algorithm. It is clear that restricted versions of all evolutionary algorithms scale much better in regards of the increasing dimensionality because the dimensionality directly reflects in the individuals size: for the restricted version the individuals have size $D$, but for the unrestricted version we need to encode the whole matrix of size $D^2$. The dimensionality of the individuals directly reflect in the population size as discussed in Section \ref{alg:cmaes}. Therefore the restricted versions scale better with the increasing dimension and their graphs are less steep. When the steepness of \ac{cmaes} and \ac{jde} algorithms is directly compared however, the \ac{jde} scales worse even in the restricted version. That is due to the population size that the algorithm uses by default being linear, compared to \ac{cmaes}, which only has a population of a logarithmic size of its individual dimensionality as already discussed in Section \ref{alg:cmaes}.

\cenfig{graphs/learning-times/dimensions}{Learning times for increasing dimension with a fixed number of samples (linear and log scale)}{fig:learning-times:dimensions}


\section{Experiment: Dimensionality reduction} \label{chap:exp:dimred}

In this experiment we use \acl{dml} algorithms for dimensionality reduction into two dimensions and we compare the resulting visualisations. The same datasets, listed in Table \ref{tab:datasets}, are used in this experiments. The data are standardized as described in Section \ref{chap:exp:preprocessing}.

The dataset is split to training set and test set using a 10-fold cross validation. The algorithms are trained using the training set. Then the trained algorithms are used to transform both train and test sets into two dimensions. To compare the dimensionality reduction, we fit \ac{knn} classifier using this 2-dimensional training set and calculate a success rate on the test set. Only exception to this process is \ac{tsne} algorithm, which does not offer a transformation function (it is a transductive method). Therefore, in experiments where \ac{tsne} is used, the \ac{tsne} is trained using the entire dataset. This makes the final \ac{knn} success rate biased and overly optimistic. % so all the success rates that come from \ac{tsne} method are prefixed with a tilde (\textapproxx) to further signify this.

In this experiment we compare \ac{pca} and \ac{tsne} methods with \acl{dml} methods supporting dimensionality reduction: \ac{nca}, \ac{lfda} and \ac{cmaesknn}. All these methods were described in Chapters \ref{chap:rw} and \ref{chap:dr}. We also experimented with evolving fully-connected neural networks for dimensionality reduction using \acl{cmaesknn} as discussed in Section \ref{chap:dr:nn}. We evolved six different neural networks differing in depth and their activation functions. We experimented with \ac{sigm}, \ac{tanh} and \ac{relu} activation functions, all of them explained in Section \ref{chap:dr:nn}. We tried evolving a shallow networks with one hidden layer with 4 neurons and deeper networks with five hidden layers with 16-12-8-6-4 neurons. To reduce the dimension into 2-dimensional space, all networks always end with an output layer of size 2. The shallow network therefore has $4d+8$ free parameters and the deeper network has $16d+354$ free parameters. No biases were used.

\ac{pca}, \ac{lfda}, \ac{nca} and \ac{cmaesknn} methods are linear and so the dimensionality reduction works by projecting the data onto the 2-dimensional space by a matrix multiplication. The methods only differ in how to find this transformation matrix. We were also interested in whether the \ac{cmaes} would help \ac{tsne} making a better visualisation if used as a preprocessing and we call this method \ac{cmaesknn}+\ac{tsne}, which works as one could expect, the \ac{cmaesknn} is trained first and data are transformed using the evolved transformation matrix keeping the original dimensionality, and only then the \ac{tsne} is used to transform the preprocessed data from \ac{cmaes} into two dimensions.

\subsection{Results}

In Table \ref{tab:dim-error-rates} we show the error rates and standard deviations for each method on each dataset from the 10-fold cross validation. The best method for every dataset is in a bold font. The best method among methods that have a transformation function (all except \ac{tsne}) is emphasized in italics. Generally, \ac{cmaesknn} works very well for dimensionality reduction and also very fast because only a $d\times2$ matrix is evolved. \ac{cmaesknn} has the lowest error only on \textit{pima-indians}, however the difference between this method and the best method on each dataset is never statistically significant. \ac{tsne} works the best on large datasets, namely \textit{digits6} and \textit{digits10} and the preprocessing with \ac{cmaes} helps \ac{tsne} on the majority of the datasets. \ac{lfda} and \ac{nca} have both also very good results, but \ac{nca} runs out of memory on three datasets (marked with "Error" in the results table).

\input{results/dim-red}

The success rate, however, does not signify that the dimensionality reduction is good and we only use it as one of the factors. To better judge the performance of the algorithms, both training and testing sets from the run with the highest success rate are plotted using a scatter plot. Then we visually check how well the algorithm performed using the following criteria: the instances with the same label are together; the clusters are well separated; clusters form small blobs and they are not spread out; all instances with the same class are in one big cluster.

In each scatter plot the instances are coloured according to their label. The training instances are highly opaque without any border. The testing instances are less opaque and they have a black or red border. Black border signifies that the instance was classified correctly using the \ac{knn} classifier and red border means the instance was classified incorrectly. The instances that were classified incorrectly are also visualised as slightly larger. In the title of each plot there is a name of the method used together with the success rate on the test set for the given run that is being visualised.

The results of the dimensionality reduction for all methods are in figures \ref{fig:dimred:balance-scale}-\ref{fig:dimred:wine}. One figure represents one dataset and each figure contains a scatter plot for each method. The \ac{pca} algorithm is the baseline method that we compare all other methods to. 

On the \textit{balance-scale} (Figure \ref{fig:dimred:balance-scale}) dataset the \ac{pca} finds a good projection of the data and it is possible to see some intrinsic relationships between the classes. Nevertheless, the \ac{nca}, \ac{lfda} and \ac{cmaesknn} algorithms all find much better projection in which the blue class forms a barrier between the orange and the green classes. \ac{tsne} by itself completely hides these relationships between the data: the green class is split into two clusters and the blue class is scattered around. Preprocessing the data using \ac{cmaesknn} first before using \ac{tsne} fixes both these problems, the green class stays clustered together and the blue class forms a cluster. The evolved neural networks are also able to separate the classes from each other, but the deep neural network with \ac{tanh} activation makes two well separated clusters, but spreads out the third class.

On the \textit{breast-cancer} (Figure \ref{fig:dimred:breast-cancer}) dataset the \ac{nca} and \ac{cmaesknn} methods separate the clusters more strictly. \ac{tsne} works well by itself, but preprocessing the data with \ac{cmaes} first improves the visualisation again. The neural networks also separate the classes very well. The networks with \ac{sigm} and \ac{tanh} activations tend to keep the instances in long chains, but \ac{relu} activation function tends to cluster one class together and spreads out the other one.

When looking at the \textit{digits6} and \textit{digits10} datasets (Figures \ref{fig:dimred:digits6}, \ref{fig:dimred:digits10}), the \ac{tsne} visualisation dominates other methods. Nevertheless, \ac{cmaesknn}+\ac{tsne} still improves the visualisation by joining some clusters together on both datasets. \ac{pca} algorithm finds some clusters, but they are merged together and not well separated. \ac{cmaesknn} works better compared to \ac{pca}, it manages to separate the clusters more and the clusters themselves are also more uniform. \ac{lfda} method fails on this dataset completely, it spreads out just few data instances, but keeps the rest together in one dense cluster. \ac{nca} method runs out of memory on these datasets. The neural networks with \ac{sigm} and \ac{tanh} activations tend to create interesting, almost cube-like structures, meanwhile \ac{relu} activation creates more of a light torch structure, where many instances are clustered in a center and the rest is spread out in one major direction.

All methods work very well on the \textit{gaussians} dataset (Figure \ref{fig:dimred:gaussians}). The supervised methods, however, find more compact clustering than \ac{pca}. \ac{tsne} splits the blue class into two clusters and this is where \ac{cmaesknn}+\ac{tsne} improves on again. Very similar observation can be made about \textit{iris} dataset (Figure \ref{fig:dimred:iris}). The neural networks again visualise and cluster the data almost as chains.

Both \textit{mice-protein} (Figure \ref{fig:dimred:mice-protein}) and \textit{pima-indians} datasets (Figure \ref{fig:dimred:pima-indians}) are hard to visualise. Regarding \textit{mice-protein} dataset, \ac{pca} does not find any nice intrinsic properties and just makes one big heterogeneous cluster. \ac{nca} ran out of memory and \ac{lfda} only separated out the grey class. \ac{cmaesknn} was able to separate out the most classes, but some classes are still mixed together and they are not well-separated. The best visualisation, at least in regards to success rate, was achieved using \ac{tsne}, but it was not able to capture the classes into homogeneous clusters, but it created many small fragmented clusters. On \textit{pima-indians} there is not a clear winner, but \ac{nca}, \ac{lfda}, \ac{cmaesknn} and \ac{cmaesknn}+\ac{tsne} were the best at separating the two classes from each other.

The \textit{sonar} (Figure \ref{fig:dimred:sonar}) and \textit{wine} (Figure \ref{fig:dimred:wine}) datasets have very few data instances and that might be the reason why \ac{tsne} and even \ac{cmaesknn}+\ac{tsne} failed completely. The \textit{sonar} is a high-dimensional dataset and \ac{pca} failed to separate the clusters as well on this dataset. On the other hand \ac{nca}, \ac{lfda} and \ac{cmaesknn} we able to separate the classes into a homogeneous clusters on both datasets. The neural network methods have high success rate which signifies that the dataset probably has some non-linear intrinsic structure.

\cenfig{graphs/dimred/balance-scale}{Visualisation of `balance-scale` dataset with DML methods}{fig:dimred:balance-scale}
\cenfig{graphs/dimred/breast-cancer}{Visualisation of `breast-cancer` dataset with DML methods}{fig:dimred:breast-cancer}
\cenfig{graphs/dimred/digits6}{Visualisation of `digits6` dataset with DML methods}{fig:dimred:digits6}
\cenfig{graphs/dimred/digits10}{Visualisation of `digits10` dataset with DML methods}{fig:dimred:digits10}
\cenfig{graphs/dimred/gaussians}{Visualisation of `gaussians` dataset with DML methods}{fig:dimred:gaussians}
\cenfig{graphs/dimred/iris}{Visualisation of `iris` dataset with DML methods}{fig:dimred:iris}
\cenfig{graphs/dimred/mice-protein}{Visualisation of `mice-protein` dataset with DML methods}{fig:dimred:mice-protein}
\cenfig{graphs/dimred/pima-indians}{Visualisation of `pima-indians` dataset with DML methods}{fig:dimred:pima-indians}
\cenfig{graphs/dimred/sonar}{Visualisation of `sonar` dataset with DML methods}{fig:dimred:sonar}
\cenfig{graphs/dimred/wine}{Visualisation of `wine` dataset with DML methods}{fig:dimred:wine}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation} \label{chap:impl}

\section{Hierarchical Grid search}

how it works - scheme

how cross-validation is in memory

\section{Metric learning in metric\_learn package}

Description of modules, fitnesses, transformers, ...

\begin{figure}[h!] \label{fig:implementation-modules}
	\centering
    \includegraphics[width=0.2\textwidth]{img/notfound}
    \caption{Interaction between modules for Metric evolution}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Discussion}

Graphs, performace, caveats, what worked, what didnt work...

LMNN, ITML, SDML take too long to calculate

LMNN and LFDA perform very well

LMNN has too many hyper parameters

Simple Covariance metric was impossible to calculate on certain datasets

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

This worked, this didn't work. What we did.

\chapter*{Future work}

%TODO
% http://researchers.lille.inria.fr/abellet/talks/metric_learning_tutorial_ECML_PKDD.pdf

What wasn`t finished and should be.

- early stopping in cmaes (how? they have periods. maybe evaluate test knn every X generations?)

- adding regularizer slide 23-24

- other than global metrics

- nonlinear extensions: kernelizations of linear methods (slides 28-32)

- learning multiple local metrics at once (would it be faster with Evolution?)

- large scale metric learning -- online learning

- metric learning for structured data (slide 46+)

- neural network shape for classification??

- add more values to fitness (multi-objective optimisation)

- compare evolving only a triangular matrix! (generalization? performance?)

- ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Bibliography
\include{bibliography}

%%% Figures used in the thesis (consider if this is needed)
\listoffigures

%%% Tables used in the thesis (consider if this is needed)
%%% In mathematical theses, it could be better to move the list of tables to the beginning of the thesis.
\listoftables

%%% Abbreviations used in the thesis, if any, including their explanation
%%% In mathematical theses, it could be better to move the list of abbreviations to the beginning of the thesis.
\chapwithtoc{List of Abbreviations}

\printacronyms[include-classes=abbrev,heading=none] % ,name=Abbreviations

\chapwithtoc{List of Notations}
\input{notation}

%%% Attachments to the master thesis, if any. Each attachment must be
%%% referred to at least once from the text of the thesis. Attachments
%%% are numbered.
%%%
%%% The printed version should preferably contain attachments, which can be
%%% read (additional tables and charts, supplementary text, examples of
%%% program output, etc.). The electronic version is more suited for attachments
%%% which will likely be used in an electronic form rather than read (program
%%% source code, data files, interactive charts, etc.). Electronic attachments
%%% should be uploaded to SIS and optionally also included in the thesis on a~CD/DVD.
\chapwithtoc{Attachments}

%TODO attach this somewhere?
% \input{results/stat-significance_classification}
% \input{results/stat-significance_dimred}

\openright
\end{document}
